{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install sklearn\n",
    "#%pip install torch\n",
    "\n",
    "from math import sqrt\n",
    "import os\n",
    "from time import time\n",
    "import random\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "## project structure\n",
    "DATA_DIR = \"/data/projects/capturingBias/research/framing/data/\"  # change to \"./\" for current directory\n",
    "DATA_NPZ = DATA_DIR + \"data.npz\"\n",
    "\n",
    "## load files\n",
    "data = np.load(DATA_NPZ)\n",
    "\n",
    "X_2D = data['X_2D']\n",
    "X_3D = data['X_3D']\n",
    "y_likert_crowd = data['y_likert_crowd']\n",
    "y_likert_experts = data['y_likert_experts']\n",
    "y_dominant_crowd = data['y_dominant_crowd']\n",
    "y_dominant_experts = data['y_dominant_experts']\n",
    "\n",
    "\n",
    "# likert\n",
    "likert_expert_idx = np.where(y_likert_experts > -1)[0]\n",
    "likert_crowd_idx = np.setdiff1d(np.where(y_likert_crowd > -1)[0],\n",
    "                                likert_expert_idx,\n",
    "                                assume_unique=True)\n",
    "\n",
    "# dominant\n",
    "dominant_expert_idx = np.where(y_dominant_experts > -1)[0]\n",
    "dominant_crowd_idx = np.setdiff1d(np.where(y_dominant_crowd > -1)[0],\n",
    "                                  dominant_expert_idx,\n",
    "                                  assume_unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=-1):\n",
    "    if seed < 0:\n",
    "        seed = np.random.randint(0, 2**32-1)\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seed(47)  # make reproducable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(y):\n",
    "    train_idx = list()\n",
    "    test_idx = list()\n",
    "    \n",
    "    strats = [np.where(y == lab)[0] for lab in np.unique(y) if lab > -1]\n",
    "    for strat in strats:\n",
    "        n = strat.shape[0]\n",
    "        train_idx.append(strat[:int(n*0.8)])\n",
    "        test_idx.append(strat[int(n*0.8):])\n",
    "        \n",
    "    train_idx = np.concatenate(train_idx)\n",
    "    test_idx = np.concatenate(test_idx)\n",
    "    \n",
    "    np.random.shuffle(train_idx)\n",
    "    np.random.shuffle(test_idx)\n",
    "    \n",
    "    return (train_idx, test_idx)\n",
    "\n",
    "def create_splits_one_hot(y):\n",
    "    vec = -np.ones(y.shape[0])\n",
    "    nonzero = y.nonzero()\n",
    "    vec[nonzero[:,0]] = nonzero[:,1].float()\n",
    "    \n",
    "    return create_splits(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def majority_class(y):\n",
    "    ct = Counter(y)\n",
    "    return ct.most_common(1)[0][1] / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority class accuracy on Likert labels (baseline)\n",
      " crowd labels:  0.2903\n",
      " expert labels: 0.2414\n",
      "\n",
      "Majority class accuracy on Dominant labels (baseline)\n",
      " crowd labels:  0.6000\n",
      " expert labels: 0.6818\n"
     ]
    }
   ],
   "source": [
    "majority_class_acc_crowd_likert = majority_class(y_likert_crowd[likert_crowd_idx])\n",
    "majority_class_acc_experts_likert = majority_class(y_likert_experts[likert_expert_idx])\n",
    "\n",
    "print(\"Majority class accuracy on Likert labels (baseline)\")\n",
    "print(\" crowd labels:  {:.4f}\".format(majority_class_acc_crowd_likert))\n",
    "print(\" expert labels: {:.4f}\".format(majority_class_acc_experts_likert))\n",
    "\n",
    "majority_class_acc_crowd_dominant = majority_class(y_dominant_crowd[dominant_crowd_idx])\n",
    "majority_class_acc_experts_dominant = majority_class(y_dominant_experts[dominant_expert_idx])\n",
    "\n",
    "print(\"\\nMajority class accuracy on Dominant labels (baseline)\")\n",
    "print(\" crowd labels:  {:.4f}\".format(majority_class_acc_crowd_dominant))\n",
    "print(\" expert labels: {:.4f}\".format(majority_class_acc_experts_dominant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest (supervised)\n",
    "\n",
    "We start with a traditional, or 'shallow', machine learning model: random forest. Because random forest does not support iterative learning, we test both the crowd and expert sets separately.\n",
    "\n",
    "We use stratified cross validation to reduce the effects caused by the small size of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "N_ESTIMATORS = [100, 250, 500, 750, 1000, 2000]\n",
    "N_FOLDS = 10\n",
    "\n",
    "def random_forest(X, y, index, n_folds=N_FOLDS, n_estimators=N_ESTIMATORS):\n",
    "    n_samples = X[index].shape[0]\n",
    "    for n_estimators in N_ESTIMATORS:\n",
    "        print(\"Training with {} estimators\".format(n_estimators))\n",
    "        acc = 0\n",
    "        for fold_i in range(N_FOLDS):\n",
    "            print(\" Starting fold {} / {}\".format(fold_i+1, N_FOLDS), end='')\n",
    "            \n",
    "            train_fold_idx, test_fold_idx  = create_splits(y[index])\n",
    "            train_idx = index[train_fold_idx]\n",
    "            test_idx = index[test_fold_idx]\n",
    "        \n",
    "            model = RandomForestClassifier(n_estimators=n_estimators)\n",
    "            model.fit(X[train_idx], y[train_idx])\n",
    "            \n",
    "            y_pred = model.predict(X[test_idx])\n",
    "            fold_acc = accuracy_score(y[test_idx], y_pred)\n",
    "        \n",
    "            acc += fold_acc\n",
    "            print(\" (acc: {:.4f})\".format(fold_acc))\n",
    "            \n",
    "        acc /= N_FOLDS\n",
    "        print(\"Mean accuracy on test set: {:.4f}\\n\".format(acc))\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results of supervised learning on expert likert labels ===\n",
      "Training with 100 estimators\n",
      " Starting fold 1 / 10 (acc: 0.1333)\n",
      " Starting fold 2 / 10 (acc: 0.2667)\n",
      " Starting fold 3 / 10 (acc: 0.2000)\n",
      " Starting fold 4 / 10 (acc: 0.1333)\n",
      " Starting fold 5 / 10 (acc: 0.2667)\n",
      " Starting fold 6 / 10 (acc: 0.1333)\n",
      " Starting fold 7 / 10 (acc: 0.0667)\n",
      " Starting fold 8 / 10 (acc: 0.2000)\n",
      " Starting fold 9 / 10 (acc: 0.1333)\n",
      " Starting fold 10 / 10 (acc: 0.0667)\n",
      "Mean accuracy on test set: 0.1600\n",
      "\n",
      "Training with 250 estimators\n",
      " Starting fold 1 / 10 (acc: 0.1333)\n",
      " Starting fold 2 / 10 (acc: 0.1333)\n",
      " Starting fold 3 / 10 (acc: 0.2000)\n",
      " Starting fold 4 / 10 (acc: 0.2000)\n",
      " Starting fold 5 / 10 (acc: 0.2000)\n",
      " Starting fold 6 / 10 (acc: 0.2000)\n",
      " Starting fold 7 / 10 (acc: 0.2000)\n",
      " Starting fold 8 / 10 (acc: 0.2667)\n",
      " Starting fold 9 / 10 (acc: 0.2667)\n",
      " Starting fold 10 / 10 (acc: 0.2000)\n",
      "Mean accuracy on test set: 0.2000\n",
      "\n",
      "Training with 500 estimators\n",
      " Starting fold 1 / 10 (acc: 0.1333)\n",
      " Starting fold 2 / 10 (acc: 0.2667)\n",
      " Starting fold 3 / 10 (acc: 0.2000)\n",
      " Starting fold 4 / 10 (acc: 0.1333)\n",
      " Starting fold 5 / 10 (acc: 0.1333)\n",
      " Starting fold 6 / 10 (acc: 0.2667)\n",
      " Starting fold 7 / 10 (acc: 0.2000)\n",
      " Starting fold 8 / 10 (acc: 0.2000)\n",
      " Starting fold 9 / 10 (acc: 0.2667)\n",
      " Starting fold 10 / 10 (acc: 0.2000)\n",
      "Mean accuracy on test set: 0.2000\n",
      "\n",
      "Training with 750 estimators\n",
      " Starting fold 1 / 10 (acc: 0.1333)\n",
      " Starting fold 2 / 10 (acc: 0.2667)\n",
      " Starting fold 3 / 10 (acc: 0.2000)\n",
      " Starting fold 4 / 10 (acc: 0.2000)\n",
      " Starting fold 5 / 10 (acc: 0.1333)\n",
      " Starting fold 6 / 10 (acc: 0.1333)\n",
      " Starting fold 7 / 10 (acc: 0.1333)\n",
      " Starting fold 8 / 10 (acc: 0.2000)\n",
      " Starting fold 9 / 10 (acc: 0.2000)\n",
      " Starting fold 10 / 10 (acc: 0.2000)\n",
      "Mean accuracy on test set: 0.1800\n",
      "\n",
      "Training with 1000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.2000)\n",
      " Starting fold 2 / 10 (acc: 0.2000)\n",
      " Starting fold 3 / 10 (acc: 0.2000)\n",
      " Starting fold 4 / 10 (acc: 0.1333)\n",
      " Starting fold 5 / 10 (acc: 0.1333)\n",
      " Starting fold 6 / 10 (acc: 0.2667)\n",
      " Starting fold 7 / 10 (acc: 0.2000)\n",
      " Starting fold 8 / 10 (acc: 0.2667)\n",
      " Starting fold 9 / 10 (acc: 0.2667)\n",
      " Starting fold 10 / 10 (acc: 0.1333)\n",
      "Mean accuracy on test set: 0.2000\n",
      "\n",
      "Training with 2000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.2667)\n",
      " Starting fold 2 / 10 (acc: 0.2667)\n",
      " Starting fold 3 / 10 (acc: 0.2667)\n",
      " Starting fold 4 / 10 (acc: 0.2000)\n",
      " Starting fold 5 / 10 (acc: 0.1333)\n",
      " Starting fold 6 / 10 (acc: 0.2000)\n",
      " Starting fold 7 / 10 (acc: 0.2000)\n",
      " Starting fold 8 / 10 (acc: 0.2000)\n",
      " Starting fold 9 / 10 (acc: 0.2000)\n",
      " Starting fold 10 / 10 (acc: 0.2000)\n",
      "Mean accuracy on test set: 0.2133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Results of supervised learning on expert likert labels ===\")\n",
    "random_forest_acc_experts_likert = random_forest(X_2D,\n",
    "                                                 y_likert_experts, \n",
    "                                                 likert_expert_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results of supervised learning on crowd likert labels ===\n",
      "Training with 100 estimators\n",
      " Starting fold 1 / 10 (acc: 0.2500)\n",
      " Starting fold 2 / 10 (acc: 0.2500)\n",
      " Starting fold 3 / 10 (acc: 0.3125)\n",
      " Starting fold 4 / 10 (acc: 0.2500)\n",
      " Starting fold 5 / 10 (acc: 0.3750)\n",
      " Starting fold 6 / 10 (acc: 0.3125)\n",
      " Starting fold 7 / 10 (acc: 0.1875)\n",
      " Starting fold 8 / 10 (acc: 0.3125)\n",
      " Starting fold 9 / 10 (acc: 0.2500)\n",
      " Starting fold 10 / 10 (acc: 0.3125)\n",
      "Mean accuracy on test set: 0.2812\n",
      "\n",
      "Training with 250 estimators\n",
      " Starting fold 1 / 10 (acc: 0.1875)\n",
      " Starting fold 2 / 10 (acc: 0.2500)\n",
      " Starting fold 3 / 10 (acc: 0.4375)\n",
      " Starting fold 4 / 10 (acc: 0.2500)\n",
      " Starting fold 5 / 10 (acc: 0.3750)\n",
      " Starting fold 6 / 10 (acc: 0.2500)\n",
      " Starting fold 7 / 10 (acc: 0.2500)\n",
      " Starting fold 8 / 10 (acc: 0.2500)\n",
      " Starting fold 9 / 10 (acc: 0.1875)\n",
      " Starting fold 10 / 10 (acc: 0.2500)\n",
      "Mean accuracy on test set: 0.2687\n",
      "\n",
      "Training with 500 estimators\n",
      " Starting fold 1 / 10 (acc: 0.2500)\n",
      " Starting fold 2 / 10 (acc: 0.1875)\n",
      " Starting fold 3 / 10 (acc: 0.3125)\n",
      " Starting fold 4 / 10 (acc: 0.1250)\n",
      " Starting fold 5 / 10 (acc: 0.2500)\n",
      " Starting fold 6 / 10 (acc: 0.1875)\n",
      " Starting fold 7 / 10 (acc: 0.1875)\n",
      " Starting fold 8 / 10 (acc: 0.1875)\n",
      " Starting fold 9 / 10 (acc: 0.2500)\n",
      " Starting fold 10 / 10 (acc: 0.1875)\n",
      "Mean accuracy on test set: 0.2125\n",
      "\n",
      "Training with 750 estimators\n",
      " Starting fold 1 / 10 (acc: 0.2500)\n",
      " Starting fold 2 / 10 (acc: 0.3125)\n",
      " Starting fold 3 / 10 (acc: 0.3125)\n",
      " Starting fold 4 / 10 (acc: 0.2500)\n",
      " Starting fold 5 / 10 (acc: 0.2500)\n",
      " Starting fold 6 / 10 (acc: 0.2500)\n",
      " Starting fold 7 / 10 (acc: 0.3750)\n",
      " Starting fold 8 / 10 (acc: 0.3125)\n",
      " Starting fold 9 / 10 (acc: 0.3750)\n",
      " Starting fold 10 / 10 (acc: 0.3125)\n",
      "Mean accuracy on test set: 0.3000\n",
      "\n",
      "Training with 1000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.3750)\n",
      " Starting fold 2 / 10 (acc: 0.3125)\n",
      " Starting fold 3 / 10 (acc: 0.3750)\n",
      " Starting fold 4 / 10 (acc: 0.1875)\n",
      " Starting fold 5 / 10 (acc: 0.1875)\n",
      " Starting fold 6 / 10 (acc: 0.3125)\n",
      " Starting fold 7 / 10 (acc: 0.3750)\n",
      " Starting fold 8 / 10 (acc: 0.3750)\n",
      " Starting fold 9 / 10 (acc: 0.4375)\n",
      " Starting fold 10 / 10 (acc: 0.2500)\n",
      "Mean accuracy on test set: 0.3187\n",
      "\n",
      "Training with 2000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.2500)\n",
      " Starting fold 2 / 10 (acc: 0.3750)\n",
      " Starting fold 3 / 10 (acc: 0.3125)\n",
      " Starting fold 4 / 10 (acc: 0.2500)\n",
      " Starting fold 5 / 10 (acc: 0.3125)\n",
      " Starting fold 6 / 10 (acc: 0.3125)\n",
      " Starting fold 7 / 10 (acc: 0.3125)\n",
      " Starting fold 8 / 10 (acc: 0.1875)\n",
      " Starting fold 9 / 10 (acc: 0.3125)\n",
      " Starting fold 10 / 10 (acc: 0.3125)\n",
      "Mean accuracy on test set: 0.2938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Results of supervised learning on crowd likert labels ===\")\n",
    "random_forest_acc_crowd_likert = random_forest(X_2D,\n",
    "                                               y_likert_crowd,\n",
    "                                               likert_crowd_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results of supervised learning on expert dominant labels ===\n",
      "Training with 100 estimators\n",
      " Starting fold 1 / 10 (acc: 0.6667)\n",
      " Starting fold 2 / 10 (acc: 0.6667)\n",
      " Starting fold 3 / 10 (acc: 0.4444)\n",
      " Starting fold 4 / 10 (acc: 0.7778)\n",
      " Starting fold 5 / 10 (acc: 0.6667)\n",
      " Starting fold 6 / 10 (acc: 0.7778)\n",
      " Starting fold 7 / 10 (acc: 0.6667)\n",
      " Starting fold 8 / 10 (acc: 0.6667)\n",
      " Starting fold 9 / 10 (acc: 0.7778)\n",
      " Starting fold 10 / 10 (acc: 0.6667)\n",
      "Mean accuracy on test set: 0.6778\n",
      "\n",
      "Training with 250 estimators\n",
      " Starting fold 1 / 10 (acc: 0.6667)\n",
      " Starting fold 2 / 10 (acc: 0.6667)\n",
      " Starting fold 3 / 10 (acc: 0.7778)\n",
      " Starting fold 4 / 10 (acc: 0.6667)\n",
      " Starting fold 5 / 10 (acc: 0.7778)\n",
      " Starting fold 6 / 10 (acc: 0.6667)\n",
      " Starting fold 7 / 10 (acc: 0.6667)\n",
      " Starting fold 8 / 10 (acc: 0.7778)\n",
      " Starting fold 9 / 10 (acc: 0.6667)\n",
      " Starting fold 10 / 10 (acc: 0.6667)\n",
      "Mean accuracy on test set: 0.7000\n",
      "\n",
      "Training with 500 estimators\n",
      " Starting fold 1 / 10 (acc: 0.6667)\n",
      " Starting fold 2 / 10 (acc: 0.6667)\n",
      " Starting fold 3 / 10 (acc: 0.6667)\n",
      " Starting fold 4 / 10 (acc: 0.6667)\n",
      " Starting fold 5 / 10 (acc: 0.6667)\n",
      " Starting fold 6 / 10 (acc: 0.6667)\n",
      " Starting fold 7 / 10 (acc: 0.6667)\n",
      " Starting fold 8 / 10 (acc: 0.6667)\n",
      " Starting fold 9 / 10 (acc: 0.6667)\n",
      " Starting fold 10 / 10 (acc: 0.6667)\n",
      "Mean accuracy on test set: 0.6667\n",
      "\n",
      "Training with 750 estimators\n",
      " Starting fold 1 / 10 (acc: 0.6667)\n",
      " Starting fold 2 / 10 (acc: 0.6667)\n",
      " Starting fold 3 / 10 (acc: 0.6667)\n",
      " Starting fold 4 / 10 (acc: 0.6667)\n",
      " Starting fold 5 / 10 (acc: 0.6667)\n",
      " Starting fold 6 / 10 (acc: 0.6667)\n",
      " Starting fold 7 / 10 (acc: 0.6667)\n",
      " Starting fold 8 / 10 (acc: 0.6667)\n",
      " Starting fold 9 / 10 (acc: 0.6667)\n",
      " Starting fold 10 / 10 (acc: 0.6667)\n",
      "Mean accuracy on test set: 0.6667\n",
      "\n",
      "Training with 1000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.6667)\n",
      " Starting fold 2 / 10 (acc: 0.6667)\n",
      " Starting fold 3 / 10 (acc: 0.6667)\n",
      " Starting fold 4 / 10 (acc: 0.6667)\n",
      " Starting fold 5 / 10 (acc: 0.6667)\n",
      " Starting fold 6 / 10 (acc: 0.6667)\n",
      " Starting fold 7 / 10 (acc: 0.6667)\n",
      " Starting fold 8 / 10 (acc: 0.6667)\n",
      " Starting fold 9 / 10 (acc: 0.6667)\n",
      " Starting fold 10 / 10 (acc: 0.6667)\n",
      "Mean accuracy on test set: 0.6667\n",
      "\n",
      "Training with 2000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.6667)\n",
      " Starting fold 2 / 10 (acc: 0.6667)\n",
      " Starting fold 3 / 10 (acc: 0.6667)\n",
      " Starting fold 4 / 10 (acc: 0.6667)\n",
      " Starting fold 5 / 10 (acc: 0.6667)\n",
      " Starting fold 6 / 10 (acc: 0.6667)\n",
      " Starting fold 7 / 10 (acc: 0.6667)\n",
      " Starting fold 8 / 10 (acc: 0.6667)\n",
      " Starting fold 9 / 10 (acc: 0.6667)\n",
      " Starting fold 10 / 10 (acc: 0.6667)\n",
      "Mean accuracy on test set: 0.6667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Results of supervised learning on expert dominant labels ===\")\n",
    "random_forest_acc_experts_dominant = random_forest(X_2D,\n",
    "                                                   y_dominant_experts, \n",
    "                                                   dominant_expert_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results of supervised learning on crowd dominant labels ===\n",
      "Training with 100 estimators\n",
      " Starting fold 1 / 10 (acc: 0.5714)\n",
      " Starting fold 2 / 10 (acc: 0.5714)\n",
      " Starting fold 3 / 10 (acc: 0.6429)\n",
      " Starting fold 4 / 10 (acc: 0.5714)\n",
      " Starting fold 5 / 10 (acc: 0.6429)\n",
      " Starting fold 6 / 10 (acc: 0.7143)\n",
      " Starting fold 7 / 10 (acc: 0.5714)\n",
      " Starting fold 8 / 10 (acc: 0.5000)\n",
      " Starting fold 9 / 10 (acc: 0.6429)\n",
      " Starting fold 10 / 10 (acc: 0.5000)\n",
      "Mean accuracy on test set: 0.5929\n",
      "\n",
      "Training with 250 estimators\n",
      " Starting fold 1 / 10 (acc: 0.7857)\n",
      " Starting fold 2 / 10 (acc: 0.5714)\n",
      " Starting fold 3 / 10 (acc: 0.5714)\n",
      " Starting fold 4 / 10 (acc: 0.7143)\n",
      " Starting fold 5 / 10 (acc: 0.5714)\n",
      " Starting fold 6 / 10 (acc: 0.6429)\n",
      " Starting fold 7 / 10 (acc: 0.6429)\n",
      " Starting fold 8 / 10 (acc: 0.7143)\n",
      " Starting fold 9 / 10 (acc: 0.5000)\n",
      " Starting fold 10 / 10 (acc: 0.5714)\n",
      "Mean accuracy on test set: 0.6286\n",
      "\n",
      "Training with 500 estimators\n",
      " Starting fold 1 / 10 (acc: 0.6429)\n",
      " Starting fold 2 / 10 (acc: 0.5714)\n",
      " Starting fold 3 / 10 (acc: 0.5714)\n",
      " Starting fold 4 / 10 (acc: 0.6429)\n",
      " Starting fold 5 / 10 (acc: 0.5714)\n",
      " Starting fold 6 / 10 (acc: 0.5714)\n",
      " Starting fold 7 / 10 (acc: 0.5714)\n",
      " Starting fold 8 / 10 (acc: 0.5714)\n",
      " Starting fold 9 / 10 (acc: 0.5714)\n",
      " Starting fold 10 / 10 (acc: 0.5714)\n",
      "Mean accuracy on test set: 0.5857\n",
      "\n",
      "Training with 750 estimators\n",
      " Starting fold 1 / 10 (acc: 0.5714)\n",
      " Starting fold 2 / 10 (acc: 0.5714)\n",
      " Starting fold 3 / 10 (acc: 0.6429)\n",
      " Starting fold 4 / 10 (acc: 0.5714)\n",
      " Starting fold 5 / 10 (acc: 0.5714)\n",
      " Starting fold 6 / 10 (acc: 0.5714)\n",
      " Starting fold 7 / 10 (acc: 0.5714)\n",
      " Starting fold 8 / 10 (acc: 0.5714)\n",
      " Starting fold 9 / 10 (acc: 0.5714)\n",
      " Starting fold 10 / 10 (acc: 0.6429)\n",
      "Mean accuracy on test set: 0.5857\n",
      "\n",
      "Training with 1000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.5714)\n",
      " Starting fold 2 / 10 (acc: 0.5714)\n",
      " Starting fold 3 / 10 (acc: 0.5714)\n",
      " Starting fold 4 / 10 (acc: 0.7143)\n",
      " Starting fold 5 / 10 (acc: 0.5714)\n",
      " Starting fold 6 / 10 (acc: 0.6429)\n",
      " Starting fold 7 / 10 (acc: 0.5714)\n",
      " Starting fold 8 / 10 (acc: 0.5714)\n",
      " Starting fold 9 / 10 (acc: 0.5714)\n",
      " Starting fold 10 / 10 (acc: 0.5714)\n",
      "Mean accuracy on test set: 0.5929\n",
      "\n",
      "Training with 2000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.5714)\n",
      " Starting fold 2 / 10 (acc: 0.5714)\n",
      " Starting fold 3 / 10 (acc: 0.5714)\n",
      " Starting fold 4 / 10 (acc: 0.5714)\n",
      " Starting fold 5 / 10 (acc: 0.5714)\n",
      " Starting fold 6 / 10 (acc: 0.5714)\n",
      " Starting fold 7 / 10 (acc: 0.5714)\n",
      " Starting fold 8 / 10 (acc: 0.5714)\n",
      " Starting fold 9 / 10 (acc: 0.5714)\n",
      " Starting fold 10 / 10 (acc: 0.5714)\n",
      "Mean accuracy on test set: 0.5714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Results of supervised learning on crowd dominant labels ===\")\n",
    "random_forest_acc_crowd_dominant = random_forest(X_2D,\n",
    "                                                 y_dominant_crowd,\n",
    "                                                 dominant_crowd_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert numpy arrays to PyTorch tensors\n",
    "X_2D = torch.from_numpy(X_2D)\n",
    "X_3D = torch.from_numpy(X_3D)\n",
    "y_likert_crowd = torch.from_numpy(y_likert_crowd)\n",
    "y_likert_experts = torch.from_numpy(y_likert_experts)\n",
    "y_dominant_crowd = torch.from_numpy(y_dominant_crowd)\n",
    "y_dominant_experts = torch.from_numpy(y_dominant_experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(y_hat, y):\n",
    "    # y := 1D array of class labels\n",
    "    # y_hat := 2D array of one-hot class labels\n",
    "    _, labels = y_hat.max(dim=1)\n",
    "    return torch.mean(torch.eq(labels, y).float())\n",
    "\n",
    "def fit(model, X, y, index, lr=0.01, l2norm=0.001, n_folds=10, n_epoch=250, patience=7):\n",
    "    n_samples = X[index].shape[0]\n",
    "\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    for fold_i in range(n_folds):\n",
    "        print(\"Starting fold {} / {}\".format(fold_i+1, n_folds), end='')\n",
    "        model.init()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "        \n",
    "        # early stopping\n",
    "        patience_left = patience\n",
    "        best_score = -1\n",
    "        delta = 1e-4\n",
    "        best_state = None\n",
    "        \n",
    "        train_fold_idx, test_fold_idx  = create_splits(y[index])\n",
    "        train_idx = index[train_fold_idx]\n",
    "        test_idx = index[test_fold_idx]\n",
    "        for epoch in range(n_epoch):\n",
    "            model.train()\n",
    "            \n",
    "            y_hat = model(X[train_idx].float())\n",
    "            train_acc = categorical_accuracy(y_hat, y[train_idx])\n",
    "            train_loss = criterion(y_hat, y[train_idx].long())\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            test_loss = None\n",
    "            with torch.no_grad():\n",
    "                y_hat = model(X[test_idx].float())\n",
    "                test_acc = categorical_accuracy(y_hat, y[test_idx])\n",
    "                test_loss = criterion(y_hat, y[test_idx].long())\n",
    "                \n",
    "            train_loss = float(train_loss.item())\n",
    "            test_loss = float(test_loss.item())\n",
    "            \n",
    "            if patience <= 0:\n",
    "                continue\n",
    "            if best_score < 0:\n",
    "                best_score = test_loss\n",
    "                best_state = model.state_dict()\n",
    "            if test_loss >= best_score - delta:\n",
    "                patience_left -= 1\n",
    "            else:\n",
    "                best_score = test_loss\n",
    "                best_state = model.state_dict()\n",
    "                patience_left = patience\n",
    "            if patience_left <= 0:\n",
    "                model.load_state_dict(best_state)\n",
    "                \n",
    "                test_idx = index[create_splits(y[index])[1]]  # get new random test set to validate on\n",
    "                with torch.no_grad():\n",
    "                    y_hat = model(X[test_idx].float())\n",
    "                    test_acc = categorical_accuracy(y_hat, y[test_idx])\n",
    "                    test_loss = float(criterion(y_hat, y[test_idx].long()).item())\n",
    "        \n",
    "        loss += test_loss\n",
    "        acc += test_acc\n",
    "        print(\" - training accuracy: {:.4f} / loss: {:.4f} - test accuracy: {:.4f} / loss: {:.4f}\".format(train_acc,\n",
    "                                                                                          train_loss,\n",
    "                                                                                          test_acc,\n",
    "                                                                                          test_loss))\n",
    "        \n",
    "    loss /= n_folds\n",
    "    acc /= n_folds\n",
    "    print(\"average loss on test set: {:.4f}\".format(loss))\n",
    "    print(\"average accuracy on test set: {:.4f}\".format(acc))\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierNN(nn.Module):\n",
    "    \"\"\"Simple Neural Network Classifier\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, p_dropout=0.05):\n",
    "        super().__init__()\n",
    "        hidden_dim = (input_dim-output_dim)//2\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_dropout),\n",
    "            \n",
    "\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.softmax(self.fc(X))\n",
    "        \n",
    "    def init(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.normal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results on expert likert labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.8372 / loss: 1.3278 - test accuracy: 0.2000 / loss: 1.9281\n",
      "Starting fold 2 / 10 - training accuracy: 0.9535 / loss: 1.2066 - test accuracy: 0.0000 / loss: 2.1007\n",
      "Starting fold 3 / 10 - training accuracy: 0.6977 / loss: 1.4595 - test accuracy: 0.0667 / loss: 2.0391\n",
      "Starting fold 4 / 10 - training accuracy: 0.8605 / loss: 1.3033 - test accuracy: 0.0667 / loss: 2.0832\n",
      "Starting fold 5 / 10 - training accuracy: 0.9302 / loss: 1.2385 - test accuracy: 0.1333 / loss: 1.9407\n",
      "Starting fold 6 / 10 - training accuracy: 0.9070 / loss: 1.2575 - test accuracy: 0.0667 / loss: 2.1006\n",
      "Starting fold 7 / 10 - training accuracy: 0.7442 / loss: 1.4175 - test accuracy: 0.2000 / loss: 1.8992\n",
      "Starting fold 8 / 10 - training accuracy: 0.8837 / loss: 1.2793 - test accuracy: 0.0000 / loss: 2.0930\n",
      "Starting fold 9 / 10 - training accuracy: 0.8837 / loss: 1.2792 - test accuracy: 0.0000 / loss: 2.1144\n",
      "Starting fold 10 / 10 - training accuracy: 0.7907 / loss: 1.3660 - test accuracy: 0.2667 / loss: 1.9171\n",
      "average loss on test set: 2.0216\n",
      "average accuracy on test set: 0.1000\n",
      "\n",
      "=== Results on crowd likert labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.9130 / loss: 1.2411 - test accuracy: 0.2500 / loss: 1.8929\n",
      "Starting fold 2 / 10 - training accuracy: 0.5652 / loss: 1.5904 - test accuracy: 0.2500 / loss: 1.9081\n",
      "Starting fold 3 / 10 - training accuracy: 0.9348 / loss: 1.2279 - test accuracy: 0.1875 / loss: 1.9601\n",
      "Starting fold 4 / 10 - training accuracy: 0.8696 / loss: 1.3074 - test accuracy: 0.1875 / loss: 1.9469\n",
      "Starting fold 5 / 10 - training accuracy: 0.9565 / loss: 1.2113 - test accuracy: 0.1250 / loss: 1.9473\n",
      "Starting fold 6 / 10 - training accuracy: 0.7826 / loss: 1.3698 - test accuracy: 0.1875 / loss: 1.9051\n",
      "Starting fold 7 / 10 - training accuracy: 0.9565 / loss: 1.2191 - test accuracy: 0.2500 / loss: 1.9019\n",
      "Starting fold 8 / 10 - training accuracy: 0.9565 / loss: 1.2025 - test accuracy: 0.2500 / loss: 1.8620\n",
      "Starting fold 9 / 10 - training accuracy: 0.8696 / loss: 1.2909 - test accuracy: 0.2500 / loss: 1.8645\n",
      "Starting fold 10 / 10 - training accuracy: 0.8261 / loss: 1.3351 - test accuracy: 0.1875 / loss: 1.9230\n",
      "average loss on test set: 1.9112\n",
      "average accuracy on test set: 0.2125\n"
     ]
    }
   ],
   "source": [
    "## hyperparameters\n",
    "lr = 0.01\n",
    "p_dropout = 0.05\n",
    "\n",
    "## define model\n",
    "indim = X_2D.shape[1]\n",
    "outdim = np.unique(y_likert_experts[likert_expert_idx]).shape[0]\n",
    "assert outdim == np.unique(y_likert_crowd[likert_crowd_idx]).shape[0]\n",
    "\n",
    "model = ClassifierNN(input_dim=indim,\n",
    "                     output_dim=outdim,\n",
    "                     p_dropout=p_dropout)\n",
    "\n",
    "print(\"=== Results on expert likert labels ===\")\n",
    "neural_net_acc_likert_experts = fit(model, X_2D, y_likert_experts, likert_expert_idx, lr=lr)\n",
    "\n",
    "print(\"\\n=== Results on crowd likert labels ===\")\n",
    "neural_net_acc_likert_crowd = fit(model, X_2D, y_likert_crowd, likert_crowd_idx, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results on expert dominant labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.9714 / loss: 0.3517 - test accuracy: 0.6667 / loss: 0.6826\n",
      "Starting fold 2 / 10 - training accuracy: 1.0000 / loss: 0.4340 - test accuracy: 0.4444 / loss: 0.7341\n",
      "Starting fold 3 / 10 - training accuracy: 0.9429 / loss: 0.3704 - test accuracy: 0.5556 / loss: 0.7052\n",
      "Starting fold 4 / 10 - training accuracy: 0.6857 / loss: 0.6275 - test accuracy: 0.6667 / loss: 0.6466\n",
      "Starting fold 5 / 10 - training accuracy: 0.9714 / loss: 0.3458 - test accuracy: 0.6667 / loss: 0.6457\n",
      "Starting fold 6 / 10 - training accuracy: 0.8857 / loss: 0.4703 - test accuracy: 0.6667 / loss: 0.6774\n",
      "Starting fold 7 / 10 - training accuracy: 0.8571 / loss: 0.4559 - test accuracy: 0.7778 / loss: 0.5920\n",
      "Starting fold 8 / 10 - training accuracy: 0.9714 / loss: 0.3428 - test accuracy: 0.6667 / loss: 0.6988\n",
      "Starting fold 9 / 10 - training accuracy: 0.9429 / loss: 0.3618 - test accuracy: 0.4444 / loss: 0.8672\n",
      "Starting fold 10 / 10 - training accuracy: 0.9714 / loss: 0.3288 - test accuracy: 0.5556 / loss: 0.7530\n",
      "average loss on test set: 0.7003\n",
      "average accuracy on test set: 0.6111\n",
      "\n",
      "=== Results on crowd dominant labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 1.0000 / loss: 0.3138 - test accuracy: 0.5000 / loss: 0.8134\n",
      "Starting fold 2 / 10 - training accuracy: 0.9608 / loss: 0.3601 - test accuracy: 0.4286 / loss: 0.8801\n",
      "Starting fold 3 / 10 - training accuracy: 0.9608 / loss: 0.3541 - test accuracy: 0.5714 / loss: 0.7012\n",
      "Starting fold 4 / 10 - training accuracy: 1.0000 / loss: 0.3164 - test accuracy: 0.6429 / loss: 0.6928\n",
      "Starting fold 5 / 10 - training accuracy: 1.0000 / loss: 0.3133 - test accuracy: 0.7143 / loss: 0.6350\n",
      "Starting fold 6 / 10 - training accuracy: 1.0000 / loss: 0.3134 - test accuracy: 0.7143 / loss: 0.6309\n",
      "Starting fold 7 / 10 - training accuracy: 1.0000 / loss: 0.3171 - test accuracy: 0.3571 / loss: 0.8311\n",
      "Starting fold 8 / 10 - training accuracy: 0.9020 / loss: 0.3861 - test accuracy: 0.5000 / loss: 0.8138\n",
      "Starting fold 9 / 10 - training accuracy: 0.9804 / loss: 0.3343 - test accuracy: 0.5714 / loss: 0.7356\n",
      "Starting fold 10 / 10 - training accuracy: 0.9608 / loss: 0.3531 - test accuracy: 0.5714 / loss: 0.6839\n",
      "average loss on test set: 0.7418\n",
      "average accuracy on test set: 0.5571\n"
     ]
    }
   ],
   "source": [
    "## hyperparameters\n",
    "lr = 0.01\n",
    "p_dropout = 0.05\n",
    "\n",
    "## define model\n",
    "indim = X_2D.shape[1]\n",
    "outdim = np.unique(y_dominant_experts[dominant_expert_idx]).shape[0]\n",
    "assert outdim == np.unique(y_dominant_crowd[dominant_crowd_idx]).shape[0]\n",
    "\n",
    "model = ClassifierNN(input_dim=indim,\n",
    "                     output_dim=outdim,\n",
    "                     p_dropout=p_dropout)\n",
    "\n",
    "print(\"=== Results on expert dominant labels ===\")\n",
    "neural_net_acc_dominant_experts = fit(model, X_2D, y_dominant_experts, dominant_expert_idx, lr=lr)\n",
    "\n",
    "print(\"\\n=== Results on crowd dominant labels ===\")\n",
    "neural_net_acc_dominant_crowd = fit(model, X_2D, y_dominant_crowd, dominant_crowd_idx, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierCNN(nn.Module):\n",
    "    \"\"\"CNN Classifier\"\"\"\n",
    "\n",
    "    def __init__(self, features_in, features_out, p_dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(features_in, int(features_in*1.5), kernel_size=7),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=3), \n",
    "\n",
    "            nn.Conv1d(int(features_in*1.5), int(features_in*2), kernel_size=7),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv1d(int(features_in*2), int(features_in*2), kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveMaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(int(features_in*2)*2, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_dropout),\n",
    "\n",
    "            nn.Linear(32, features_out)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.conv(X)\n",
    "        X = X.view(X.size(0), -1)\n",
    "\n",
    "        return self.softmax(self.fc(X))\n",
    "        \n",
    "    def init(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.normal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results on expert likert labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.2558 / loss: 1.9096 - test accuracy: 0.2000 / loss: 1.9654\n",
      "Starting fold 2 / 10 - training accuracy: 0.0930 / loss: 2.0724 - test accuracy: 0.1333 / loss: 2.0321\n",
      "Starting fold 3 / 10 - training accuracy: 0.1163 / loss: 2.0491 - test accuracy: 0.1333 / loss: 2.0321\n",
      "Starting fold 4 / 10 - training accuracy: 0.0930 / loss: 2.0724 - test accuracy: 0.1333 / loss: 2.0321\n",
      "Starting fold 5 / 10 - training accuracy: 0.2093 / loss: 1.9561 - test accuracy: 0.2000 / loss: 1.9654\n",
      "Starting fold 6 / 10 - training accuracy: 0.1395 / loss: 2.0259 - test accuracy: 0.1333 / loss: 2.0321\n",
      "Starting fold 7 / 10 - training accuracy: 0.1628 / loss: 2.0026 - test accuracy: 0.1333 / loss: 2.0321\n",
      "Starting fold 8 / 10 - training accuracy: 0.0000 / loss: 2.1654 - test accuracy: 0.0667 / loss: 2.0988\n",
      "Starting fold 9 / 10 - training accuracy: 0.1395 / loss: 2.0259 - test accuracy: 0.1333 / loss: 2.0321\n",
      "Starting fold 10 / 10 - training accuracy: 0.1163 / loss: 2.0491 - test accuracy: 0.1333 / loss: 2.0321\n",
      "average loss on test set: 2.0254\n",
      "average accuracy on test set: 0.1400\n",
      "\n",
      "=== Results on crowd likert labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.1304 / loss: 2.0350 - test accuracy: 0.1250 / loss: 2.0404\n",
      "Starting fold 2 / 10 - training accuracy: 0.3043 / loss: 1.8611 - test accuracy: 0.2500 / loss: 1.9154\n",
      "Starting fold 3 / 10 - training accuracy: 0.0217 / loss: 2.1437 - test accuracy: 0.0625 / loss: 2.1029\n",
      "Starting fold 4 / 10 - training accuracy: 0.3043 / loss: 1.8611 - test accuracy: 0.2500 / loss: 1.9154\n",
      "Starting fold 5 / 10 - training accuracy: 0.0000 / loss: 2.1654 - test accuracy: 0.0625 / loss: 2.1029\n",
      "Starting fold 6 / 10 - training accuracy: 0.3043 / loss: 1.8611 - test accuracy: 0.2500 / loss: 1.9154\n",
      "Starting fold 7 / 10 - training accuracy: 0.0217 / loss: 2.1437 - test accuracy: 0.0625 / loss: 2.1029\n",
      "Starting fold 8 / 10 - training accuracy: 0.0217 / loss: 2.1437 - test accuracy: 0.0625 / loss: 2.1029\n",
      "Starting fold 9 / 10 - training accuracy: 0.0217 / loss: 2.1437 - test accuracy: 0.0625 / loss: 2.1029\n",
      "Starting fold 10 / 10 - training accuracy: 0.3043 / loss: 1.8611 - test accuracy: 0.2500 / loss: 1.9154\n",
      "average loss on test set: 2.0217\n",
      "average accuracy on test set: 0.1437\n"
     ]
    }
   ],
   "source": [
    "## hyperparameters\n",
    "lr = 0.01\n",
    "p_dropout = 0.05\n",
    "\n",
    "## define model\n",
    "indim = X_3D.shape[2]\n",
    "outdim = np.unique(y_likert_experts[likert_expert_idx]).shape[0]\n",
    "assert outdim == np.unique(y_likert_crowd[likert_crowd_idx]).shape[0]\n",
    "\n",
    "model = ClassifierCNN(features_in=indim,\n",
    "                      features_out=outdim,\n",
    "                      p_dropout=p_dropout)\n",
    "\n",
    "print(\"=== Results on expert likert labels ===\")\n",
    "cnn_acc_likert_experts = fit(model, X_3D.transpose(1, 2), y_likert_experts, likert_expert_idx, lr=lr)\n",
    "\n",
    "print(\"\\n=== Results on crowd likert labels ===\")\n",
    "cnn_acc_likert_crowd = fit(model, X_3D.transpose(1, 2), y_likert_crowd, likert_crowd_idx, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results on expert dominant labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.6857 / loss: 1.4797 - test accuracy: 0.6667 / loss: 1.4988\n",
      "Starting fold 2 / 10 - training accuracy: 0.0000 / loss: 2.1654 - test accuracy: 0.0000 / loss: 2.1654\n",
      "Starting fold 3 / 10 - training accuracy: 0.0000 / loss: 2.1654 - test accuracy: 0.0000 / loss: 2.1654\n",
      "Starting fold 4 / 10 - training accuracy: 0.6857 / loss: 1.4797 - test accuracy: 0.6667 / loss: 1.4988\n",
      "Starting fold 5 / 10 - training accuracy: 0.6857 / loss: 1.4797 - test accuracy: 0.6667 / loss: 1.4988\n",
      "Starting fold 6 / 10 - training accuracy: 0.6857 / loss: 1.4797 - test accuracy: 0.6667 / loss: 1.4988\n",
      "Starting fold 7 / 10 - training accuracy: 0.6857 / loss: 1.4797 - test accuracy: 0.6667 / loss: 1.4988\n",
      "Starting fold 8 / 10 - training accuracy: 0.6857 / loss: 1.4797 - test accuracy: 0.6667 / loss: 1.4988\n",
      "Starting fold 9 / 10 - training accuracy: 0.0000 / loss: 2.1654 - test accuracy: 0.0000 / loss: 2.1654\n",
      "Starting fold 10 / 10 - training accuracy: 0.0000 / loss: 2.1654 - test accuracy: 0.0000 / loss: 2.1654\n",
      "average loss on test set: 1.7654\n",
      "average accuracy on test set: 0.4000\n",
      "\n",
      "=== Results on crowd dominant labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.6078 / loss: 1.5576 - test accuracy: 0.5714 / loss: 1.5940\n",
      "Starting fold 2 / 10 - training accuracy: 0.0000 / loss: 2.1654 - test accuracy: 0.0000 / loss: 2.1654\n",
      "Starting fold 3 / 10 - training accuracy: 0.6078 / loss: 1.5576 - test accuracy: 0.5714 / loss: 1.5940\n",
      "Starting fold 4 / 10 - training accuracy: 0.3922 / loss: 1.7733 - test accuracy: 0.4286 / loss: 1.7369\n",
      "Starting fold 5 / 10 - training accuracy: 0.5882 / loss: 1.5772 - test accuracy: 0.5714 / loss: 1.5940\n",
      "Starting fold 6 / 10 - training accuracy: 0.0000 / loss: 2.1654 - test accuracy: 0.0000 / loss: 2.1654\n",
      "Starting fold 7 / 10 - training accuracy: 0.0000 / loss: 2.1654 - test accuracy: 0.0000 / loss: 2.1654\n",
      "Starting fold 8 / 10 - training accuracy: 0.0000 / loss: 2.1654 - test accuracy: 0.0000 / loss: 2.1654\n",
      "Starting fold 9 / 10 - training accuracy: 0.6078 / loss: 1.5576 - test accuracy: 0.5714 / loss: 1.5940\n",
      "Starting fold 10 / 10 - training accuracy: 0.0000 / loss: 2.1654 - test accuracy: 0.0000 / loss: 2.1654\n",
      "average loss on test set: 1.8940\n",
      "average accuracy on test set: 0.2714\n"
     ]
    }
   ],
   "source": [
    "## hyperparameters\n",
    "lr = 0.01\n",
    "p_dropout = 0.05\n",
    "\n",
    "## define model\n",
    "indim = X_3D.shape[2]\n",
    "outdim = np.unique(y_likert_experts[likert_expert_idx]).shape[0]\n",
    "assert outdim == np.unique(y_likert_crowd[likert_crowd_idx]).shape[0]\n",
    "\n",
    "model = ClassifierCNN(features_in=indim,\n",
    "                      features_out=outdim,\n",
    "                      p_dropout=p_dropout)\n",
    "\n",
    "print(\"=== Results on expert dominant labels ===\")\n",
    "cnn_acc_dominant_experts = fit(model, X_3D.transpose(1, 2), y_dominant_experts, dominant_expert_idx, lr=lr)\n",
    "\n",
    "print(\"\\n=== Results on crowd dominant labels ===\")\n",
    "cnn_acc_dominant_crowd = fit(model, X_3D.transpose(1, 2), y_dominant_crowd, dominant_crowd_idx, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierLSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 hidden_dim,\n",
    "                 num_layers=1,\n",
    "                 p_dropout=0.0):\n",
    "        \"\"\"\n",
    "        LSTM\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            bias=True,\n",
    "                            batch_first=True)  # (batch, seq, feature)\n",
    "                            \n",
    "        fc_hidden_dim = (hidden_dim-output_dim)//2\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_dim, fc_hidden_dim),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Dropout(p=p_dropout),\n",
    "                                \n",
    "                                nn.Linear(fc_hidden_dim, output_dim))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # default H0 is zero vector\n",
    "        # output Hn is representation of entire sequence\n",
    "        X, _ = self.lstm(X)\n",
    "        X = X[:,-1,:]  # only consider final output\n",
    "\n",
    "        return self.softmax(self.fc(X))\n",
    "\n",
    "    def init(self):\n",
    "        sqrt_k = sqrt(1.0/self.hidden_dim)\n",
    "        for param in self.parameters():\n",
    "            nn.init.uniform_(param, -sqrt_k, sqrt_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results on expert likert labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.2791 / loss: 1.8700 - test accuracy: 0.2000 / loss: 1.9163\n",
      "Starting fold 2 / 10 - training accuracy: 0.2791 / loss: 1.8688 - test accuracy: 0.2000 / loss: 1.9160\n",
      "Starting fold 3 / 10 - training accuracy: 0.2558 / loss: 1.8711 - test accuracy: 0.2000 / loss: 1.9177\n",
      "Starting fold 4 / 10 - training accuracy: 0.2558 / loss: 1.8734 - test accuracy: 0.2000 / loss: 1.9168\n",
      "Starting fold 5 / 10 - training accuracy: 0.3023 / loss: 1.8692 - test accuracy: 0.2000 / loss: 1.9161\n",
      "Starting fold 6 / 10 - training accuracy: 0.1860 / loss: 1.8842 - test accuracy: 0.2000 / loss: 1.9160\n",
      "Starting fold 7 / 10 - training accuracy: 0.3023 / loss: 1.8693 - test accuracy: 0.2000 / loss: 1.9161\n",
      "Starting fold 8 / 10 - training accuracy: 0.2558 / loss: 1.8737 - test accuracy: 0.2000 / loss: 1.9167\n",
      "Starting fold 9 / 10 - training accuracy: 0.2558 / loss: 1.8719 - test accuracy: 0.2000 / loss: 1.9160\n",
      "Starting fold 10 / 10 - training accuracy: 0.2093 / loss: 1.8802 - test accuracy: 0.2000 / loss: 1.9161\n",
      "average loss on test set: 1.9164\n",
      "average accuracy on test set: 0.2000\n",
      "\n",
      "=== Results on crowd likert labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.3261 / loss: 1.8092 - test accuracy: 0.2500 / loss: 1.8663\n",
      "Starting fold 2 / 10 - training accuracy: 0.3043 / loss: 1.8153 - test accuracy: 0.2500 / loss: 1.8667\n",
      "Starting fold 3 / 10 - training accuracy: 0.3261 / loss: 1.8095 - test accuracy: 0.2500 / loss: 1.8659\n",
      "Starting fold 4 / 10 - training accuracy: 0.3043 / loss: 1.8138 - test accuracy: 0.2500 / loss: 1.8660\n",
      "Starting fold 5 / 10 - training accuracy: 0.3043 / loss: 1.8036 - test accuracy: 0.2500 / loss: 1.8662\n",
      "Starting fold 6 / 10 - training accuracy: 0.3043 / loss: 1.8184 - test accuracy: 0.2500 / loss: 1.8669\n",
      "Starting fold 7 / 10 - training accuracy: 0.3043 / loss: 1.8219 - test accuracy: 0.2500 / loss: 1.8678\n",
      "Starting fold 8 / 10 - training accuracy: 0.2609 / loss: 1.8169 - test accuracy: 0.2500 / loss: 1.8664\n",
      "Starting fold 9 / 10 - training accuracy: 0.2826 / loss: 1.8157 - test accuracy: 0.2500 / loss: 1.8665\n",
      "Starting fold 10 / 10 - training accuracy: 0.2826 / loss: 1.8157 - test accuracy: 0.2500 / loss: 1.8667\n",
      "average loss on test set: 1.8665\n",
      "average accuracy on test set: 0.2500\n"
     ]
    }
   ],
   "source": [
    "## hyperparameters\n",
    "lr = 0.01\n",
    "p_dropout = 0.05\n",
    "\n",
    "## define model\n",
    "indim = X_3D.shape[2]\n",
    "outdim = np.unique(y_likert_experts[likert_expert_idx]).shape[0]\n",
    "assert outdim == np.unique(y_likert_crowd[likert_crowd_idx]).shape[0]\n",
    "hidden_dim = (indim-outdim)//2\n",
    "\n",
    "model = ClassifierLSTM(input_dim=indim,\n",
    "                       output_dim=outdim,\n",
    "                       hidden_dim=hidden_dim,\n",
    "                       p_dropout=p_dropout)\n",
    "\n",
    "print(\"=== Results on expert likert labels ===\")\n",
    "lstm_acc_likert_experts = fit(model, X_3D, y_likert_experts, likert_expert_idx, lr=lr)\n",
    "\n",
    "print(\"\\n=== Results on crowd likert labels ===\")\n",
    "lstm_acc_likert_crowd = fit(model, X_3D, y_likert_crowd, likert_crowd_idx, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results on expert dominant labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.6857 / loss: 1.4797 - test accuracy: 0.6667 / loss: 1.4988\n",
      "Starting fold 2 / 10 - training accuracy: 0.6857 / loss: 1.4797 - test accuracy: 0.6667 / loss: 1.4988\n",
      "Starting fold 3 / 10 - training accuracy: 0.6857 / loss: 1.4828 - test accuracy: 0.6667 / loss: 1.4988\n",
      "Starting fold 4 / 10 - training accuracy: 0.6857 / loss: 1.4802 - test accuracy: 0.6667 / loss: 1.4988\n",
      "Starting fold 5 / 10 - training accuracy: 0.6857 / loss: 1.4797 - test accuracy: 0.6667 / loss: 1.4988\n",
      "Starting fold 6 / 10 - training accuracy: 0.6857 / loss: 1.4797 - test accuracy: 0.6667 / loss: 1.4988\n",
      "Starting fold 7 / 10 - training accuracy: 0.6857 / loss: 1.4807 - test accuracy: 0.6667 / loss: 1.4988\n",
      "Starting fold 8 / 10 - training accuracy: 0.6857 / loss: 1.4807 - test accuracy: 0.6667 / loss: 1.4990\n",
      "Starting fold 9 / 10 - training accuracy: 0.6857 / loss: 1.4797 - test accuracy: 0.6667 / loss: 1.4989\n",
      "Starting fold 10 / 10 - training accuracy: 0.6857 / loss: 1.4797 - test accuracy: 0.6667 / loss: 1.4997\n",
      "average loss on test set: 1.4989\n",
      "average accuracy on test set: 0.6667\n",
      "\n",
      "=== Results on crowd dominant labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.6078 / loss: 1.5599 - test accuracy: 0.5714 / loss: 1.5940\n",
      "Starting fold 2 / 10 - training accuracy: 0.6078 / loss: 1.5576 - test accuracy: 0.5714 / loss: 1.5940\n",
      "Starting fold 3 / 10 - training accuracy: 0.6078 / loss: 1.5576 - test accuracy: 0.5714 / loss: 1.5940\n",
      "Starting fold 4 / 10 - training accuracy: 0.6078 / loss: 1.5576 - test accuracy: 0.5714 / loss: 1.5940\n",
      "Starting fold 5 / 10 - training accuracy: 0.6078 / loss: 1.5576 - test accuracy: 0.5714 / loss: 1.5940\n",
      "Starting fold 6 / 10 - training accuracy: 0.6078 / loss: 1.5576 - test accuracy: 0.5714 / loss: 1.5940\n",
      "Starting fold 7 / 10 - training accuracy: 0.6078 / loss: 1.5576 - test accuracy: 0.5714 / loss: 1.5940\n",
      "Starting fold 8 / 10 - training accuracy: 0.6078 / loss: 1.5583 - test accuracy: 0.5714 / loss: 1.5940\n",
      "Starting fold 9 / 10 - training accuracy: 0.6078 / loss: 1.5577 - test accuracy: 0.5714 / loss: 1.5940\n",
      "Starting fold 10 / 10 - training accuracy: 0.6078 / loss: 1.5576 - test accuracy: 0.5714 / loss: 1.5940\n",
      "average loss on test set: 1.5940\n",
      "average accuracy on test set: 0.5714\n"
     ]
    }
   ],
   "source": [
    "## hyperparameters\n",
    "lr = 0.01\n",
    "p_dropout = 0.05\n",
    "\n",
    "## define model\n",
    "indim = X_3D.shape[2]\n",
    "outdim = np.unique(y_likert_experts[likert_expert_idx]).shape[0]\n",
    "assert outdim == np.unique(y_likert_crowd[likert_crowd_idx]).shape[0]\n",
    "\n",
    "model = ClassifierLSTM(input_dim=indim,\n",
    "                       output_dim=outdim,\n",
    "                       hidden_dim=indim,\n",
    "                       p_dropout=p_dropout)\n",
    "\n",
    "print(\"=== Results on expert dominant labels ===\")\n",
    "lstm_acc_dominant_experts = fit(model, X_3D, y_dominant_experts, dominant_expert_idx, lr=lr)\n",
    "\n",
    "print(\"\\n=== Results on crowd dominant labels ===\")\n",
    "lstm_acc_dominant_crowd = fit(model, X_3D, y_dominant_crowd, dominant_crowd_idx, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
