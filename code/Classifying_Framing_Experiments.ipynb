{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install sklearn\n",
    "#%pip install torch\n",
    "\n",
    "from math import sqrt\n",
    "import os\n",
    "from time import time\n",
    "import random\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "## project structure\n",
    "DATA_DIR = \"/data/projects/capturingBias/research/framing/data/\"  # change to \"./\" for current directory\n",
    "DATA_NPZ = DATA_DIR + \"data.npz\"\n",
    "\n",
    "## load files\n",
    "data = np.load(DATA_NPZ)\n",
    "\n",
    "X_2D = data['X_2D']\n",
    "X_3D = data['X_3D']\n",
    "y_likert_crowd = data['y_likert_crowd']\n",
    "y_likert_experts = data['y_likert_experts']\n",
    "y_dominant_crowd = data['y_dominant_crowd']\n",
    "y_dominant_experts = data['y_dominant_experts']\n",
    "y_likert_combined = data['y_likert_combined']\n",
    "y_dominant_combined = data['y_dominant_combined']\n",
    "\n",
    "\n",
    "# likert\n",
    "likert_expert_idx = np.where(y_likert_experts > -1)[0]\n",
    "likert_crowd_idx = np.setdiff1d(np.where(y_likert_crowd > -1)[0],\n",
    "                                likert_expert_idx,\n",
    "                                assume_unique=True)\n",
    "likert_combined_idx = np.concatenate([likert_crowd_idx,\n",
    "                                      likert_expert_idx])\n",
    "\n",
    "# dominant\n",
    "dominant_expert_idx = np.where(y_dominant_experts > -1)[0]\n",
    "dominant_crowd_idx = np.setdiff1d(np.where(y_dominant_crowd > -1)[0],\n",
    "                                  dominant_expert_idx,\n",
    "                                  assume_unique=True)\n",
    "dominant_combined_idx = np.concatenate([dominant_crowd_idx,\n",
    "                                        dominant_expert_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=-1):\n",
    "    if seed < 0:\n",
    "        seed = np.random.randint(0, 2**32-1)\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seed(47)  # make reproducable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(y):\n",
    "    train_idx = list()\n",
    "    test_idx = list()\n",
    "    \n",
    "    strats = [np.where(y == lab)[0] for lab in np.unique(y) if lab > -1]\n",
    "    for strat in strats:\n",
    "        n = strat.shape[0]\n",
    "        train_idx.append(strat[:int(n*0.8)])\n",
    "        test_idx.append(strat[int(n*0.8):])\n",
    "        \n",
    "    train_idx = np.concatenate(train_idx)\n",
    "    test_idx = np.concatenate(test_idx)\n",
    "    \n",
    "    np.random.shuffle(train_idx)\n",
    "    np.random.shuffle(test_idx)\n",
    "    \n",
    "    return (train_idx, test_idx)\n",
    "\n",
    "def create_splits_one_hot(y):\n",
    "    vec = -np.ones(y.shape[0])\n",
    "    nonzero = y.nonzero()\n",
    "    vec[nonzero[:,0]] = nonzero[:,1].float()\n",
    "    \n",
    "    return create_splits(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def majority_class(y):\n",
    "    ct = Counter(y)\n",
    "    return ct.most_common(1)[0][1] / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority class accuracy on Likert labels (baseline)\n",
      " crowd labels:  0.3226\n",
      " expert labels: 0.2414\n",
      " combined labels: 0.2500\n",
      "\n",
      "Majority class accuracy on Dominant labels (baseline)\n",
      " crowd labels:  0.5312\n",
      " expert labels: 0.6591\n",
      " combined labels: 0.5833\n"
     ]
    }
   ],
   "source": [
    "majority_class_acc_crowd_likert = majority_class(y_likert_crowd[likert_crowd_idx])\n",
    "majority_class_acc_experts_likert = majority_class(y_likert_experts[likert_expert_idx])\n",
    "majority_class_acc_combined_likert = majority_class(y_likert_combined[likert_combined_idx])\n",
    "\n",
    "print(\"Majority class accuracy on Likert labels (baseline)\")\n",
    "print(\" crowd labels:  {:.4f}\".format(majority_class_acc_crowd_likert))\n",
    "print(\" expert labels: {:.4f}\".format(majority_class_acc_experts_likert))\n",
    "print(\" combined labels: {:.4f}\".format(majority_class_acc_combined_likert))\n",
    "\n",
    "majority_class_acc_crowd_dominant = majority_class(y_dominant_crowd[dominant_crowd_idx])\n",
    "majority_class_acc_experts_dominant = majority_class(y_dominant_experts[dominant_expert_idx])\n",
    "majority_class_acc_combined_dominant = majority_class(y_dominant_combined[dominant_combined_idx])\n",
    "\n",
    "print(\"\\nMajority class accuracy on Dominant labels (baseline)\")\n",
    "print(\" crowd labels:  {:.4f}\".format(majority_class_acc_crowd_dominant))\n",
    "print(\" expert labels: {:.4f}\".format(majority_class_acc_experts_dominant))\n",
    "print(\" combined labels: {:.4f}\".format(majority_class_acc_combined_dominant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest (supervised)\n",
    "\n",
    "We start with a traditional, or 'shallow', machine learning model: random forest. Because random forest does not support iterative learning, we test both the crowd and expert sets separately.\n",
    "\n",
    "We use stratified cross validation to reduce the effects caused by the small size of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "N_ESTIMATORS = [100, 250, 500, 750, 1000, 2000]\n",
    "N_FOLDS = 10\n",
    "\n",
    "def random_forest(X, y, index, n_folds=N_FOLDS, n_estimators=N_ESTIMATORS):\n",
    "    n_samples = X[index].shape[0]\n",
    "    for n_estimators in N_ESTIMATORS:\n",
    "        print(\"Training with {} estimators\".format(n_estimators))\n",
    "        acc = 0\n",
    "        for fold_i in range(N_FOLDS):\n",
    "            print(\" Starting fold {} / {}\".format(fold_i+1, N_FOLDS), end='')\n",
    "            \n",
    "            train_fold_idx, test_fold_idx  = create_splits(y[index])\n",
    "            train_idx = index[train_fold_idx]\n",
    "            test_idx = index[test_fold_idx]\n",
    "        \n",
    "            model = RandomForestClassifier(n_estimators=n_estimators)\n",
    "            model.fit(X[train_idx], y[train_idx])\n",
    "            \n",
    "            y_pred = model.predict(X[test_idx])\n",
    "            fold_acc = accuracy_score(y[test_idx], y_pred)\n",
    "        \n",
    "            acc += fold_acc\n",
    "            print(\" (acc: {:.4f})\".format(fold_acc))\n",
    "            \n",
    "        acc /= N_FOLDS\n",
    "        print(\"Mean accuracy on test set: {:.4f}\\n\".format(acc))\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results of supervised learning on expert likert labels ===\n",
      "Training with 100 estimators\n",
      " Starting fold 1 / 10 (acc: 0.2667)\n",
      " Starting fold 2 / 10 (acc: 0.3333)\n",
      " Starting fold 3 / 10 (acc: 0.2667)\n",
      " Starting fold 4 / 10 (acc: 0.2000)\n",
      " Starting fold 5 / 10 (acc: 0.2000)\n",
      " Starting fold 6 / 10 (acc: 0.2000)\n",
      " Starting fold 7 / 10 (acc: 0.2000)\n",
      " Starting fold 8 / 10 (acc: 0.2000)\n",
      " Starting fold 9 / 10 (acc: 0.2667)\n",
      " Starting fold 10 / 10 (acc: 0.2000)\n",
      "Mean accuracy on test set: 0.2333\n",
      "\n",
      "Training with 250 estimators\n",
      " Starting fold 1 / 10 (acc: 0.2000)\n",
      " Starting fold 2 / 10 (acc: 0.2000)\n",
      " Starting fold 3 / 10 (acc: 0.2000)\n",
      " Starting fold 4 / 10 (acc: 0.2000)\n",
      " Starting fold 5 / 10 (acc: 0.2000)\n",
      " Starting fold 6 / 10 (acc: 0.1333)\n",
      " Starting fold 7 / 10 (acc: 0.3333)\n",
      " Starting fold 8 / 10 (acc: 0.1333)\n",
      " Starting fold 9 / 10 (acc: 0.2000)\n",
      " Starting fold 10 / 10 (acc: 0.2000)\n",
      "Mean accuracy on test set: 0.2000\n",
      "\n",
      "Training with 500 estimators\n",
      " Starting fold 1 / 10 (acc: 0.2667)\n",
      " Starting fold 2 / 10 (acc: 0.2000)\n",
      " Starting fold 3 / 10 (acc: 0.2000)\n",
      " Starting fold 4 / 10 (acc: 0.2667)\n",
      " Starting fold 5 / 10 (acc: 0.2000)\n",
      " Starting fold 6 / 10 (acc: 0.2000)\n",
      " Starting fold 7 / 10 (acc: 0.2000)\n",
      " Starting fold 8 / 10 (acc: 0.2000)\n",
      " Starting fold 9 / 10 (acc: 0.2667)\n",
      " Starting fold 10 / 10 (acc: 0.2000)\n",
      "Mean accuracy on test set: 0.2200\n",
      "\n",
      "Training with 750 estimators\n",
      " Starting fold 1 / 10 (acc: 0.2667)\n",
      " Starting fold 2 / 10 (acc: 0.2000)\n",
      " Starting fold 3 / 10 (acc: 0.2000)\n",
      " Starting fold 4 / 10 (acc: 0.2000)\n",
      " Starting fold 5 / 10 (acc: 0.2000)\n",
      " Starting fold 6 / 10 (acc: 0.2000)\n",
      " Starting fold 7 / 10 (acc: 0.2000)\n",
      " Starting fold 8 / 10 (acc: 0.2000)\n",
      " Starting fold 9 / 10 (acc: 0.2000)\n",
      " Starting fold 10 / 10 (acc: 0.2000)\n",
      "Mean accuracy on test set: 0.2067\n",
      "\n",
      "Training with 1000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.2000)\n",
      " Starting fold 2 / 10 (acc: 0.2000)\n",
      " Starting fold 3 / 10 (acc: 0.2000)\n",
      " Starting fold 4 / 10 (acc: 0.2000)\n",
      " Starting fold 5 / 10 (acc: 0.2000)\n",
      " Starting fold 6 / 10 (acc: 0.2000)\n",
      " Starting fold 7 / 10 (acc: 0.2000)\n",
      " Starting fold 8 / 10 (acc: 0.2000)\n",
      " Starting fold 9 / 10 (acc: 0.2000)\n",
      " Starting fold 10 / 10 (acc: 0.2000)\n",
      "Mean accuracy on test set: 0.2000\n",
      "\n",
      "Training with 2000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.2000)\n",
      " Starting fold 2 / 10 (acc: 0.2000)\n",
      " Starting fold 3 / 10 (acc: 0.2000)\n",
      " Starting fold 4 / 10 (acc: 0.2000)\n",
      " Starting fold 5 / 10 (acc: 0.2000)\n",
      " Starting fold 6 / 10 (acc: 0.2000)\n",
      " Starting fold 7 / 10 (acc: 0.2000)\n",
      " Starting fold 8 / 10 (acc: 0.2000)\n",
      " Starting fold 9 / 10 (acc: 0.2000)\n",
      " Starting fold 10 / 10 (acc: 0.2000)\n",
      "Mean accuracy on test set: 0.2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Results of supervised learning on expert likert labels ===\")\n",
    "random_forest_acc_experts_likert = random_forest(X_2D,\n",
    "                                                 y_likert_experts, \n",
    "                                                 likert_expert_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results of supervised learning on crowd likert labels ===\n",
      "Training with 100 estimators\n",
      " Starting fold 1 / 10 (acc: 0.3333)\n",
      " Starting fold 2 / 10 (acc: 0.2000)\n",
      " Starting fold 3 / 10 (acc: 0.3333)\n",
      " Starting fold 4 / 10 (acc: 0.3333)\n",
      " Starting fold 5 / 10 (acc: 0.3333)\n",
      " Starting fold 6 / 10 (acc: 0.3333)\n",
      " Starting fold 7 / 10 (acc: 0.3333)\n",
      " Starting fold 8 / 10 (acc: 0.2667)\n",
      " Starting fold 9 / 10 (acc: 0.3333)\n",
      " Starting fold 10 / 10 (acc: 0.2667)\n",
      "Mean accuracy on test set: 0.3067\n",
      "\n",
      "Training with 250 estimators\n",
      " Starting fold 1 / 10 (acc: 0.2667)\n",
      " Starting fold 2 / 10 (acc: 0.3333)\n",
      " Starting fold 3 / 10 (acc: 0.2667)\n",
      " Starting fold 4 / 10 (acc: 0.3333)\n",
      " Starting fold 5 / 10 (acc: 0.3333)\n",
      " Starting fold 6 / 10 (acc: 0.3333)\n",
      " Starting fold 7 / 10 (acc: 0.3333)\n",
      " Starting fold 8 / 10 (acc: 0.3333)\n",
      " Starting fold 9 / 10 (acc: 0.3333)\n",
      " Starting fold 10 / 10 (acc: 0.3333)\n",
      "Mean accuracy on test set: 0.3200\n",
      "\n",
      "Training with 500 estimators\n",
      " Starting fold 1 / 10 (acc: 0.3333)\n",
      " Starting fold 2 / 10 (acc: 0.3333)\n",
      " Starting fold 3 / 10 (acc: 0.3333)\n",
      " Starting fold 4 / 10 (acc: 0.3333)\n",
      " Starting fold 5 / 10 (acc: 0.2667)\n",
      " Starting fold 6 / 10 (acc: 0.3333)\n",
      " Starting fold 7 / 10 (acc: 0.3333)\n",
      " Starting fold 8 / 10 (acc: 0.3333)\n",
      " Starting fold 9 / 10 (acc: 0.3333)\n",
      " Starting fold 10 / 10 (acc: 0.3333)\n",
      "Mean accuracy on test set: 0.3267\n",
      "\n",
      "Training with 750 estimators\n",
      " Starting fold 1 / 10 (acc: 0.3333)\n",
      " Starting fold 2 / 10 (acc: 0.3333)\n",
      " Starting fold 3 / 10 (acc: 0.3333)\n",
      " Starting fold 4 / 10 (acc: 0.2667)\n",
      " Starting fold 5 / 10 (acc: 0.3333)\n",
      " Starting fold 6 / 10 (acc: 0.3333)\n",
      " Starting fold 7 / 10 (acc: 0.3333)\n",
      " Starting fold 8 / 10 (acc: 0.3333)\n",
      " Starting fold 9 / 10 (acc: 0.3333)\n",
      " Starting fold 10 / 10 (acc: 0.3333)\n",
      "Mean accuracy on test set: 0.3267\n",
      "\n",
      "Training with 1000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.3333)\n",
      " Starting fold 2 / 10 (acc: 0.3333)\n",
      " Starting fold 3 / 10 (acc: 0.3333)\n",
      " Starting fold 4 / 10 (acc: 0.3333)\n",
      " Starting fold 5 / 10 (acc: 0.3333)\n",
      " Starting fold 6 / 10 (acc: 0.3333)\n",
      " Starting fold 7 / 10 (acc: 0.3333)\n",
      " Starting fold 8 / 10 (acc: 0.3333)\n",
      " Starting fold 9 / 10 (acc: 0.3333)\n",
      " Starting fold 10 / 10 (acc: 0.2667)\n",
      "Mean accuracy on test set: 0.3267\n",
      "\n",
      "Training with 2000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.3333)\n",
      " Starting fold 2 / 10 (acc: 0.3333)\n",
      " Starting fold 3 / 10 (acc: 0.3333)\n",
      " Starting fold 4 / 10 (acc: 0.3333)\n",
      " Starting fold 5 / 10 (acc: 0.3333)\n",
      " Starting fold 6 / 10 (acc: 0.3333)\n",
      " Starting fold 7 / 10 (acc: 0.3333)\n",
      " Starting fold 8 / 10 (acc: 0.3333)\n",
      " Starting fold 9 / 10 (acc: 0.3333)\n",
      " Starting fold 10 / 10 (acc: 0.3333)\n",
      "Mean accuracy on test set: 0.3333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Results of supervised learning on crowd likert labels ===\")\n",
    "random_forest_acc_crowd_likert = random_forest(X_2D,\n",
    "                                               y_likert_crowd,\n",
    "                                               likert_crowd_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results of supervised learning on combined likert labels ===\n",
      "Training with 100 estimators\n",
      " Starting fold 1 / 10 (acc: 0.2692)\n",
      " Starting fold 2 / 10 (acc: 0.4231)\n",
      " Starting fold 3 / 10 (acc: 0.3077)\n",
      " Starting fold 4 / 10 (acc: 0.2308)\n",
      " Starting fold 5 / 10 (acc: 0.2692)\n",
      " Starting fold 6 / 10 (acc: 0.2308)\n",
      " Starting fold 7 / 10 (acc: 0.3077)\n",
      " Starting fold 8 / 10 (acc: 0.2692)\n",
      " Starting fold 9 / 10 (acc: 0.2308)\n",
      " Starting fold 10 / 10 (acc: 0.3462)\n",
      "Mean accuracy on test set: 0.2885\n",
      "\n",
      "Training with 250 estimators\n",
      " Starting fold 1 / 10 (acc: 0.3077)\n",
      " Starting fold 2 / 10 (acc: 0.2308)\n",
      " Starting fold 3 / 10 (acc: 0.3077)\n",
      " Starting fold 4 / 10 (acc: 0.3077)\n",
      " Starting fold 5 / 10 (acc: 0.2692)\n",
      " Starting fold 6 / 10 (acc: 0.2308)\n",
      " Starting fold 7 / 10 (acc: 0.2308)\n",
      " Starting fold 8 / 10 (acc: 0.3077)\n",
      " Starting fold 9 / 10 (acc: 0.2692)\n",
      " Starting fold 10 / 10 (acc: 0.2308)\n",
      "Mean accuracy on test set: 0.2692\n",
      "\n",
      "Training with 500 estimators\n",
      " Starting fold 1 / 10 (acc: 0.2692)\n",
      " Starting fold 2 / 10 (acc: 0.2692)\n",
      " Starting fold 3 / 10 (acc: 0.2692)\n",
      " Starting fold 4 / 10 (acc: 0.3462)\n",
      " Starting fold 5 / 10 (acc: 0.2308)\n",
      " Starting fold 6 / 10 (acc: 0.2692)\n",
      " Starting fold 7 / 10 (acc: 0.2692)\n",
      " Starting fold 8 / 10 (acc: 0.2692)\n",
      " Starting fold 9 / 10 (acc: 0.3077)\n",
      " Starting fold 10 / 10 (acc: 0.3462)\n",
      "Mean accuracy on test set: 0.2846\n",
      "\n",
      "Training with 750 estimators\n",
      " Starting fold 1 / 10 (acc: 0.2692)\n",
      " Starting fold 2 / 10 (acc: 0.2308)\n",
      " Starting fold 3 / 10 (acc: 0.3077)\n",
      " Starting fold 4 / 10 (acc: 0.4231)\n",
      " Starting fold 5 / 10 (acc: 0.2692)\n",
      " Starting fold 6 / 10 (acc: 0.3462)\n",
      " Starting fold 7 / 10 (acc: 0.2692)\n",
      " Starting fold 8 / 10 (acc: 0.2308)\n",
      " Starting fold 9 / 10 (acc: 0.2692)\n",
      " Starting fold 10 / 10 (acc: 0.3462)\n",
      "Mean accuracy on test set: 0.2962\n",
      "\n",
      "Training with 1000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.2692)\n",
      " Starting fold 2 / 10 (acc: 0.2308)\n",
      " Starting fold 3 / 10 (acc: 0.3077)\n",
      " Starting fold 4 / 10 (acc: 0.3077)\n",
      " Starting fold 5 / 10 (acc: 0.3077)\n",
      " Starting fold 6 / 10 (acc: 0.2692)\n",
      " Starting fold 7 / 10 (acc: 0.2692)\n",
      " Starting fold 8 / 10 (acc: 0.2692)\n",
      " Starting fold 9 / 10 (acc: 0.3077)\n",
      " Starting fold 10 / 10 (acc: 0.2692)\n",
      "Mean accuracy on test set: 0.2808\n",
      "\n",
      "Training with 2000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.2692)\n",
      " Starting fold 2 / 10 (acc: 0.2692)\n",
      " Starting fold 3 / 10 (acc: 0.2308)\n",
      " Starting fold 4 / 10 (acc: 0.2308)\n",
      " Starting fold 5 / 10 (acc: 0.3077)\n",
      " Starting fold 6 / 10 (acc: 0.3077)\n",
      " Starting fold 7 / 10 (acc: 0.2692)\n",
      " Starting fold 8 / 10 (acc: 0.3077)\n",
      " Starting fold 9 / 10 (acc: 0.3077)\n",
      " Starting fold 10 / 10 (acc: 0.2692)\n",
      "Mean accuracy on test set: 0.2769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Results of supervised learning on combined likert labels ===\")\n",
    "random_forest_acc_combined_likert = random_forest(X_2D,\n",
    "                                                  y_likert_combined,\n",
    "                                                  likert_combined_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results of supervised learning on expert dominant labels ===\n",
      "Training with 100 estimators\n",
      " Starting fold 1 / 10 (acc: 0.6667)\n",
      " Starting fold 2 / 10 (acc: 0.6667)\n",
      " Starting fold 3 / 10 (acc: 0.8889)\n",
      " Starting fold 4 / 10 (acc: 0.6667)\n",
      " Starting fold 5 / 10 (acc: 0.6667)\n",
      " Starting fold 6 / 10 (acc: 0.8889)\n",
      " Starting fold 7 / 10 (acc: 0.7778)\n",
      " Starting fold 8 / 10 (acc: 0.7778)\n",
      " Starting fold 9 / 10 (acc: 0.6667)\n",
      " Starting fold 10 / 10 (acc: 0.7778)\n",
      "Mean accuracy on test set: 0.7444\n",
      "\n",
      "Training with 250 estimators\n",
      " Starting fold 1 / 10 (acc: 0.6667)\n",
      " Starting fold 2 / 10 (acc: 0.8889)\n",
      " Starting fold 3 / 10 (acc: 0.6667)\n",
      " Starting fold 4 / 10 (acc: 0.8889)\n",
      " Starting fold 5 / 10 (acc: 0.6667)\n",
      " Starting fold 6 / 10 (acc: 0.7778)\n",
      " Starting fold 7 / 10 (acc: 0.7778)\n",
      " Starting fold 8 / 10 (acc: 0.6667)\n",
      " Starting fold 9 / 10 (acc: 0.5556)\n",
      " Starting fold 10 / 10 (acc: 0.7778)\n",
      "Mean accuracy on test set: 0.7333\n",
      "\n",
      "Training with 500 estimators\n",
      " Starting fold 1 / 10 (acc: 0.7778)\n",
      " Starting fold 2 / 10 (acc: 0.8889)\n",
      " Starting fold 3 / 10 (acc: 0.7778)\n",
      " Starting fold 4 / 10 (acc: 0.8889)\n",
      " Starting fold 5 / 10 (acc: 0.6667)\n",
      " Starting fold 6 / 10 (acc: 0.7778)\n",
      " Starting fold 7 / 10 (acc: 0.7778)\n",
      " Starting fold 8 / 10 (acc: 0.6667)\n",
      " Starting fold 9 / 10 (acc: 0.7778)\n",
      " Starting fold 10 / 10 (acc: 0.6667)\n",
      "Mean accuracy on test set: 0.7667\n",
      "\n",
      "Training with 750 estimators\n",
      " Starting fold 1 / 10 (acc: 0.7778)\n",
      " Starting fold 2 / 10 (acc: 0.7778)\n",
      " Starting fold 3 / 10 (acc: 0.8889)\n",
      " Starting fold 4 / 10 (acc: 0.7778)\n",
      " Starting fold 5 / 10 (acc: 0.7778)\n",
      " Starting fold 6 / 10 (acc: 0.6667)\n",
      " Starting fold 7 / 10 (acc: 0.6667)\n",
      " Starting fold 8 / 10 (acc: 0.7778)\n",
      " Starting fold 9 / 10 (acc: 0.6667)\n",
      " Starting fold 10 / 10 (acc: 0.6667)\n",
      "Mean accuracy on test set: 0.7444\n",
      "\n",
      "Training with 1000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.7778)\n",
      " Starting fold 2 / 10 (acc: 0.8889)\n",
      " Starting fold 3 / 10 (acc: 0.8889)\n",
      " Starting fold 4 / 10 (acc: 0.7778)\n",
      " Starting fold 5 / 10 (acc: 0.7778)\n",
      " Starting fold 6 / 10 (acc: 0.6667)\n",
      " Starting fold 7 / 10 (acc: 0.6667)\n",
      " Starting fold 8 / 10 (acc: 0.7778)\n",
      " Starting fold 9 / 10 (acc: 0.8889)\n",
      " Starting fold 10 / 10 (acc: 0.7778)\n",
      "Mean accuracy on test set: 0.7889\n",
      "\n",
      "Training with 2000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.6667)\n",
      " Starting fold 2 / 10 (acc: 0.7778)\n",
      " Starting fold 3 / 10 (acc: 0.7778)\n",
      " Starting fold 4 / 10 (acc: 0.7778)\n",
      " Starting fold 5 / 10 (acc: 0.7778)\n",
      " Starting fold 6 / 10 (acc: 0.6667)\n",
      " Starting fold 7 / 10 (acc: 0.7778)\n",
      " Starting fold 8 / 10 (acc: 0.7778)\n",
      " Starting fold 9 / 10 (acc: 0.8889)\n",
      " Starting fold 10 / 10 (acc: 0.7778)\n",
      "Mean accuracy on test set: 0.7667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Results of supervised learning on expert dominant labels ===\")\n",
    "random_forest_acc_experts_dominant = random_forest(X_2D,\n",
    "                                                   y_dominant_experts, \n",
    "                                                   dominant_expert_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results of supervised learning on crowd dominant labels ===\n",
      "Training with 100 estimators\n",
      " Starting fold 1 / 10 (acc: 0.5385)\n",
      " Starting fold 2 / 10 (acc: 0.5385)\n",
      " Starting fold 3 / 10 (acc: 0.6154)\n",
      " Starting fold 4 / 10 (acc: 0.5385)\n",
      " Starting fold 5 / 10 (acc: 0.5385)\n",
      " Starting fold 6 / 10 (acc: 0.6154)\n",
      " Starting fold 7 / 10 (acc: 0.6154)\n",
      " Starting fold 8 / 10 (acc: 0.6154)\n",
      " Starting fold 9 / 10 (acc: 0.4615)\n",
      " Starting fold 10 / 10 (acc: 0.5385)\n",
      "Mean accuracy on test set: 0.5615\n",
      "\n",
      "Training with 250 estimators\n",
      " Starting fold 1 / 10 (acc: 0.6923)\n",
      " Starting fold 2 / 10 (acc: 0.5385)\n",
      " Starting fold 3 / 10 (acc: 0.5385)\n",
      " Starting fold 4 / 10 (acc: 0.6154)\n",
      " Starting fold 5 / 10 (acc: 0.6154)\n",
      " Starting fold 6 / 10 (acc: 0.6154)\n",
      " Starting fold 7 / 10 (acc: 0.6154)\n",
      " Starting fold 8 / 10 (acc: 0.5385)\n",
      " Starting fold 9 / 10 (acc: 0.5385)\n",
      " Starting fold 10 / 10 (acc: 0.6154)\n",
      "Mean accuracy on test set: 0.5923\n",
      "\n",
      "Training with 500 estimators\n",
      " Starting fold 1 / 10 (acc: 0.5385)\n",
      " Starting fold 2 / 10 (acc: 0.5385)\n",
      " Starting fold 3 / 10 (acc: 0.6154)\n",
      " Starting fold 4 / 10 (acc: 0.5385)\n",
      " Starting fold 5 / 10 (acc: 0.6154)\n",
      " Starting fold 6 / 10 (acc: 0.5385)\n",
      " Starting fold 7 / 10 (acc: 0.5385)\n",
      " Starting fold 8 / 10 (acc: 0.5385)\n",
      " Starting fold 9 / 10 (acc: 0.6154)\n",
      " Starting fold 10 / 10 (acc: 0.5385)\n",
      "Mean accuracy on test set: 0.5615\n",
      "\n",
      "Training with 750 estimators\n",
      " Starting fold 1 / 10 (acc: 0.5385)\n",
      " Starting fold 2 / 10 (acc: 0.5385)\n",
      " Starting fold 3 / 10 (acc: 0.5385)\n",
      " Starting fold 4 / 10 (acc: 0.5385)\n",
      " Starting fold 5 / 10 (acc: 0.5385)\n",
      " Starting fold 6 / 10 (acc: 0.6154)\n",
      " Starting fold 7 / 10 (acc: 0.5385)\n",
      " Starting fold 8 / 10 (acc: 0.6154)\n",
      " Starting fold 9 / 10 (acc: 0.5385)\n",
      " Starting fold 10 / 10 (acc: 0.6154)\n",
      "Mean accuracy on test set: 0.5615\n",
      "\n",
      "Training with 1000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.5385)\n",
      " Starting fold 2 / 10 (acc: 0.6154)\n",
      " Starting fold 3 / 10 (acc: 0.5385)\n",
      " Starting fold 4 / 10 (acc: 0.5385)\n",
      " Starting fold 5 / 10 (acc: 0.5385)\n",
      " Starting fold 6 / 10 (acc: 0.5385)\n",
      " Starting fold 7 / 10 (acc: 0.5385)\n",
      " Starting fold 8 / 10 (acc: 0.5385)\n",
      " Starting fold 9 / 10 (acc: 0.5385)\n",
      " Starting fold 10 / 10 (acc: 0.5385)\n",
      "Mean accuracy on test set: 0.5462\n",
      "\n",
      "Training with 2000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.5385)\n",
      " Starting fold 2 / 10 (acc: 0.5385)\n",
      " Starting fold 3 / 10 (acc: 0.5385)\n",
      " Starting fold 4 / 10 (acc: 0.6154)\n",
      " Starting fold 5 / 10 (acc: 0.5385)\n",
      " Starting fold 6 / 10 (acc: 0.5385)\n",
      " Starting fold 7 / 10 (acc: 0.5385)\n",
      " Starting fold 8 / 10 (acc: 0.5385)\n",
      " Starting fold 9 / 10 (acc: 0.5385)\n",
      " Starting fold 10 / 10 (acc: 0.5385)\n",
      "Mean accuracy on test set: 0.5462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Results of supervised learning on crowd dominant labels ===\")\n",
    "random_forest_acc_crowd_dominant = random_forest(X_2D,\n",
    "                                                 y_dominant_crowd,\n",
    "                                                 dominant_crowd_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results of supervised learning on combined dominant labels ===\n",
      "Training with 100 estimators\n",
      " Starting fold 1 / 10 (acc: 0.5000)\n",
      " Starting fold 2 / 10 (acc: 0.5000)\n",
      " Starting fold 3 / 10 (acc: 0.5000)\n",
      " Starting fold 4 / 10 (acc: 0.5455)\n",
      " Starting fold 5 / 10 (acc: 0.5000)\n",
      " Starting fold 6 / 10 (acc: 0.4545)\n",
      " Starting fold 7 / 10 (acc: 0.5000)\n",
      " Starting fold 8 / 10 (acc: 0.4545)\n",
      " Starting fold 9 / 10 (acc: 0.5000)\n",
      " Starting fold 10 / 10 (acc: 0.4091)\n",
      "Mean accuracy on test set: 0.4864\n",
      "\n",
      "Training with 250 estimators\n",
      " Starting fold 1 / 10 (acc: 0.4545)\n",
      " Starting fold 2 / 10 (acc: 0.5000)\n",
      " Starting fold 3 / 10 (acc: 0.4545)\n",
      " Starting fold 4 / 10 (acc: 0.5000)\n",
      " Starting fold 5 / 10 (acc: 0.4545)\n",
      " Starting fold 6 / 10 (acc: 0.5455)\n",
      " Starting fold 7 / 10 (acc: 0.5455)\n",
      " Starting fold 8 / 10 (acc: 0.5000)\n",
      " Starting fold 9 / 10 (acc: 0.5000)\n",
      " Starting fold 10 / 10 (acc: 0.5000)\n",
      "Mean accuracy on test set: 0.4955\n",
      "\n",
      "Training with 500 estimators\n",
      " Starting fold 1 / 10 (acc: 0.4545)\n",
      " Starting fold 2 / 10 (acc: 0.4091)\n",
      " Starting fold 3 / 10 (acc: 0.5000)\n",
      " Starting fold 4 / 10 (acc: 0.5000)\n",
      " Starting fold 5 / 10 (acc: 0.5000)\n",
      " Starting fold 6 / 10 (acc: 0.5000)\n",
      " Starting fold 7 / 10 (acc: 0.5000)\n",
      " Starting fold 8 / 10 (acc: 0.5000)\n",
      " Starting fold 9 / 10 (acc: 0.5000)\n",
      " Starting fold 10 / 10 (acc: 0.5000)\n",
      "Mean accuracy on test set: 0.4864\n",
      "\n",
      "Training with 750 estimators\n",
      " Starting fold 1 / 10 (acc: 0.5000)\n",
      " Starting fold 2 / 10 (acc: 0.4545)\n",
      " Starting fold 3 / 10 (acc: 0.5455)\n",
      " Starting fold 4 / 10 (acc: 0.5000)\n",
      " Starting fold 5 / 10 (acc: 0.5000)\n",
      " Starting fold 6 / 10 (acc: 0.5000)\n",
      " Starting fold 7 / 10 (acc: 0.5000)\n",
      " Starting fold 8 / 10 (acc: 0.5455)\n",
      " Starting fold 9 / 10 (acc: 0.5000)\n",
      " Starting fold 10 / 10 (acc: 0.4545)\n",
      "Mean accuracy on test set: 0.5000\n",
      "\n",
      "Training with 1000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.5000)\n",
      " Starting fold 2 / 10 (acc: 0.5000)\n",
      " Starting fold 3 / 10 (acc: 0.5000)\n",
      " Starting fold 4 / 10 (acc: 0.4545)\n",
      " Starting fold 5 / 10 (acc: 0.4545)\n",
      " Starting fold 6 / 10 (acc: 0.5000)\n",
      " Starting fold 7 / 10 (acc: 0.5455)\n",
      " Starting fold 8 / 10 (acc: 0.5455)\n",
      " Starting fold 9 / 10 (acc: 0.5455)\n",
      " Starting fold 10 / 10 (acc: 0.5455)\n",
      "Mean accuracy on test set: 0.5091\n",
      "\n",
      "Training with 2000 estimators\n",
      " Starting fold 1 / 10 (acc: 0.5000)\n",
      " Starting fold 2 / 10 (acc: 0.5000)\n",
      " Starting fold 3 / 10 (acc: 0.5000)\n",
      " Starting fold 4 / 10 (acc: 0.5000)\n",
      " Starting fold 5 / 10 (acc: 0.5455)\n",
      " Starting fold 6 / 10 (acc: 0.5455)\n",
      " Starting fold 7 / 10 (acc: 0.5000)\n",
      " Starting fold 8 / 10 (acc: 0.5000)\n",
      " Starting fold 9 / 10 (acc: 0.5000)\n",
      " Starting fold 10 / 10 (acc: 0.5000)\n",
      "Mean accuracy on test set: 0.5091\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Results of supervised learning on combined dominant labels ===\")\n",
    "random_forest_acc_combined_dominant = random_forest(X_2D,\n",
    "                                                    y_dominant_combined,\n",
    "                                                    dominant_combined_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "def pac(X, y, index, model=None, partial=False):\n",
    "    n_samples = X[index].shape[0]\n",
    "    acc = 0.0\n",
    "    best_score = -1\n",
    "    best_model = None\n",
    "    for fold_i in range(N_FOLDS):\n",
    "        print(\" Starting fold {} / {}\".format(fold_i+1, N_FOLDS), end='')\n",
    "\n",
    "        train_fold_idx, test_fold_idx  = create_splits(y[index])\n",
    "        train_idx = index[train_fold_idx]\n",
    "        test_idx = index[test_fold_idx]\n",
    "\n",
    "        if model is None:\n",
    "            if partial:\n",
    "                classes = np.unique(y)\n",
    "                model = PassiveAggressiveClassifier(max_iter=2000, warm_start=True)\n",
    "                model.partial_fit(X[train_idx], y[train_idx], classes)\n",
    "            else:\n",
    "                model = PassiveAggressiveClassifier(max_iter=2000, warm_start=False)\n",
    "                model.fit(X[train_idx], y[train_idx])\n",
    "        else:\n",
    "            model.partial_fit(X[train_idx], y[train_idx])\n",
    "\n",
    "        y_pred = model.predict(X[test_idx])\n",
    "        fold_acc = accuracy_score(y[test_idx], y_pred)\n",
    "        \n",
    "        if best_score < 0 or best_score < (fold_acc - 0.02):\n",
    "            best_score = fold_acc\n",
    "            best_model = model\n",
    "        \n",
    "        if not partial:\n",
    "            model = None\n",
    "\n",
    "        acc += fold_acc\n",
    "        print(\" (acc: {:.4f})\".format(fold_acc))\n",
    "\n",
    "    acc /= N_FOLDS\n",
    "    print(\"Mean accuracy on test set: {:.4f}\\n\".format(acc))\n",
    "    \n",
    "    return (acc, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results of supervised learning on expert likert labels ===\n",
      " Starting fold 1 / 10 (acc: 0.2000)\n",
      " Starting fold 2 / 10 (acc: 0.2000)\n",
      " Starting fold 3 / 10 (acc: 0.2000)\n",
      " Starting fold 4 / 10 (acc: 0.2000)\n",
      " Starting fold 5 / 10 (acc: 0.2000)\n",
      " Starting fold 6 / 10 (acc: 0.2000)\n",
      " Starting fold 7 / 10 (acc: 0.2000)\n",
      " Starting fold 8 / 10 (acc: 0.2000)\n",
      " Starting fold 9 / 10 (acc: 0.2000)\n",
      " Starting fold 10 / 10 (acc: 0.2000)\n",
      "Mean accuracy on test set: 0.2000\n",
      "\n",
      "=== Results of supervised learning on crowd likert labels ===\n",
      " Starting fold 1 / 10 (acc: 0.3333)\n",
      " Starting fold 2 / 10 (acc: 0.2667)\n",
      " Starting fold 3 / 10 (acc: 0.2667)\n",
      " Starting fold 4 / 10 (acc: 0.4667)\n",
      " Starting fold 5 / 10 (acc: 0.3333)\n",
      " Starting fold 6 / 10 (acc: 0.3333)\n",
      " Starting fold 7 / 10 (acc: 0.3333)\n",
      " Starting fold 8 / 10 (acc: 0.4000)\n",
      " Starting fold 9 / 10 (acc: 0.3333)\n",
      " Starting fold 10 / 10 (acc: 0.4000)\n",
      "Mean accuracy on test set: 0.3467\n",
      "\n",
      "=== Results of supervised learning on combined likert labels ===\n",
      " Starting fold 1 / 10 (acc: 0.2308)\n",
      " Starting fold 2 / 10 (acc: 0.2308)\n",
      " Starting fold 3 / 10 (acc: 0.2308)\n",
      " Starting fold 4 / 10 (acc: 0.2308)\n",
      " Starting fold 5 / 10 (acc: 0.2308)\n",
      " Starting fold 6 / 10 (acc: 0.1923)\n",
      " Starting fold 7 / 10 (acc: 0.1538)\n",
      " Starting fold 8 / 10 (acc: 0.2692)\n",
      " Starting fold 9 / 10 (acc: 0.2308)\n",
      " Starting fold 10 / 10 (acc: 0.2308)\n",
      "Mean accuracy on test set: 0.2231\n",
      "\n",
      "=== Results of supervised learning on expert dominant labels ===\n",
      " Starting fold 1 / 10 (acc: 0.6667)\n",
      " Starting fold 2 / 10 (acc: 0.6667)\n",
      " Starting fold 3 / 10 (acc: 0.6667)\n",
      " Starting fold 4 / 10 (acc: 0.6667)\n",
      " Starting fold 5 / 10 (acc: 0.6667)\n",
      " Starting fold 6 / 10 (acc: 0.6667)\n",
      " Starting fold 7 / 10 (acc: 0.7778)\n",
      " Starting fold 8 / 10 (acc: 0.6667)\n",
      " Starting fold 9 / 10 (acc: 0.6667)\n",
      " Starting fold 10 / 10 (acc: 0.5556)\n",
      "Mean accuracy on test set: 0.6667\n",
      "\n",
      "=== Results of supervised learning on crowd dominant labels ===\n",
      " Starting fold 1 / 10 (acc: 0.6923)\n",
      " Starting fold 2 / 10 (acc: 0.6923)\n",
      " Starting fold 3 / 10 (acc: 0.6923)\n",
      " Starting fold 4 / 10 (acc: 0.6923)\n",
      " Starting fold 5 / 10 (acc: 0.6923)\n",
      " Starting fold 6 / 10 (acc: 0.7692)\n",
      " Starting fold 7 / 10 (acc: 0.7692)\n",
      " Starting fold 8 / 10 (acc: 0.6923)\n",
      " Starting fold 9 / 10 (acc: 0.6923)\n",
      " Starting fold 10 / 10 (acc: 0.3077)\n",
      "Mean accuracy on test set: 0.6692\n",
      "\n",
      "=== Results of supervised learning on combined dominant labels ===\n",
      " Starting fold 1 / 10 (acc: 0.5455)\n",
      " Starting fold 2 / 10 (acc: 0.4091)\n",
      " Starting fold 3 / 10 (acc: 0.5000)\n",
      " Starting fold 4 / 10 (acc: 0.5455)\n",
      " Starting fold 5 / 10 (acc: 0.3636)\n",
      " Starting fold 6 / 10 (acc: 0.2727)\n",
      " Starting fold 7 / 10 (acc: 0.4091)\n",
      " Starting fold 8 / 10 (acc: 0.5455)\n",
      " Starting fold 9 / 10 (acc: 0.6364)\n",
      " Starting fold 10 / 10 (acc: 0.5455)\n",
      "Mean accuracy on test set: 0.4773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Results of supervised learning on expert likert labels ===\")\n",
    "pac_acc_experts_likert, _ = pac(X_2D,\n",
    "                                y_likert_experts, \n",
    "                                likert_expert_idx)\n",
    "\n",
    "print(\"=== Results of supervised learning on crowd likert labels ===\")\n",
    "pac_acc_crowd_likert, _ = pac(X_2D,\n",
    "                              y_likert_crowd,\n",
    "                              likert_crowd_idx)\n",
    "\n",
    "print(\"=== Results of supervised learning on combined likert labels ===\")\n",
    "pac_acc_combined_likert, _ = pac(X_2D,\n",
    "                                 y_likert_combined,\n",
    "                                 likert_combined_idx)\n",
    "\n",
    "print(\"=== Results of supervised learning on expert dominant labels ===\")\n",
    "pac_acc_experts_dominant, _ = pac(X_2D,\n",
    "                                  y_dominant_experts, \n",
    "                                  dominant_expert_idx)\n",
    "\n",
    "print(\"=== Results of supervised learning on crowd dominant labels ===\")\n",
    "pac_acc_crowd_dominant, _ = pac(X_2D,\n",
    "                                y_dominant_crowd,\n",
    "                                dominant_crowd_idx)\n",
    "\n",
    "print(\"=== Results of supervised learning on combined dominant labels ===\")\n",
    "pac_acc_combined_dominant, _ = pac(X_2D,\n",
    "                                   y_dominant_combined,\n",
    "                                   dominant_combined_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## incremental learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results of supervised learning on expert likert labels ===\n",
      " Starting fold 1 / 10 (acc: 0.2000)\n",
      " Starting fold 2 / 10 (acc: 0.2667)\n",
      " Starting fold 3 / 10 (acc: 0.2667)\n",
      " Starting fold 4 / 10 (acc: 0.2667)\n",
      " Starting fold 5 / 10 (acc: 0.2667)\n",
      " Starting fold 6 / 10 (acc: 0.2000)\n",
      " Starting fold 7 / 10 (acc: 0.2000)\n",
      " Starting fold 8 / 10 (acc: 0.2000)\n",
      " Starting fold 9 / 10 (acc: 0.2000)\n",
      " Starting fold 10 / 10 (acc: 0.2000)\n",
      "Mean accuracy on test set: 0.2267\n",
      "\n",
      "=== Results of supervised learning on crowd likert labels ===\n",
      " Starting fold 1 / 10 (acc: 0.2667)\n",
      " Starting fold 2 / 10 (acc: 0.2000)\n",
      " Starting fold 3 / 10 (acc: 0.2667)\n",
      " Starting fold 4 / 10 (acc: 0.2667)\n",
      " Starting fold 5 / 10 (acc: 0.2667)\n",
      " Starting fold 6 / 10 (acc: 0.4000)\n",
      " Starting fold 7 / 10 (acc: 0.3333)\n",
      " Starting fold 8 / 10 (acc: 0.2667)\n",
      " Starting fold 9 / 10 (acc: 0.3333)\n",
      " Starting fold 10 / 10 (acc: 0.2667)\n",
      "Mean accuracy on test set: 0.2867\n",
      "\n",
      "=== Results of supervised learning on expert dominant labels ===\n",
      " Starting fold 1 / 10 (acc: 0.5556)\n",
      " Starting fold 2 / 10 (acc: 0.6667)\n",
      " Starting fold 3 / 10 (acc: 0.6667)\n",
      " Starting fold 4 / 10 (acc: 0.6667)\n",
      " Starting fold 5 / 10 (acc: 0.6667)\n",
      " Starting fold 6 / 10 (acc: 0.6667)\n",
      " Starting fold 7 / 10 (acc: 0.6667)\n",
      " Starting fold 8 / 10 (acc: 0.6667)\n",
      " Starting fold 9 / 10 (acc: 0.6667)\n",
      " Starting fold 10 / 10 (acc: 0.7778)\n",
      "Mean accuracy on test set: 0.6667\n",
      "\n",
      "=== Results of supervised learning on crowd dominant labels ===\n",
      " Starting fold 1 / 10 (acc: 0.3846)\n",
      " Starting fold 2 / 10 (acc: 0.5385)\n",
      " Starting fold 3 / 10 (acc: 0.3846)\n",
      " Starting fold 4 / 10 (acc: 0.5385)\n",
      " Starting fold 5 / 10 (acc: 0.5385)\n",
      " Starting fold 6 / 10 (acc: 0.5385)\n",
      " Starting fold 7 / 10 (acc: 0.5385)\n",
      " Starting fold 8 / 10 (acc: 0.6154)\n",
      " Starting fold 9 / 10 (acc: 0.5385)\n",
      " Starting fold 10 / 10 (acc: 0.6923)\n",
      "Mean accuracy on test set: 0.5308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Results of supervised learning on expert likert labels ===\")\n",
    "pac_acc_experts_likert, model = pac(X_2D,\n",
    "                                    y_likert_experts, \n",
    "                                    likert_expert_idx,\n",
    "                                    partial=True)\n",
    "\n",
    "print(\"=== Results of supervised learning on crowd likert labels ===\")\n",
    "pac_acc_crowd_likert, _ = pac(X_2D,\n",
    "                              y_likert_crowd,\n",
    "                              likert_crowd_idx,\n",
    "                              model=model,\n",
    "                              partial=True)\n",
    "\n",
    "print(\"=== Results of supervised learning on expert dominant labels ===\")\n",
    "pac_acc_experts_dominant, model = pac(X_2D,\n",
    "                                      y_dominant_experts, \n",
    "                                      dominant_expert_idx,\n",
    "                                      partial=True)\n",
    "\n",
    "print(\"=== Results of supervised learning on crowd dominant labels ===\")\n",
    "pac_acc_crowd_dominant, _ = pac(X_2D,\n",
    "                                y_dominant_crowd,\n",
    "                                dominant_crowd_idx,\n",
    "                                model=model,\n",
    "                                partial=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert numpy arrays to PyTorch tensors\n",
    "X_2D = torch.from_numpy(X_2D)\n",
    "X_3D = torch.from_numpy(X_3D)\n",
    "y_likert_crowd = torch.from_numpy(y_likert_crowd)\n",
    "y_likert_experts = torch.from_numpy(y_likert_experts)\n",
    "y_likert_combined = torch.from_numpy(y_likert_combined)\n",
    "y_dominant_crowd = torch.from_numpy(y_dominant_crowd)\n",
    "y_dominant_experts = torch.from_numpy(y_dominant_experts)\n",
    "y_dominant_combined = torch.from_numpy(y_dominant_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(y_hat, y):\n",
    "    # y := 1D array of class labels\n",
    "    # y_hat := 2D array of one-hot class labels\n",
    "    _, labels = y_hat.max(dim=1)\n",
    "    return torch.mean(torch.eq(labels, y).float())\n",
    "\n",
    "def fit(model, X, y, index, lr=0.01, l2norm=0.01, n_folds=10, n_epoch=250, patience=7, state=None):\n",
    "    n_samples = X[index].shape[0]\n",
    "\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    best_state = None\n",
    "    best_score = -1\n",
    "    for fold_i in range(n_folds):\n",
    "        print(\"Starting fold {} / {}\".format(fold_i+1, n_folds), end='')\n",
    "        if state is None:\n",
    "            model.init()\n",
    "        else:\n",
    "            model.load_state_dict(state)\n",
    "                \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "        \n",
    "        # early stopping\n",
    "        patience_left = patience\n",
    "        best_fold_score = -1\n",
    "        delta = 1e-4\n",
    "        best_fold_state = None\n",
    "        \n",
    "        train_fold_idx, test_fold_idx  = create_splits(y[index])\n",
    "        train_idx = index[train_fold_idx]\n",
    "        test_idx = index[test_fold_idx]\n",
    "        for epoch in range(n_epoch):\n",
    "            model.train()\n",
    "            \n",
    "            y_hat = model(X[train_idx].float())\n",
    "            train_acc = categorical_accuracy(y_hat, y[train_idx])\n",
    "            train_loss = criterion(y_hat, y[train_idx].long())\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            test_loss = None\n",
    "            with torch.no_grad():\n",
    "                y_hat = model(X[test_idx].float())\n",
    "                test_acc = categorical_accuracy(y_hat, y[test_idx])\n",
    "                test_loss = criterion(y_hat, y[test_idx].long())\n",
    "                \n",
    "            train_loss = float(train_loss.item())\n",
    "            test_loss = float(test_loss.item())\n",
    "            \n",
    "            if patience <= 0:\n",
    "                continue\n",
    "            if best_fold_score < 0:\n",
    "                best_fold_score = test_loss\n",
    "                best_fold_state = model.state_dict()\n",
    "            if test_loss >= best_fold_score - delta:\n",
    "                patience_left -= 1\n",
    "            else:\n",
    "                best_fold_score = test_loss\n",
    "                best_fold_state = model.state_dict()\n",
    "                patience_left = patience\n",
    "            if patience_left <= 0:\n",
    "                model.load_state_dict(best_fold_state)\n",
    "                \n",
    "                test_idx = index[create_splits(y[index])[1]]  # get new random test set to validate on\n",
    "                with torch.no_grad():\n",
    "                    y_hat = model(X[test_idx].float())\n",
    "                    test_acc = categorical_accuracy(y_hat, y[test_idx])\n",
    "                    test_loss = float(criterion(y_hat, y[test_idx].long()).item())\n",
    "        \n",
    "        loss += test_loss\n",
    "        acc += test_acc\n",
    "        if best_score < 0 or best_score > test_loss:\n",
    "            best_state = best_fold_state\n",
    "            best_score = test_loss\n",
    "        print(\" - training accuracy: {:.4f} / loss: {:.4f} - test accuracy: {:.4f} / loss: {:.4f}\".format(train_acc,\n",
    "                                                                                          train_loss,\n",
    "                                                                                          test_acc,\n",
    "                                                                                          test_loss))\n",
    "        \n",
    "    loss /= n_folds\n",
    "    acc /= n_folds\n",
    "    print(\"average loss on test set: {:.4f}\".format(loss))\n",
    "    print(\"average accuracy on test set: {:.4f}\".format(acc))\n",
    "    \n",
    "    return (acc, best_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierNN(nn.Module):\n",
    "    \"\"\"Simple Neural Network Classifier\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, p_dropout=0.05):\n",
    "        super().__init__()\n",
    "        hidden_dim = (input_dim-output_dim)//2\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_dropout),\n",
    "            \n",
    "\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.softmax(self.fc(X))\n",
    "        \n",
    "    def init(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.normal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results on expert likert labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.7442 / loss: 1.4658 - test accuracy: 0.1333 / loss: 1.9222\n",
      "Starting fold 2 / 10 - training accuracy: 0.6279 / loss: 1.5487 - test accuracy: 0.3333 / loss: 1.8956\n",
      "Starting fold 3 / 10 - training accuracy: 0.7674 / loss: 1.4162 - test accuracy: 0.1333 / loss: 1.9810\n",
      "Starting fold 4 / 10 - training accuracy: 0.5814 / loss: 1.5849 - test accuracy: 0.0000 / loss: 2.0061\n",
      "Starting fold 5 / 10 - training accuracy: 0.7209 / loss: 1.4515 - test accuracy: 0.0667 / loss: 2.0493\n",
      "Starting fold 6 / 10 - training accuracy: 0.6512 / loss: 1.4926 - test accuracy: 0.3333 / loss: 1.9161\n",
      "Starting fold 7 / 10 - training accuracy: 0.6512 / loss: 1.4984 - test accuracy: 0.3333 / loss: 1.7920\n",
      "Starting fold 8 / 10 - training accuracy: 0.7907 / loss: 1.4231 - test accuracy: 0.2000 / loss: 1.8810\n",
      "Starting fold 9 / 10 - training accuracy: 0.8140 / loss: 1.3716 - test accuracy: 0.3333 / loss: 1.8516\n",
      "Starting fold 10 / 10 - training accuracy: 0.5116 / loss: 1.6202 - test accuracy: 0.2667 / loss: 1.9243\n",
      "average loss on test set: 1.9219\n",
      "average accuracy on test set: 0.2133\n",
      "\n",
      "=== Results on crowd likert labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.7447 / loss: 1.4231 - test accuracy: 0.3333 / loss: 1.8302\n",
      "Starting fold 2 / 10 - training accuracy: 0.5106 / loss: 1.6049 - test accuracy: 0.2000 / loss: 1.9404\n",
      "Starting fold 3 / 10 - training accuracy: 0.8085 / loss: 1.3951 - test accuracy: 0.3333 / loss: 1.8712\n",
      "Starting fold 4 / 10 - training accuracy: 0.5532 / loss: 1.5612 - test accuracy: 0.2667 / loss: 1.8949\n",
      "Starting fold 5 / 10 - training accuracy: 0.8085 / loss: 1.3684 - test accuracy: 0.4000 / loss: 1.7208\n",
      "Starting fold 6 / 10 - training accuracy: 0.5319 / loss: 1.5961 - test accuracy: 0.3333 / loss: 1.8418\n",
      "Starting fold 7 / 10 - training accuracy: 0.8723 / loss: 1.3346 - test accuracy: 0.3333 / loss: 1.8317\n",
      "Starting fold 8 / 10 - training accuracy: 0.7234 / loss: 1.4481 - test accuracy: 0.3333 / loss: 1.8625\n",
      "Starting fold 9 / 10 - training accuracy: 0.7234 / loss: 1.4527 - test accuracy: 0.4000 / loss: 1.7726\n",
      "Starting fold 10 / 10 - training accuracy: 0.8511 / loss: 1.3730 - test accuracy: 0.3333 / loss: 1.8162\n",
      "average loss on test set: 1.8382\n",
      "average accuracy on test set: 0.3267\n",
      "\n",
      "=== Results on combined likert labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.5745 / loss: 1.6017 - test accuracy: 0.2308 / loss: 1.8833\n",
      "Starting fold 2 / 10 - training accuracy: 0.5426 / loss: 1.6547 - test accuracy: 0.2308 / loss: 1.9232\n",
      "Starting fold 3 / 10 - training accuracy: 0.5851 / loss: 1.5868 - test accuracy: 0.3077 / loss: 1.8927\n",
      "Starting fold 4 / 10 - training accuracy: 0.6489 / loss: 1.5629 - test accuracy: 0.3077 / loss: 1.8427\n",
      "Starting fold 5 / 10 - training accuracy: 0.5745 / loss: 1.6342 - test accuracy: 0.2692 / loss: 1.8839\n",
      "Starting fold 6 / 10 - training accuracy: 0.6170 / loss: 1.5854 - test accuracy: 0.3846 / loss: 1.7861\n",
      "Starting fold 7 / 10 - training accuracy: 0.6489 / loss: 1.5595 - test accuracy: 0.4231 / loss: 1.7889\n",
      "Starting fold 8 / 10 - training accuracy: 0.6277 / loss: 1.5847 - test accuracy: 0.2308 / loss: 1.9299\n",
      "Starting fold 9 / 10 - training accuracy: 0.5638 / loss: 1.6026 - test accuracy: 0.2308 / loss: 1.9399\n",
      "Starting fold 10 / 10 - training accuracy: 0.7021 / loss: 1.5278 - test accuracy: 0.3077 / loss: 1.8707\n",
      "average loss on test set: 1.8741\n",
      "average accuracy on test set: 0.2923\n"
     ]
    }
   ],
   "source": [
    "## hyperparameters\n",
    "lr = 0.01\n",
    "n_epoch = 250\n",
    "p_dropout = 0.05\n",
    "\n",
    "## define model\n",
    "indim = X_2D.shape[1]\n",
    "outdim = np.unique(y_likert_experts[likert_expert_idx]).shape[0]\n",
    "assert outdim == np.unique(y_likert_crowd[likert_crowd_idx]).shape[0]\n",
    "\n",
    "model = ClassifierNN(input_dim=indim,\n",
    "                     output_dim=outdim,\n",
    "                     p_dropout=p_dropout)\n",
    "\n",
    "print(\"=== Results on expert likert labels ===\")\n",
    "neural_net_acc_likert_experts, _ = fit(model, X_2D, y_likert_experts, likert_expert_idx, lr=lr, n_epoch=n_epoch)\n",
    "\n",
    "print(\"\\n=== Results on crowd likert labels ===\")\n",
    "neural_net_acc_likert_crowd, _ = fit(model, X_2D, y_likert_crowd, likert_crowd_idx, lr=lr, n_epoch=n_epoch)\n",
    "\n",
    "print(\"\\n=== Results on combined likert labels ===\")\n",
    "neural_net_acc_likert_combined, _ = fit(model, X_2D, y_likert_combined, likert_combined_idx, lr=lr, n_epoch=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results on expert dominant labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.9429 / loss: 0.4674 - test accuracy: 0.6667 / loss: 0.6439\n",
      "Starting fold 2 / 10 - training accuracy: 0.9429 / loss: 0.3717 - test accuracy: 0.7778 / loss: 0.5279\n",
      "Starting fold 3 / 10 - training accuracy: 0.8000 / loss: 0.4407 - test accuracy: 0.5556 / loss: 0.6636\n",
      "Starting fold 4 / 10 - training accuracy: 0.9429 / loss: 0.4123 - test accuracy: 0.5556 / loss: 0.6027\n",
      "Starting fold 5 / 10 - training accuracy: 0.9143 / loss: 0.4176 - test accuracy: 0.6667 / loss: 0.5906\n",
      "Starting fold 6 / 10 - training accuracy: 0.8857 / loss: 0.4684 - test accuracy: 0.8889 / loss: 0.4631\n",
      "Starting fold 7 / 10 - training accuracy: 0.8571 / loss: 0.4434 - test accuracy: 0.5556 / loss: 0.6796\n",
      "Starting fold 8 / 10 - training accuracy: 0.9429 / loss: 0.3833 - test accuracy: 0.5556 / loss: 0.7476\n",
      "Starting fold 9 / 10 - training accuracy: 0.9429 / loss: 0.4058 - test accuracy: 0.5556 / loss: 0.6397\n",
      "Starting fold 10 / 10 - training accuracy: 0.8857 / loss: 0.4509 - test accuracy: 0.4444 / loss: 0.7615\n",
      "average loss on test set: 0.6320\n",
      "average accuracy on test set: 0.6222\n",
      "\n",
      "=== Results on crowd dominant labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.9412 / loss: 0.4245 - test accuracy: 0.6923 / loss: 0.6456\n",
      "Starting fold 2 / 10 - training accuracy: 1.0000 / loss: 0.3371 - test accuracy: 0.6923 / loss: 0.5914\n",
      "Starting fold 3 / 10 - training accuracy: 0.9608 / loss: 0.3654 - test accuracy: 0.7692 / loss: 0.5924\n",
      "Starting fold 4 / 10 - training accuracy: 0.9608 / loss: 0.3860 - test accuracy: 0.6154 / loss: 0.6971\n",
      "Starting fold 5 / 10 - training accuracy: 0.8627 / loss: 0.4522 - test accuracy: 0.5385 / loss: 0.7428\n",
      "Starting fold 6 / 10 - training accuracy: 0.7451 / loss: 0.5388 - test accuracy: 0.3846 / loss: 0.8667\n",
      "Starting fold 7 / 10 - training accuracy: 0.9216 / loss: 0.4043 - test accuracy: 0.6923 / loss: 0.6314\n",
      "Starting fold 8 / 10 - training accuracy: 0.8627 / loss: 0.4584 - test accuracy: 0.4615 / loss: 0.8089\n",
      "Starting fold 9 / 10 - training accuracy: 0.8627 / loss: 0.4940 - test accuracy: 0.6923 / loss: 0.6327\n",
      "Starting fold 10 / 10 - training accuracy: 0.9020 / loss: 0.4151 - test accuracy: 0.4615 / loss: 0.8294\n",
      "average loss on test set: 0.7038\n",
      "average accuracy on test set: 0.6000\n",
      "\n",
      "=== Results on combined dominant labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.9186 / loss: 0.4107 - test accuracy: 0.4545 / loss: 0.7558\n",
      "Starting fold 2 / 10 - training accuracy: 0.9535 / loss: 0.4004 - test accuracy: 0.5000 / loss: 0.7755\n",
      "Starting fold 3 / 10 - training accuracy: 0.9302 / loss: 0.4174 - test accuracy: 0.4091 / loss: 0.7976\n",
      "Starting fold 4 / 10 - training accuracy: 0.9419 / loss: 0.4417 - test accuracy: 0.4091 / loss: 0.8004\n",
      "Starting fold 5 / 10 - training accuracy: 0.9070 / loss: 0.4185 - test accuracy: 0.5000 / loss: 0.7546\n",
      "Starting fold 6 / 10 - training accuracy: 0.8721 / loss: 0.4354 - test accuracy: 0.5909 / loss: 0.7285\n",
      "Starting fold 7 / 10 - training accuracy: 0.9186 / loss: 0.4300 - test accuracy: 0.5000 / loss: 0.8108\n",
      "Starting fold 8 / 10 - training accuracy: 0.9302 / loss: 0.4582 - test accuracy: 0.4545 / loss: 0.8081\n",
      "Starting fold 9 / 10 - training accuracy: 0.9419 / loss: 0.4091 - test accuracy: 0.4545 / loss: 0.8076\n",
      "Starting fold 10 / 10 - training accuracy: 0.9419 / loss: 0.4191 - test accuracy: 0.5455 / loss: 0.7146\n",
      "average loss on test set: 0.7754\n",
      "average accuracy on test set: 0.4818\n"
     ]
    }
   ],
   "source": [
    "## hyperparameters\n",
    "lr = 0.01\n",
    "n_epoch = 250\n",
    "p_dropout = 0.1\n",
    "\n",
    "## define model\n",
    "indim = X_2D.shape[1]\n",
    "outdim = np.unique(y_dominant_experts[dominant_expert_idx]).shape[0]\n",
    "assert outdim == np.unique(y_dominant_crowd[dominant_crowd_idx]).shape[0]\n",
    "\n",
    "model = ClassifierNN(input_dim=indim,\n",
    "                     output_dim=outdim,\n",
    "                     p_dropout=p_dropout)\n",
    "\n",
    "print(\"=== Results on expert dominant labels ===\")\n",
    "neural_net_acc_dominant_experts, _ = fit(model, X_2D, y_dominant_experts, dominant_expert_idx, lr=lr, n_epoch=n_epoch)\n",
    "\n",
    "print(\"\\n=== Results on crowd dominant labels ===\")\n",
    "neural_net_acc_dominant_crowd, _ = fit(model, X_2D, y_dominant_crowd, dominant_crowd_idx, lr=lr, n_epoch=n_epoch)\n",
    "\n",
    "print(\"\\n=== Results on combined dominant labels ===\")\n",
    "neural_net_acc_dominant_combined, _ = fit(model, X_2D, y_dominant_combined, dominant_combined_idx, lr=lr, n_epoch=n_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results on expert dominant labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.9714 / loss: 0.3628 - test accuracy: 0.5556 / loss: 0.6231\n",
      "Starting fold 2 / 10 - training accuracy: 0.9714 / loss: 0.3398 - test accuracy: 0.5556 / loss: 0.6435\n",
      "Starting fold 3 / 10 - training accuracy: 0.8571 / loss: 0.4756 - test accuracy: 0.6667 / loss: 0.7349\n",
      "Starting fold 4 / 10 - training accuracy: 0.9429 / loss: 0.3894 - test accuracy: 0.6667 / loss: 0.6108\n",
      "Starting fold 5 / 10 - training accuracy: 0.9143 / loss: 0.4527 - test accuracy: 0.6667 / loss: 0.5506\n",
      "Starting fold 6 / 10 - training accuracy: 0.9429 / loss: 0.4277 - test accuracy: 0.7778 / loss: 0.6080\n",
      "Starting fold 7 / 10 - training accuracy: 0.9429 / loss: 0.3874 - test accuracy: 0.8889 / loss: 0.4615\n",
      "Starting fold 8 / 10 - training accuracy: 0.9429 / loss: 0.3813 - test accuracy: 0.7778 / loss: 0.5870\n",
      "Starting fold 9 / 10 - training accuracy: 1.0000 / loss: 0.3410 - test accuracy: 0.5556 / loss: 0.7376\n",
      "Starting fold 10 / 10 - training accuracy: 0.9714 / loss: 0.3625 - test accuracy: 0.6667 / loss: 0.5506\n",
      "average loss on test set: 0.6108\n",
      "average accuracy on test set: 0.6778\n",
      "\n",
      "=== Results on crowd dominant labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 1.0000 / loss: 0.3324 - test accuracy: 0.7692 / loss: 0.5890\n",
      "Starting fold 2 / 10 - training accuracy: 1.0000 / loss: 0.3429 - test accuracy: 0.6923 / loss: 0.6063\n",
      "Starting fold 3 / 10 - training accuracy: 1.0000 / loss: 0.3310 - test accuracy: 0.7692 / loss: 0.5979\n",
      "Starting fold 4 / 10 - training accuracy: 1.0000 / loss: 0.3333 - test accuracy: 0.8462 / loss: 0.5905\n",
      "Starting fold 5 / 10 - training accuracy: 0.9804 / loss: 0.3422 - test accuracy: 0.7692 / loss: 0.5926\n",
      "Starting fold 6 / 10 - training accuracy: 1.0000 / loss: 0.3297 - test accuracy: 0.7692 / loss: 0.5835\n",
      "Starting fold 7 / 10 - training accuracy: 1.0000 / loss: 0.3352 - test accuracy: 0.8462 / loss: 0.5767\n",
      "Starting fold 8 / 10 - training accuracy: 0.9804 / loss: 0.3468 - test accuracy: 0.7692 / loss: 0.5765\n",
      "Starting fold 9 / 10 - training accuracy: 0.9804 / loss: 0.3481 - test accuracy: 0.7692 / loss: 0.5759\n",
      "Starting fold 10 / 10 - training accuracy: 0.9804 / loss: 0.3555 - test accuracy: 0.7692 / loss: 0.5881\n",
      "average loss on test set: 0.5877\n",
      "average accuracy on test set: 0.7769\n"
     ]
    }
   ],
   "source": [
    "## hyperparameters\n",
    "lr = 0.01\n",
    "n_epoch = 250\n",
    "p_dropout = 0.1\n",
    "\n",
    "## define model\n",
    "indim = X_2D.shape[1]\n",
    "outdim = np.unique(y_dominant_experts[dominant_expert_idx]).shape[0]\n",
    "assert outdim == np.unique(y_dominant_crowd[dominant_crowd_idx]).shape[0]\n",
    "\n",
    "model = ClassifierNN(input_dim=indim,\n",
    "                     output_dim=outdim,\n",
    "                     p_dropout=p_dropout)\n",
    "\n",
    "print(\"=== Results on expert dominant labels ===\")\n",
    "neural_net_acc_dominant_experts, state = fit(model, X_2D, y_dominant_experts, dominant_expert_idx, lr=lr, n_epoch=n_epoch)\n",
    "\n",
    "print(\"\\n=== Results on crowd dominant labels ===\")\n",
    "neural_net_acc_dominant_crowd, _ = fit(model, X_2D, y_dominant_crowd, dominant_crowd_idx, lr=lr, n_epoch=n_epoch, state=state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierCNN(nn.Module):\n",
    "    \"\"\"CNN Classifier\"\"\"\n",
    "\n",
    "    def __init__(self, features_in, features_out, p_dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(features_in, int(features_in*1.5), kernel_size=7),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=3), \n",
    "\n",
    "            nn.Conv1d(int(features_in*1.5), int(features_in*2), kernel_size=7),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv1d(int(features_in*2), int(features_in*2), kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveMaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(int(features_in*2)*2, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_dropout),\n",
    "\n",
    "            nn.Linear(32, features_out)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.conv(X)\n",
    "        X = X.view(X.size(0), -1)\n",
    "\n",
    "        return self.softmax(self.fc(X))\n",
    "        \n",
    "    def init(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.normal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results on expert likert labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.0000 / loss: 2.1654 - test accuracy: 0.0667 / loss: 2.0988\n",
      "Starting fold 2 / 10 - training accuracy: 0.0930 / loss: 2.0724 - test accuracy: 0.1333 / loss: 2.0321\n",
      "Starting fold 3 / 10 - training accuracy: 0.0930 / loss: 2.0724 - test accuracy: 0.1333 / loss: 2.0321\n",
      "Starting fold 4 / 10 - training accuracy: 0.2326 / loss: 1.9329 - test accuracy: 0.2000 / loss: 1.9654\n",
      "Starting fold 5 / 10 - training accuracy: 0.2326 / loss: 1.9329 - test accuracy: 0.2000 / loss: 1.9654\n",
      "Starting fold 6 / 10 - training accuracy: 0.1628 / loss: 2.0026 - test accuracy: 0.1333 / loss: 2.0321\n",
      "Starting fold 7 / 10 - training accuracy: 0.2326 / loss: 1.9329 - test accuracy: 0.2000 / loss: 1.9654\n",
      "Starting fold 8 / 10 - training accuracy: 0.2558 / loss: 1.9096 - test accuracy: 0.2000 / loss: 1.9654\n",
      "Starting fold 9 / 10 - training accuracy: 0.1163 / loss: 2.0491 - test accuracy: 0.1333 / loss: 2.0321\n",
      "Starting fold 10 / 10 - training accuracy: 0.1163 / loss: 2.0491 - test accuracy: 0.1333 / loss: 2.0321\n",
      "average loss on test set: 2.0121\n",
      "average accuracy on test set: 0.1533\n",
      "\n",
      "=== Results on crowd likert labels ===\n",
      "Starting fold 1 / 10"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c39012e17b7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== Results on crowd likert labels ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mcnn_acc_likert_crowd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_3D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_likert_crowd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikert_crowd_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-56ca24edf86b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, X, y, index, lr, l2norm, n_folds, n_epoch, patience, state)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## hyperparameters\n",
    "lr = 0.01\n",
    "p_dropout = 0.05\n",
    "\n",
    "## define model\n",
    "indim = X_3D.shape[2]\n",
    "outdim = np.unique(y_likert_experts[likert_expert_idx]).shape[0]\n",
    "assert outdim == np.unique(y_likert_crowd[likert_crowd_idx]).shape[0]\n",
    "\n",
    "model = ClassifierCNN(features_in=indim,\n",
    "                      features_out=outdim,\n",
    "                      p_dropout=p_dropout)\n",
    "\n",
    "print(\"=== Results on expert likert labels ===\")\n",
    "cnn_acc_likert_experts = fit(model, X_3D.transpose(1, 2), y_likert_experts, likert_expert_idx, lr=lr)\n",
    "\n",
    "print(\"\\n=== Results on crowd likert labels ===\")\n",
    "cnn_acc_likert_crowd = fit(model, X_3D.transpose(1, 2), y_likert_crowd, likert_crowd_idx, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results on expert dominant labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.3429 / loss: 1.8226 - test accuracy: 0.3333 / loss: 1.8321\n",
      "Starting fold 2 / 10 - training accuracy: 0.3429 / loss: 1.8226 - test accuracy: 0.3333 / loss: 1.8321\n",
      "Starting fold 3 / 10"
     ]
    }
   ],
   "source": [
    "## hyperparameters\n",
    "lr = 0.01\n",
    "p_dropout = 0.05\n",
    "\n",
    "## define model\n",
    "indim = X_3D.shape[2]\n",
    "outdim = np.unique(y_likert_experts[likert_expert_idx]).shape[0]\n",
    "assert outdim == np.unique(y_likert_crowd[likert_crowd_idx]).shape[0]\n",
    "\n",
    "model = ClassifierCNN(features_in=indim,\n",
    "                      features_out=outdim,\n",
    "                      p_dropout=p_dropout)\n",
    "\n",
    "print(\"=== Results on expert dominant labels ===\")\n",
    "cnn_acc_dominant_experts = fit(model, X_3D.transpose(1, 2), y_dominant_experts, dominant_expert_idx, lr=lr)\n",
    "\n",
    "print(\"\\n=== Results on crowd dominant labels ===\")\n",
    "cnn_acc_dominant_crowd = fit(model, X_3D.transpose(1, 2), y_dominant_crowd, dominant_crowd_idx, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierLSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 hidden_dim,\n",
    "                 num_layers=1,\n",
    "                 p_dropout=0.0):\n",
    "        \"\"\"\n",
    "        LSTM\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            bias=True,\n",
    "                            batch_first=True)  # (batch, seq, feature)\n",
    "                            \n",
    "        fc_hidden_dim = (hidden_dim-output_dim)//2\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_dim, fc_hidden_dim),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Dropout(p=p_dropout),\n",
    "                                \n",
    "                                nn.Linear(fc_hidden_dim, output_dim))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # default H0 is zero vector\n",
    "        # output Hn is representation of entire sequence\n",
    "        X, _ = self.lstm(X)\n",
    "        X = X[:,-1,:]  # only consider final output\n",
    "\n",
    "        return self.softmax(self.fc(X))\n",
    "\n",
    "    def init(self):\n",
    "        sqrt_k = sqrt(1.0/self.hidden_dim)\n",
    "        for param in self.parameters():\n",
    "            nn.init.uniform_(param, -sqrt_k, sqrt_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## hyperparameters\n",
    "lr = 0.01\n",
    "p_dropout = 0.05\n",
    "\n",
    "## define model\n",
    "indim = X_3D.shape[2]\n",
    "outdim = np.unique(y_likert_experts[likert_expert_idx]).shape[0]\n",
    "assert outdim == np.unique(y_likert_crowd[likert_crowd_idx]).shape[0]\n",
    "hidden_dim = (indim-outdim)//2\n",
    "\n",
    "model = ClassifierLSTM(input_dim=indim,\n",
    "                       output_dim=outdim,\n",
    "                       hidden_dim=hidden_dim,\n",
    "                       p_dropout=p_dropout)\n",
    "\n",
    "print(\"=== Results on expert likert labels ===\")\n",
    "lstm_acc_likert_experts = fit(model, X_3D, y_likert_experts, likert_expert_idx, lr=lr)\n",
    "\n",
    "print(\"\\n=== Results on crowd likert labels ===\")\n",
    "lstm_acc_likert_crowd = fit(model, X_3D, y_likert_crowd, likert_crowd_idx, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyperparameters\n",
    "lr = 0.01\n",
    "p_dropout = 0.05\n",
    "\n",
    "## define model\n",
    "indim = X_3D.shape[2]\n",
    "outdim = np.unique(y_likert_experts[likert_expert_idx]).shape[0]\n",
    "assert outdim == np.unique(y_likert_crowd[likert_crowd_idx]).shape[0]\n",
    "\n",
    "model = ClassifierLSTM(input_dim=indim,\n",
    "                       output_dim=outdim,\n",
    "                       hidden_dim=indim,\n",
    "                       p_dropout=p_dropout)\n",
    "\n",
    "print(\"=== Results on combined dominant labels ===\")\n",
    "lstm_acc_dominant_combined = fit(model, X_3D, y_dominant_combined, dominant_combined_idx, lr=lr)\n",
    "\n",
    "print(\"=== Results on expert dominant labels ===\")\n",
    "lstm_acc_dominant_experts = fit(model, X_3D, y_dominant_experts, dominant_expert_idx, lr=lr)\n",
    "\n",
    "print(\"\\n=== Results on crowd dominant labels ===\")\n",
    "lstm_acc_dominant_crowd = fit(model, X_3D, y_dominant_crowd, dominant_crowd_idx, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
