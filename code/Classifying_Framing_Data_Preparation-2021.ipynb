{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying framing in videos\n",
    "\n",
    "Our aim is to investigate whether we can determine the kind of framing, episodic or thematic, that is used in news videos. \n",
    "\n",
    "## Limitations\n",
    "\n",
    "Only a small number of labeled samples are available, even less of which have been labeled by experts (as opposed to the crowd). This places a higher bound on the generalizability of our models, and makes it more challenging to train deep models. Therefor, this will serve as a proof-of-concept study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/xander/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/xander/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## prequisites\n",
    "#%pip install pandas\n",
    "#%pip install numpy\n",
    "#%pip install gensim\n",
    "#%pip install nltk\n",
    "\n",
    "## libraries\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from functools import reduce\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from tqdm import tqdm\n",
    "\n",
    "## project structure\n",
    "DATA_DIR = \"/data/projects/capturingBias/research/framing/data/\"  # change to \"./\" for current directory\n",
    "VIDEO_METADATA = DATA_DIR + \"2014_metadata.csv\"\n",
    "VIDEO_TRANSCRIPTIONS = DATA_DIR + \"2014_transcripts_months_1to4.csv\"\n",
    "CROWD_LABELS_FULL = DATA_DIR + \"crowd_annotations_aggregated_all_videos.csv\"  # old and new annotations\n",
    "CROWD_LABELS_PILOT = DATA_DIR + \"crowd_annotations_aggregated_pilot_videos.csv\"  # subset of above for expert videos\n",
    "EXPERT_LABELS = DATA_DIR + \"dominant_frame_expert_labels_main_experiment_oana.csv\"\n",
    "DATA_NPZ = DATA_DIR + \"data2021.npz\"\n",
    "WORDVECTORS_KV = DATA_DIR + \"wordvectors2021.kv\"\n",
    "\n",
    "## load files\n",
    "video_metadata = pd.read_csv(VIDEO_METADATA, delimiter=';')\n",
    "video_transcriptions = pd.read_csv(VIDEO_TRANSCRIPTIONS)\n",
    "crowd_labels = pd.read_csv(CROWD_LABELS_FULL)\n",
    "crowd_labels_pilot = pd.read_csv(CROWD_LABELS_PILOT)\n",
    "expert_labels_pilot = pd.read_csv(EXPERT_LABELS)\n",
    "\n",
    "## download wordnet vocabulary used in preprocessing the transcriptions\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972917174\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed=-1):\n",
    "    if seed < 0:\n",
    "        seed = np.random.randint(0, 2**32-1)\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    return seed\n",
    "    \n",
    "print(set_seed())  # make reproducable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep relevant columns\n",
    "crowd_labels = crowd_labels[['display_id', 'framing_type']]\n",
    "crowd_labels_pilot = crowd_labels_pilot[['display_id', 'framing_type']]\n",
    "\n",
    "video_transcriptions = video_transcriptions[['display_id', 'clean_text']]\n",
    "video_metadata = video_metadata[['display_id', 'fulltitle', 'description', 'tags']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    display_id framing_type\n",
      "0  -2xBhpFi9JU     Thematic\n",
      "1  -7xsam1-KSQ     Episodic\n",
      "2  0h91y8pUuEc     Episodic\n",
      "3  1eDeqiV3mnQ     Episodic\n",
      "4  1r79RJzMGOQ     Episodic\n",
      "    display_id framing_type\n",
      "0  C9SdBZamyCE     thematic\n",
      "1  5HqBX0TJV-U     thematic\n",
      "2  I-b5s8ImMGE     thematic\n",
      "3  yfX6U4je05g     episodic\n",
      "4  e5Y_wjT-530     thematic\n",
      "    display_id framing_type\n",
      "0  C9SdBZamyCE     thematic\n",
      "1  5HqBX0TJV-U     thematic\n",
      "2  I-b5s8ImMGE     thematic\n",
      "3  yfX6U4je05g     episodic\n",
      "4  e5Y_wjT-530     thematic\n"
     ]
    }
   ],
   "source": [
    "print(expert_labels_pilot.head(n=5))\n",
    "\n",
    "print(crowd_labels_pilot.head(n=5))\n",
    "\n",
    "print(crowd_labels.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproces Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10493/10493 [00:45<00:00, 231.75it/s]\n",
      "100%|██████████| 10493/10493 [00:14<00:00, 727.61it/s]\n"
     ]
    }
   ],
   "source": [
    "## preprocess sequences\n",
    "stemmer = WordNetLemmatizer()\n",
    "def prep_text(s):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', s)\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "    document = [stemmer.lemmatize(word) for word in document if len(word) > 1]\n",
    "    document = [word for word in document if word not in stops]\n",
    "    doc_length = len(document)\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    return (document, doc_length)\n",
    "\n",
    "## drop duplicates\n",
    "video_transcriptions.drop_duplicates(subset='display_id', ignore_index=True, inplace=True)\n",
    "video_metadata.drop_duplicates(subset='display_id', ignore_index=True, inplace=True)\n",
    "\n",
    "## remove rows with missing transcriptions\n",
    "video_transcriptions = video_transcriptions[video_transcriptions['clean_text'].notna()]\n",
    "\n",
    "## remove meta data entries that we don't have transcriptions for\n",
    "video_ids = np.intersect1d(video_metadata['display_id'].values, video_transcriptions['display_id'].values)\n",
    "video_metadata = video_metadata[video_metadata['display_id'].isin(video_ids)]\n",
    "video_transcriptions = video_transcriptions[video_transcriptions['display_id'].isin(video_ids)]\n",
    "\n",
    "assert(len(video_metadata) == len(video_transcriptions))\n",
    "\n",
    "## process text\n",
    "for index, row in tqdm(video_transcriptions.iterrows(), total=len(video_transcriptions)):\n",
    "    text = row['clean_text']\n",
    "    text_processed, nwords = prep_text(text)\n",
    "    video_transcriptions.loc[index, 'clean_text'] = text_processed\n",
    "  \n",
    "for index, row in tqdm(video_metadata.iterrows(), total=len(video_metadata)):\n",
    "    for label in ['fulltitle', 'description']:\n",
    "        text = row[label]\n",
    "        text_processed, nwords = prep_text(text)\n",
    "        video_metadata.loc[index, label] = text_processed\n",
    "\n",
    "    text = ' '.join([tag for tag in row['tags'].split('+')])\n",
    "    text_processed, nwords = prep_text(text)\n",
    "    video_metadata.loc[index, 'tags'] = text_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics\n",
      "======================================================================\n",
      " - experts watched 58 videos\n",
      "   58 of which are part of our 120 videos dataset\n",
      " - crowd watched 120 videos\n",
      "   120 of which are part of our 120 videos dataset\n",
      "   58 of which are also labeled by our experts\n",
      "\n",
      "\n",
      "Video Metadata\n",
      "======================================================================\n",
      "        display_id                                          fulltitle  \\\n",
      "10390  -2xBhpFi9JU  exporting chaos west spent billion destabilizi...   \n",
      "\n",
      "                                             description  \\\n",
      "10390  despite previously supporting self determinati...   \n",
      "\n",
      "                                                    tags  \n",
      "10390  rt russia today kiev crimea anastasia churkina...  \n",
      "\n",
      "Video Transcriptions\n",
      "======================================================================\n",
      "       display_id                                         clean_text\n",
      "6575  -2xBhpFi9JU  despite previously supporting self determinati...\n",
      "\n",
      "Crowd Labels\n",
      "======================================================================\n",
      "     display_id framing_type\n",
      "12  -2xBhpFi9JU     thematic\n",
      "\n",
      "Crowd Labels Pilot\n",
      "======================================================================\n",
      "     display_id framing_type\n",
      "12  -2xBhpFi9JU     thematic\n",
      "\n",
      "Expert Results\n",
      "======================================================================\n",
      "    display_id framing_type\n",
      "0  -2xBhpFi9JU     Thematic\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStatistics\\n\" + '='*70)\n",
    "crowd_videos_uniq = np.unique(crowd_labels['display_id'].values)\n",
    "crowd_videos_pilot_uniq = np.unique(crowd_labels_pilot['display_id'].values)\n",
    "expert_videos_uniq = np.unique(expert_labels_pilot['display_id'].values)\n",
    "print(\" - experts watched {} videos\".format(expert_videos_uniq.shape[0]))\n",
    "print(\"   {} of which are part of our 120 videos dataset\".format(np.isin(expert_videos_uniq,\n",
    "                                                                         video_transcriptions['display_id']).sum()))\n",
    "print(\" - crowd watched {} videos\".format(crowd_videos_uniq.shape[0]))\n",
    "print(\"   {} of which are part of our 120 videos dataset\".format(np.isin(crowd_videos_uniq,\n",
    "                                                                         video_transcriptions['display_id']).sum()))\n",
    "print(\"   {} of which are also labeled by our experts\\n\\n\".format(np.isin(expert_videos_uniq,\n",
    "                                                                      crowd_videos_pilot_uniq).sum()))\n",
    "\n",
    "\n",
    "sample = reduce(np.intersect1d, (video_metadata['display_id'].values,\n",
    "                        video_transcriptions['display_id'].values,\n",
    "                        expert_videos_uniq,\n",
    "                        crowd_videos_pilot_uniq))[0]\n",
    "\n",
    "print(\"Video Metadata\\n\" + '='*70)\n",
    "print(video_metadata[video_metadata['display_id']==sample])\n",
    "\n",
    "print(\"\\nVideo Transcriptions\\n\" + '='*70)\n",
    "print(video_transcriptions[video_transcriptions['display_id']==sample])\n",
    "\n",
    "print(\"\\nCrowd Labels\\n\" + '='*70)\n",
    "print(crowd_labels[crowd_labels['display_id']==sample])\n",
    "\n",
    "print(\"\\nCrowd Labels Pilot\\n\" + '='*70)\n",
    "print(crowd_labels_pilot[crowd_labels_pilot['display_id']==sample])\n",
    "\n",
    "print(\"\\nExpert Results\\n\" + '='*70)\n",
    "print(expert_labels_pilot[expert_labels_pilot['display_id']==sample])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "Our input data consist of the concatenation of transcriptions, titles, descriptions, and tags---all of which can be acquired from the video. For our traditional machine learning models we need a 1-dimensional fixed-length input vector per sequence, which are learned using Doc2Vec. A 2-dimensional variable-width vector per sequence will be used for our deep models, for which we use pre-trained Word2Vec vectors. In all cases we use the dominant framing score as labels. We treat the expert labels as gold standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert text to tensor of size NUM_SEQUENCES x MAX_SEQUENCE_LENGTH x 300\n",
    "def vectorize_sequences3D(sequences, sequence_length, vector_length=300):\n",
    "    n = sequences.shape[0]\n",
    "    a = np.zeros((n, sequence_length, vector_length))  # time on vertical axis; zero padding\n",
    "    unseen = dict()\n",
    "    for i, terms in enumerate(sequences):\n",
    "        for j, term in enumerate(terms):\n",
    "            try:\n",
    "                wv = w2v_model[term][:vector_length]\n",
    "            except:\n",
    "                # words for which we don't have a vector get a random one\n",
    "                if term not in unseen.keys():\n",
    "                    unseen[term] = np.random.rand(vector_length)\n",
    "                wv = unseen[term]\n",
    "            \n",
    "            a[i, j, :] = wv\n",
    "                    \n",
    "    return a\n",
    "\n",
    "## convert text to matrix of size NUM_SEQUENCES x 300 \n",
    "def vectorize_sequences2D(model, idx, vector_length=300):\n",
    "    n = len(idx)\n",
    "    a = np.zeros((n, vector_length))\n",
    "    for i in range(n):\n",
    "        a[i] = model.dv[idx[i]]\n",
    "        \n",
    "    return a\n",
    "\n",
    "## learn word embeddings using Word2vec\n",
    "def train_word_embeddings(sequences, vector_length=300):\n",
    "    return Word2Vec(sequences, vector_size=vector_length, workers=4)\n",
    "\n",
    "## learn sequence vectors using Doc2Vec\n",
    "def train_sequence_embeddings(sequences, vector_length=300):\n",
    "    corpus = list()\n",
    "    for i, terms in enumerate(sequences):\n",
    "        terms = [term for term in terms if len(terms) > 0]\n",
    "        corpus.append(TaggedDocument(terms, [i]))\n",
    "                    \n",
    "    model = Doc2Vec(vector_size=vector_length, min_count=2, epochs=300)\n",
    "    model.build_vocab(corpus)\n",
    "    model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "    return model   \n",
    "\n",
    "def create_splits(n):\n",
    "    sample_idx = np.arange(n)\n",
    "    np.random.shuffle(sample_idx)\n",
    "    \n",
    "    return (sample_idx[:int(n*0.8)], sample_idx[int(n*0.8):])\n",
    "\n",
    "def mklists(*args):\n",
    "    data = list()\n",
    "    for a in args:\n",
    "        for sentence in a:\n",
    "            if len(sentence) <= 0:\n",
    "                continue\n",
    "            data.append(sentence.split())\n",
    "                 \n",
    "    return data\n",
    "\n",
    "def mkdata(*args):\n",
    "    data = [list() for i in range(args[0].shape[0])]\n",
    "    for a in args:\n",
    "        slc = list()\n",
    "        max_length = 0\n",
    "        for row in a:\n",
    "            if len(row) <= 0:\n",
    "                continue\n",
    "                \n",
    "            s = row.split()\n",
    "            slc.append(s)\n",
    "            if len(s) > max_length:\n",
    "                max_length = len(s)\n",
    "\n",
    "        for i, row in enumerate(slc):\n",
    "            if len(row) < max_length:\n",
    "                row.extend(['' for k in range(max_length-len(row))])\n",
    "            data[i].extend(row)\n",
    "                \n",
    "    return np.array(data, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort using same index so video_metadate[i] matches video_transcriptions[i]\n",
    "video_metadata.sort_values(by='display_id', inplace=True)\n",
    "video_metadata = video_metadata.set_index('display_id')\n",
    "video_transcriptions = video_transcriptions.set_index('display_id')\n",
    "video_transcriptions = video_transcriptions.reindex(index=video_metadata.index)\n",
    "video_transcriptions = video_transcriptions.reset_index()\n",
    "video_metadata = video_metadata.reset_index()\n",
    "\n",
    "## create mappings\n",
    "idx_video_map = dict(enumerate(video_metadata['display_id'].values))  # i:display_id\n",
    "video_idx_map = {display_id: i for i, display_id in idx_video_map.items()}  # display_id:i\n",
    "labeled_samples_ids = np.union1d(crowd_videos_uniq, expert_videos_uniq)  # all video_ids we have annotations for\n",
    "labeled_video_idx_map = {video_id: idx for video_id, idx in video_idx_map.items()\n",
    "                                       if video_id in labeled_samples_ids} \n",
    "## get idx of labeled samples in unfiltered list\n",
    "labeled_idx = sorted(labeled_video_idx_map.values())  # ensure natural ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data matrix size: 10493\n",
      "training Doc2Vec model\n",
      "vectorizing 2D data\n",
      "Data matrix size: 10493\n",
      "training Doc2Vec model\n",
      "vectorizing 2D data\n",
      "Data matrix size: 10493\n",
      "training Doc2Vec model\n",
      "vectorizing 2D data\n"
     ]
    }
   ],
   "source": [
    "# create data matrix for title\n",
    "data = mkdata(video_metadata['fulltitle'].values)\n",
    "print(\"Data matrix size: %d\" % data.shape[0])\n",
    "\n",
    "## train doc2vec\n",
    "print(\"training Doc2Vec model\")\n",
    "d2v_model = train_sequence_embeddings(data, vector_length=4)\n",
    "\n",
    "print(\"vectorizing 2D data\")\n",
    "X_2D_title = vectorize_sequences2D(d2v_model,\n",
    "                                   labeled_idx,\n",
    "                                   vector_length=4)\n",
    "\n",
    "\n",
    "# create data matrix for transcriptions only\n",
    "data = mkdata(video_transcriptions['clean_text'].values)\n",
    "print(\"Data matrix size: %d\" % data.shape[0])\n",
    "\n",
    "## train doc2vec\n",
    "print(\"training Doc2Vec model\")\n",
    "d2v_model = train_sequence_embeddings(data, vector_length=25)\n",
    "\n",
    "print(\"vectorizing 2D data\")\n",
    "X_2D_transcriptions = vectorize_sequences2D(d2v_model,\n",
    "                                            labeled_idx,\n",
    "                                            vector_length=25)\n",
    "\n",
    "\n",
    "# create data matrix for description only\n",
    "data = mkdata(video_metadata['description'].values)\n",
    "print(\"Data matrix size: %d\" % data.shape[0])\n",
    "\n",
    "## train doc2vec\n",
    "print(\"training Doc2Vec model\")\n",
    "d2v_model = train_sequence_embeddings(data, vector_length=10)\n",
    "\n",
    "print(\"vectorizing 2D data\")\n",
    "X_2D_descriptions = vectorize_sequences2D(d2v_model,\n",
    "                                         labeled_idx,\n",
    "                                         vector_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap to subset (assumes natural ordening)\n",
    "labeled_video_idx_map = {idx_video_map[idx]:i for i, idx in enumerate(labeled_idx)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_2D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-11bb1b33b193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%d labeled samples\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_experts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdisplay_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabeled_video_idx_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_2D' is not defined"
     ]
    }
   ],
   "source": [
    "num_samples = X_2D_title.shape[0]\n",
    "print(\"%d labeled samples\" % num_samples)\n",
    "\n",
    "y_experts = -np.ones(num_samples)\n",
    "for display_id, idx in labeled_video_idx_map.items():\n",
    "    if display_id not in expert_videos_uniq:  # not in pilot\n",
    "        continue\n",
    "        \n",
    "    framing_type = expert_labels_pilot[expert_labels_pilot['display_id']==display_id]['framing_type'].values[0]\n",
    "    framing_type = framing_type.lower()\n",
    "\n",
    "    if framing_type == \"thematic\" :\n",
    "        label = 0\n",
    "    elif framing_type == \"episodic\":\n",
    "        label = 1\n",
    "    else:\n",
    "        print(\"Unexpected expert label: %s\" % framing_type)\n",
    "        \n",
    "    y_experts[idx] = label\n",
    "\n",
    "y_crowd = -np.ones(num_samples)\n",
    "for display_id, idx in labeled_video_idx_map.items():\n",
    "    framing_type = crowd_labels[crowd_labels['display_id']==display_id]['framing_type'].values[0]\n",
    "    framing_type = framing_type.lower()\n",
    "\n",
    "    if framing_type == \"thematic\" :\n",
    "        label = 0\n",
    "    elif framing_type == \"episodic\":\n",
    "        label = 1\n",
    "    elif framing_type == \"episodic, thematic\":\n",
    "        # let expert vote solve stalemate (since we don't have a gold standard for this case anyway)\n",
    "        # defaults to -1 if no expert vote exists for this video\n",
    "        label = y_experts[idx]\n",
    "    else:\n",
    "        print(\"Unexpected crowd label: %s\" % framing_type)\n",
    "        \n",
    "    y_crowd[idx] = label\n",
    "\n",
    "    \n",
    "## combined set - no distinction between experts and crowd\n",
    "# expert labels are favoured over crowd labels if both are provided\n",
    "y_combined = np.copy(y_experts) \n",
    "copy_idx = np.where(y_combined == -1)[0]  # supplement missing expert labels with crowd labels\n",
    "y_combined[copy_idx] = y_crowd[copy_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(DATA_NPZ,\n",
    "                    X_2D_title = X_2D_title,\n",
    "                    X_2D_descriptions = X_2D_descriptions,\n",
    "                    X_2D_transcriptions = X_2D_transcriptions,\n",
    "                    y_crowd = y_crowd,\n",
    "                    y_experts = y_experts,\n",
    "                    y_combined = y_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2D_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2D_transcriptions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2D_descriptions.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
