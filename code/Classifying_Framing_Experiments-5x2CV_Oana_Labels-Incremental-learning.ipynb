{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install sklearn\n",
    "#%pip install torch\n",
    "\n",
    "from math import sqrt\n",
    "import os\n",
    "from time import time\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.base import clone\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "## project structure\n",
    "DATA_DIR = \"/data/projects/capturingBias/research/framing/data/\"  # change to \"./\" for current directory\n",
    "DATA_NPZ = DATA_DIR + \"data.npz\"\n",
    "\n",
    "## load files\n",
    "data = np.load(DATA_NPZ)\n",
    "\n",
    "X_2D = data['X_2D']\n",
    "X_3D = data['X_3D']\n",
    "y_crowd = data['y_crowd']\n",
    "y_experts = data['y_experts']\n",
    "y_combined = data['y_combined']\n",
    "\n",
    "# retrieve indices of labeled samples\n",
    "experts_idx = np.where(y_experts > -1)[0]\n",
    "crowd_idx = np.where(y_crowd > -1)[0]\n",
    "\n",
    "_crowd_unique_idx = np.setdiff1d(crowd_idx, experts_idx,\n",
    "                                 assume_unique=True)\n",
    "combined_idx = np.concatenate([_crowd_unique_idx, experts_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2810509032\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed=-1):\n",
    "    if seed < 0:\n",
    "        seed = np.random.randint(0, 2**32-1)\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    return seed\n",
    "    \n",
    "print(set_seed())  # make reproducable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(y, test_ratio=.5):\n",
    "    train_idx = list()\n",
    "    test_idx = list()\n",
    "    \n",
    "    strats = [np.where(y == lab)[0] for lab in np.unique(y) if lab > -1]\n",
    "    for strat in strats:\n",
    "        n = strat.shape[0]\n",
    "        train_idx.append(strat[:int(n*(1-test_ratio))])\n",
    "        test_idx.append(strat[int(n*(1-test_ratio)):])\n",
    "        \n",
    "    train_idx = np.concatenate(train_idx)\n",
    "    test_idx = np.concatenate(test_idx)\n",
    "    \n",
    "    np.random.shuffle(train_idx)\n",
    "    np.random.shuffle(test_idx)\n",
    "    \n",
    "    return (train_idx, test_idx)\n",
    "\n",
    "def create_splits_one_hot(y):\n",
    "    vec = -np.ones(y.shape[0])\n",
    "    nonzero = y.nonzero()\n",
    "    vec[nonzero[:,0]] = nonzero[:,1].float()\n",
    "    \n",
    "    return create_splits(vec)\n",
    "\n",
    "def alpaydin_F_test(c1_acc_lst, c2_acc_lst):\n",
    "    # acc_list := [np.array([acc_ij, acc_i(j+1)]) for i in 5, j in 2]\n",
    "    assert len(c1_acc_lst) == len(c2_acc_lst)\n",
    "    diff_acc_lst = [c1_acc_lst[i] - c2_acc_lst[i] for i in range(len(c1_acc_lst))]\n",
    "    \n",
    "    mean_lst = [np.mean(a) for a in diff_acc_lst] \n",
    "    var_lst = [ (diff_acc_lst[i][0] - mean_lst[i])**2\n",
    "               +(diff_acc_lst[i][1] - mean_lst[i])**2 for i in range(len(diff_acc_lst))]\n",
    "    \n",
    "    numerator = sum([sum(a**2) for a in diff_acc_lst])\n",
    "    denumerator = 2 * sum(var_lst)\n",
    "    f = numerator / denumerator\n",
    "    p_value = stats.f.sf(f, 10, 5)\n",
    "    \n",
    "    return (f, p_value, np.mean(mean_lst), np.mean(var_lst))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure same datasets per model\n",
    "crowd_splits = [create_splits(y_crowd[crowd_idx]) for i in range(5)]\n",
    "experts_splits = [create_splits(y_experts[experts_idx]) for i in range(5)]\n",
    "combined_splits = [create_splits(y_combined[combined_idx]) for i in range(5)]\n",
    "\n",
    "crowd_on_experts_splits = [create_splits(y_crowd[experts_idx]) for i in range(5)]\n",
    "crowd_unique_splits = [create_splits(y_crowd[_crowd_unique_idx]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def majority_class(y):\n",
    "    ct = Counter(y)\n",
    "    return ct.most_common(1)[0][1] / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Majority class accuracy on dominant labels (baseline)\n",
      " crowd labels:  0.6355\n",
      " expert labels: 0.5345\n",
      " combined labels: 0.5741\n"
     ]
    }
   ],
   "source": [
    "majority_class_acc_crowd = majority_class(y_crowd[crowd_idx])\n",
    "majority_class_acc_experts = majority_class(y_experts[experts_idx])\n",
    "majority_class_acc_combined = majority_class(y_combined[combined_idx])\n",
    "\n",
    "print(\"\\nMajority class accuracy on dominant labels (baseline)\")\n",
    "print(\" crowd labels:  {:.4f}\".format(majority_class_acc_crowd))\n",
    "print(\" expert labels: {:.4f}\".format(majority_class_acc_experts))\n",
    "print(\" combined labels: {:.4f}\".format(majority_class_acc_combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest (supervised)\n",
    "\n",
    "We start with a traditional, or 'shallow', machine learning model: random forest. Because random forest does not support iterative learning, we test both the crowd and expert sets separately.\n",
    "\n",
    "We use stratified cross validation to reduce the effects caused by the small size of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "N_ESTIMATORS = [100, 250, 500, 750, 1000, 2000]\n",
    "\n",
    "def random_forest(X, y, index, splits, n_estimators=N_ESTIMATORS):\n",
    "    n_samples = X[index].shape[0]\n",
    "    acc_est_lst = list()\n",
    "    for n_estimators in N_ESTIMATORS:\n",
    "        print(\"Training with {} estimators\".format(n_estimators))\n",
    "        acc_lst = list()\n",
    "        for fold_i in range(5):\n",
    "            print(\" Starting outer fold {} / {}\".format(fold_i+1, 5))\n",
    "            acc_inner = list()\n",
    "            split_a_idx, split_b_idx  = splits[fold_i]\n",
    "            for fold_j in range(2):\n",
    "                print(\"  Starting inner fold {} / {}\".format(fold_j+1, 2), end='')\n",
    "                if fold_j % 2 == 0:\n",
    "                    train_fold_idx, test_fold_idx  = split_a_idx, split_b_idx\n",
    "                else:\n",
    "                    train_fold_idx, test_fold_idx  = split_b_idx, split_a_idx\n",
    "\n",
    "                train_idx = index[train_fold_idx]\n",
    "                test_idx = index[test_fold_idx]\n",
    "\n",
    "                model = RandomForestClassifier(n_estimators=n_estimators)\n",
    "                model.fit(X[train_idx], y[train_idx])\n",
    "\n",
    "                y_pred = model.predict(X[test_idx])\n",
    "                fold_acc = accuracy_score(y[test_idx], y_pred)\n",
    "\n",
    "                acc_inner.append(fold_acc)\n",
    "                print(\" (acc: {:.4f})\".format(fold_acc))\n",
    "\n",
    "            acc_lst.append(np.array(acc_inner))\n",
    "        print(\" => mean acc: {:.4f}\\n\".format(np.mean(np.array([np.mean(inner) for inner in acc_lst]))))\n",
    "        acc_est_lst.append(acc_lst)\n",
    "    \n",
    "    return acc_est_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results of supervised learning on expert dominant labels ===\n",
      "Training with 100 estimators\n",
      " Starting outer fold 1 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5333)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6429)\n",
      " Starting outer fold 2 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6000)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6071)\n",
      " Starting outer fold 3 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.4000)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5000)\n",
      " Starting outer fold 4 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6000)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5357)\n",
      " Starting outer fold 5 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6000)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6429)\n",
      " => mean acc: 0.5662\n",
      "\n",
      "Training with 250 estimators\n",
      " Starting outer fold 1 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5333)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6429)\n",
      " Starting outer fold 2 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5000)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6071)\n",
      " Starting outer fold 3 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5333)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6071)\n",
      " Starting outer fold 4 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5667)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6429)\n",
      " Starting outer fold 5 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5667)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6071)\n",
      " => mean acc: 0.5807\n",
      "\n",
      "Training with 500 estimators\n",
      " Starting outer fold 1 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5667)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6429)\n",
      " Starting outer fold 2 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5667)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6429)\n",
      " Starting outer fold 3 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6333)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6071)\n",
      " Starting outer fold 4 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6000)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5714)\n",
      " Starting outer fold 5 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5333)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5714)\n",
      " => mean acc: 0.5936\n",
      "\n",
      "Training with 750 estimators\n",
      " Starting outer fold 1 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5667)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6071)\n",
      " Starting outer fold 2 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6333)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6429)\n",
      " Starting outer fold 3 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5333)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6071)\n",
      " Starting outer fold 4 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6000)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5714)\n",
      " Starting outer fold 5 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5667)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6429)\n",
      " => mean acc: 0.5971\n",
      "\n",
      "Training with 1000 estimators\n",
      " Starting outer fold 1 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5667)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6786)\n",
      " Starting outer fold 2 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6000)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6071)\n",
      " Starting outer fold 3 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6000)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6429)\n",
      " Starting outer fold 4 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6000)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6071)\n",
      " Starting outer fold 5 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5667)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6429)\n",
      " => mean acc: 0.6112\n",
      "\n",
      "Training with 2000 estimators\n",
      " Starting outer fold 1 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6000)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6429)\n",
      " Starting outer fold 2 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5667)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6071)\n",
      " Starting outer fold 3 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5667)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6071)\n",
      " Starting outer fold 4 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6333)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6071)\n",
      " Starting outer fold 5 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5667)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6071)\n",
      " => mean acc: 0.6005\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p-values</th>\n",
       "      <th>100</th>\n",
       "      <th>250</th>\n",
       "      <th>500</th>\n",
       "      <th>750</th>\n",
       "      <th>1000</th>\n",
       "      <th>2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250</td>\n",
       "      <td>0.271797</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>0.090884</td>\n",
       "      <td>0.486303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750</td>\n",
       "      <td>0.052031</td>\n",
       "      <td>0.492410</td>\n",
       "      <td>0.415552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.020404</td>\n",
       "      <td>0.414563</td>\n",
       "      <td>0.534881</td>\n",
       "      <td>0.234685</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.043735</td>\n",
       "      <td>0.663432</td>\n",
       "      <td>0.291258</td>\n",
       "      <td>0.075373</td>\n",
       "      <td>0.534881</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   p-values       100       250       500       750      1000  2000\n",
       "0       100       NaN       NaN       NaN       NaN       NaN   NaN\n",
       "1       250  0.271797       NaN       NaN       NaN       NaN   NaN\n",
       "2       500  0.090884  0.486303       NaN       NaN       NaN   NaN\n",
       "3       750  0.052031  0.492410  0.415552       NaN       NaN   NaN\n",
       "4      1000  0.020404  0.414563  0.534881  0.234685       NaN   NaN\n",
       "5      2000  0.043735  0.663432  0.291258  0.075373  0.534881   NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=== Results of supervised learning on expert dominant labels ===\")\n",
    "random_forest_acc_experts_dominant = random_forest(X_2D,\n",
    "                                                   y_experts, \n",
    "                                                   experts_idx,\n",
    "                                                   experts_splits)\n",
    "table = {'p-values': N_ESTIMATORS}\n",
    "table.update({est: list() for est in N_ESTIMATORS})\n",
    "nhypotheses = len(random_forest_acc_experts_dominant)\n",
    "for i in range(nhypotheses):\n",
    "    for e in range(i+1):\n",
    "        table[N_ESTIMATORS[i]].append(np.nan)\n",
    "    for j in range(i+1, nhypotheses):\n",
    "        f, p, mean, variance = alpaydin_F_test(random_forest_acc_experts_dominant[i],\n",
    "                                               random_forest_acc_experts_dominant[j])\n",
    "        table[N_ESTIMATORS[i]].append(p)\n",
    "        #print(\"RF {} vs {} estimators\".format(N_ESTIMATORS[i], N_ESTIMATORS[j]))\n",
    "        #print(\" f: {:.4f}, p: {:.4f}, mean: {:.4f}, var: {:.4f}\".format(f, p, mean, variance))\n",
    "\n",
    "significance = pd.DataFrame(table)\n",
    "display(significance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results of supervised learning on crowd dominant labels ===\n",
      "Training with 100 estimators\n",
      " Starting outer fold 1 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6111)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5283)\n",
      " Starting outer fold 2 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5741)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5849)\n",
      " Starting outer fold 3 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6111)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5849)\n",
      " Starting outer fold 4 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6296)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6038)\n",
      " Starting outer fold 5 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6481)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5472)\n",
      " => mean acc: 0.5923\n",
      "\n",
      "Training with 250 estimators\n",
      " Starting outer fold 1 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6481)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5094)\n",
      " Starting outer fold 2 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6111)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5472)\n",
      " Starting outer fold 3 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5926)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5283)\n",
      " Starting outer fold 4 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6111)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5472)\n",
      " Starting outer fold 5 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6111)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5283)\n",
      " => mean acc: 0.5734\n",
      "\n",
      "Training with 500 estimators\n",
      " Starting outer fold 1 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5926)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5283)\n",
      " Starting outer fold 2 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6111)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5472)\n",
      " Starting outer fold 3 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6296)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5283)\n",
      " Starting outer fold 4 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5741)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5472)\n",
      " Starting outer fold 5 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6111)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5283)\n",
      " => mean acc: 0.5698\n",
      "\n",
      "Training with 750 estimators\n",
      " Starting outer fold 1 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6111)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5094)\n",
      " Starting outer fold 2 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6296)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5283)\n",
      " Starting outer fold 3 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5926)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5094)\n",
      " Starting outer fold 4 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5926)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5472)\n",
      " Starting outer fold 5 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6111)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5094)\n",
      " => mean acc: 0.5641\n",
      "\n",
      "Training with 1000 estimators\n",
      " Starting outer fold 1 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6296)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5283)\n",
      " Starting outer fold 2 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6111)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5283)\n",
      " Starting outer fold 3 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6296)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5094)\n",
      " Starting outer fold 4 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6296)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4906)\n",
      " Starting outer fold 5 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6111)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5094)\n",
      " => mean acc: 0.5677\n",
      "\n",
      "Training with 2000 estimators\n",
      " Starting outer fold 1 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6111)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5472)\n",
      " Starting outer fold 2 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6296)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5283)\n",
      " Starting outer fold 3 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6111)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5283)\n",
      " Starting outer fold 4 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6296)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5283)\n",
      " Starting outer fold 5 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6111)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5283)\n",
      " => mean acc: 0.5753\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p-values</th>\n",
       "      <th>100</th>\n",
       "      <th>250</th>\n",
       "      <th>500</th>\n",
       "      <th>750</th>\n",
       "      <th>1000</th>\n",
       "      <th>2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250</td>\n",
       "      <td>0.480250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>0.431662</td>\n",
       "      <td>0.676063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750</td>\n",
       "      <td>0.440980</td>\n",
       "      <td>0.633720</td>\n",
       "      <td>0.534881</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.607047</td>\n",
       "      <td>0.760611</td>\n",
       "      <td>0.782516</td>\n",
       "      <td>0.726688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.656283</td>\n",
       "      <td>0.822993</td>\n",
       "      <td>0.697800</td>\n",
       "      <td>0.610524</td>\n",
       "      <td>0.695393</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   p-values       100       250       500       750      1000  2000\n",
       "0       100       NaN       NaN       NaN       NaN       NaN   NaN\n",
       "1       250  0.480250       NaN       NaN       NaN       NaN   NaN\n",
       "2       500  0.431662  0.676063       NaN       NaN       NaN   NaN\n",
       "3       750  0.440980  0.633720  0.534881       NaN       NaN   NaN\n",
       "4      1000  0.607047  0.760611  0.782516  0.726688       NaN   NaN\n",
       "5      2000  0.656283  0.822993  0.697800  0.610524  0.695393   NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=== Results of supervised learning on crowd dominant labels ===\")\n",
    "random_forest_acc_crowd_dominant = random_forest(X_2D,\n",
    "                                                 y_crowd,\n",
    "                                                 crowd_idx,\n",
    "                                                 crowd_splits)\n",
    "table = {'p-values': N_ESTIMATORS}\n",
    "table.update({est: list() for est in N_ESTIMATORS})\n",
    "nhypotheses = len(random_forest_acc_crowd_dominant)\n",
    "for i in range(nhypotheses):\n",
    "    for e in range(i+1):\n",
    "        table[N_ESTIMATORS[i]].append(np.nan)\n",
    "    for j in range(i+1, nhypotheses):\n",
    "        f, p, mean, variance = alpaydin_F_test(random_forest_acc_crowd_dominant[i],\n",
    "                                               random_forest_acc_crowd_dominant[j])\n",
    "        table[N_ESTIMATORS[i]].append(p)\n",
    "        #print(\"RF {} vs {} estimators\".format(N_ESTIMATORS[i], N_ESTIMATORS[j]))\n",
    "        #print(\" f: {:.4f}, p: {:.4f}, mean: {:.4f}, var: {:.4f}\".format(f, p, mean, variance))\n",
    "\n",
    "significance = pd.DataFrame(table)\n",
    "display(significance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results of supervised learning on combined dominant labels ===\n",
      "Training with 100 estimators\n",
      " Starting outer fold 1 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5926)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5926)\n",
      " Starting outer fold 2 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5741)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5370)\n",
      " Starting outer fold 3 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6481)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4815)\n",
      " Starting outer fold 4 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5556)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4630)\n",
      " Starting outer fold 5 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5556)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5370)\n",
      " => mean acc: 0.5537\n",
      "\n",
      "Training with 250 estimators\n",
      " Starting outer fold 1 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6852)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6296)\n",
      " Starting outer fold 2 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6296)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5000)\n",
      " Starting outer fold 3 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5926)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5370)\n",
      " Starting outer fold 4 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5926)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4815)\n",
      " Starting outer fold 5 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5741)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5556)\n",
      " => mean acc: 0.5778\n",
      "\n",
      "Training with 500 estimators\n",
      " Starting outer fold 1 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5926)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5185)\n",
      " Starting outer fold 2 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6296)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5000)\n",
      " Starting outer fold 3 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6296)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5556)\n",
      " Starting outer fold 4 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6296)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5556)\n",
      " Starting outer fold 5 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6481)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5556)\n",
      " => mean acc: 0.5815\n",
      "\n",
      "Training with 750 estimators\n",
      " Starting outer fold 1 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6296)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5000)\n",
      " Starting outer fold 2 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6296)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5000)\n",
      " Starting outer fold 3 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6667)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4815)\n",
      " Starting outer fold 4 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6111)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4815)\n",
      " Starting outer fold 5 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6481)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5185)\n",
      " => mean acc: 0.5667\n",
      "\n",
      "Training with 1000 estimators\n",
      " Starting outer fold 1 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6111)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5185)\n",
      " Starting outer fold 2 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6111)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5000)\n",
      " Starting outer fold 3 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6296)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5185)\n",
      " Starting outer fold 4 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6111)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5185)\n",
      " Starting outer fold 5 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6296)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5000)\n",
      " => mean acc: 0.5648\n",
      "\n",
      "Training with 2000 estimators\n",
      " Starting outer fold 1 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.5926)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4630)\n",
      " Starting outer fold 2 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6296)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5000)\n",
      " Starting outer fold 3 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6296)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5185)\n",
      " Starting outer fold 4 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.6667)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4815)\n",
      " Starting outer fold 5 / 5\n",
      "  Starting inner fold 1 / 2 (acc: 0.7037)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4815)\n",
      " => mean acc: 0.5667\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p-values</th>\n",
       "      <th>100</th>\n",
       "      <th>250</th>\n",
       "      <th>500</th>\n",
       "      <th>750</th>\n",
       "      <th>1000</th>\n",
       "      <th>2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250</td>\n",
       "      <td>0.563585</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>0.385846</td>\n",
       "      <td>0.052149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750</td>\n",
       "      <td>0.713696</td>\n",
       "      <td>0.525260</td>\n",
       "      <td>0.648012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.649448</td>\n",
       "      <td>0.318082</td>\n",
       "      <td>0.246339</td>\n",
       "      <td>0.718145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.660143</td>\n",
       "      <td>0.451359</td>\n",
       "      <td>0.775549</td>\n",
       "      <td>0.668008</td>\n",
       "      <td>0.676942</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   p-values       100       250       500       750      1000  2000\n",
       "0       100       NaN       NaN       NaN       NaN       NaN   NaN\n",
       "1       250  0.563585       NaN       NaN       NaN       NaN   NaN\n",
       "2       500  0.385846  0.052149       NaN       NaN       NaN   NaN\n",
       "3       750  0.713696  0.525260  0.648012       NaN       NaN   NaN\n",
       "4      1000  0.649448  0.318082  0.246339  0.718145       NaN   NaN\n",
       "5      2000  0.660143  0.451359  0.775549  0.668008  0.676942   NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=== Results of supervised learning on combined dominant labels ===\")\n",
    "random_forest_acc_combined_dominant = random_forest(X_2D,\n",
    "                                                    y_combined,\n",
    "                                                    combined_idx,\n",
    "                                                    combined_splits)\n",
    "table = {'p-values': N_ESTIMATORS}\n",
    "table.update({est: list() for est in N_ESTIMATORS})\n",
    "nhypotheses = len(random_forest_acc_combined_dominant)\n",
    "for i in range(nhypotheses):\n",
    "    for e in range(i+1):\n",
    "        table[N_ESTIMATORS[i]].append(np.nan)\n",
    "    for j in range(i+1, nhypotheses):\n",
    "        f, p, mean, variance = alpaydin_F_test(random_forest_acc_combined_dominant[i],\n",
    "                                               random_forest_acc_combined_dominant[j])\n",
    "        table[N_ESTIMATORS[i]].append(p)\n",
    "        #print(\"RF {} vs {} estimators\".format(N_ESTIMATORS[i], N_ESTIMATORS[j]))\n",
    "        #print(\" f: {:.4f}, p: {:.4f}, mean: {:.4f}, var: {:.4f}\".format(f, p, mean, variance))\n",
    "\n",
    "significance = pd.DataFrame(table)\n",
    "display(significance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "N_FOLDS = 5\n",
    "def pac(X, y, index):\n",
    "    n_samples = X[index].shape[0]\n",
    "    acc = 0.0\n",
    "    best_score = -1\n",
    "    best_model = None\n",
    "    for fold_i in range(N_FOLDS):\n",
    "        print(\" Starting fold {} / {}\".format(fold_i+1, N_FOLDS), end='')\n",
    "        train_fold_idx, test_fold_idx  = create_splits(y[index], test_ratio=0.2)\n",
    "        train_idx = index[train_fold_idx]\n",
    "        test_idx = index[test_fold_idx]\n",
    "        \n",
    "        classes = np.unique(y)\n",
    "        model = PassiveAggressiveClassifier(max_iter=2000, warm_start=False)\n",
    "        model.partial_fit(X[train_idx], y[train_idx], classes)\n",
    "        \n",
    "        y_pred = model.predict(X[test_idx])\n",
    "        fold_acc = accuracy_score(y[test_idx], y_pred)\n",
    "        \n",
    "        if best_score < 0 or best_score < (fold_acc - 0.02):\n",
    "            best_score = fold_acc\n",
    "            best_model = model\n",
    "\n",
    "        acc += fold_acc\n",
    "        print(\" (acc: {:.4f})\".format(fold_acc))\n",
    "\n",
    "    acc /= N_FOLDS\n",
    "    print(\"Mean accuracy on test set: {:.4f}\\n\".format(acc))\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def pac_test(X, y, index, splits, model_pretrained=None):\n",
    "    n_samples = X[index].shape[0]\n",
    "    acc_lst = list()\n",
    "    for fold_i in range(5):\n",
    "        print(\" Starting outer fold {} / {}\".format(fold_i+1, 5), end='')\n",
    "        acc_inner = list()\n",
    "        split_a_idx, split_b_idx  = splits[fold_i]\n",
    "        for fold_j in range(2):\n",
    "            print(\"  Starting inner fold {} / {}\".format(fold_j+1, 2), end='')\n",
    "            if fold_j % 2 == 0:\n",
    "                train_fold_idx, test_fold_idx  = split_a_idx, split_b_idx\n",
    "            else:\n",
    "                train_fold_idx, test_fold_idx  = split_b_idx, split_a_idx\n",
    "\n",
    "            train_idx = index[train_fold_idx]\n",
    "            test_idx = index[test_fold_idx]\n",
    "            \n",
    "            classes = np.unique(y)\n",
    "            if model_pretrained is None:\n",
    "                model = PassiveAggressiveClassifier(max_iter=2000, warm_start=False)\n",
    "            else:\n",
    "                model = clone(model_pretrained)\n",
    "            model.partial_fit(X[train_idx], y[train_idx], classes)\n",
    " \n",
    "            y_pred = model.predict(X[test_idx])\n",
    "            fold_acc = accuracy_score(y[test_idx], y_pred)\n",
    "\n",
    "            acc_inner.append(fold_acc)\n",
    "            print(\" (acc: {:.4f})\".format(fold_acc))\n",
    "            \n",
    "        acc_lst.append(np.array(acc_inner))\n",
    "    print(\" => mean acc: {:.4f}\\n\".format(np.mean(np.array([np.mean(inner) for inner in acc_lst]))))\n",
    "    \n",
    "    return acc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results of supervised learning on expert dominant labels ===\n",
      " Starting fold 1 / 5 (acc: 0.4615)\n",
      " Starting fold 2 / 5 (acc: 0.3846)\n",
      " Starting fold 3 / 5 (acc: 0.6154)\n",
      " Starting fold 4 / 5 (acc: 0.7692)\n",
      " Starting fold 5 / 5 (acc: 0.6154)\n",
      "Mean accuracy on test set: 0.5692\n",
      "\n",
      " Starting outer fold 1 / 5  Starting inner fold 1 / 2 (acc: 0.6333)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5714)\n",
      " Starting outer fold 2 / 5  Starting inner fold 1 / 2 (acc: 0.5667)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5357)\n",
      " Starting outer fold 3 / 5  Starting inner fold 1 / 2 (acc: 0.4667)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6071)\n",
      " Starting outer fold 4 / 5  Starting inner fold 1 / 2 (acc: 0.5333)\n",
      "  Starting inner fold 2 / 2 (acc: 0.6071)\n",
      " Starting outer fold 5 / 5  Starting inner fold 1 / 2 (acc: 0.6000)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4643)\n",
      " => mean acc: 0.5586\n",
      "\n",
      "=== Results of supervised learning on crowd dominant labels ===\n",
      " Starting fold 1 / 5 (acc: 0.4545)\n",
      " Starting fold 2 / 5 (acc: 0.5455)\n",
      " Starting fold 3 / 5 (acc: 0.4545)\n",
      " Starting fold 4 / 5 (acc: 0.4545)\n",
      " Starting fold 5 / 5 (acc: 0.5000)\n",
      "Mean accuracy on test set: 0.4818\n",
      "\n",
      " Starting outer fold 1 / 5  Starting inner fold 1 / 2 (acc: 0.5185)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4340)\n",
      " Starting outer fold 2 / 5  Starting inner fold 1 / 2 (acc: 0.5000)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5472)\n",
      " Starting outer fold 3 / 5  Starting inner fold 1 / 2 (acc: 0.4630)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4906)\n",
      " Starting outer fold 4 / 5  Starting inner fold 1 / 2 (acc: 0.5926)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4717)\n",
      " Starting outer fold 5 / 5  Starting inner fold 1 / 2 (acc: 0.5741)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5849)\n",
      " => mean acc: 0.5176\n",
      "\n",
      "=== Results of supervised learning of crowd labels on expert subset ===\n",
      " Starting fold 1 / 5 (acc: 0.7500)\n",
      " Starting fold 2 / 5 (acc: 0.7500)\n",
      " Starting fold 3 / 5 (acc: 0.6667)\n",
      " Starting fold 4 / 5 (acc: 0.7500)\n",
      " Starting fold 5 / 5 (acc: 0.6667)\n",
      "Mean accuracy on test set: 0.7167\n",
      "\n",
      " Starting outer fold 1 / 5  Starting inner fold 1 / 2 (acc: 0.5517)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5000)\n",
      " Starting outer fold 2 / 5  Starting inner fold 1 / 2 (acc: 0.4483)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5000)\n",
      " Starting outer fold 3 / 5  Starting inner fold 1 / 2 (acc: 0.5862)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5000)\n",
      " Starting outer fold 4 / 5  Starting inner fold 1 / 2 (acc: 0.5862)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5714)\n",
      " Starting outer fold 5 / 5  Starting inner fold 1 / 2 (acc: 0.6552)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5000)\n",
      " => mean acc: 0.5399\n",
      "\n",
      "=== Results of supervised learning on crowd unique dominant labels ===\n",
      " Starting fold 1 / 5 (acc: 0.2727)\n",
      " Starting fold 2 / 5 (acc: 0.5455)\n",
      " Starting fold 3 / 5 (acc: 0.3636)\n",
      " Starting fold 4 / 5 (acc: 0.4545)\n",
      " Starting fold 5 / 5 (acc: 0.3636)\n",
      "Mean accuracy on test set: 0.4000\n",
      "\n",
      " Starting outer fold 1 / 5  Starting inner fold 1 / 2 (acc: 0.3462)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5417)\n",
      " Starting outer fold 2 / 5  Starting inner fold 1 / 2 (acc: 0.5385)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4583)\n",
      " Starting outer fold 3 / 5  Starting inner fold 1 / 2 (acc: 0.4231)\n",
      "  Starting inner fold 2 / 2 (acc: 0.3750)\n",
      " Starting outer fold 4 / 5  Starting inner fold 1 / 2 (acc: 0.5000)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5000)\n",
      " Starting outer fold 5 / 5  Starting inner fold 1 / 2 (acc: 0.4615)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5417)\n",
      " => mean acc: 0.4686\n",
      "\n",
      "=== Results of supervised learning on combined dominant labels ===\n",
      " Starting fold 1 / 5 (acc: 0.3913)\n",
      " Starting fold 2 / 5 (acc: 0.4783)\n",
      " Starting fold 3 / 5 (acc: 0.4348)\n",
      " Starting fold 4 / 5 (acc: 0.4348)\n",
      " Starting fold 5 / 5 (acc: 0.3913)\n",
      "Mean accuracy on test set: 0.4261\n",
      "\n",
      " Starting outer fold 1 / 5  Starting inner fold 1 / 2 (acc: 0.5926)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5185)\n",
      " Starting outer fold 2 / 5  Starting inner fold 1 / 2 (acc: 0.5741)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5000)\n",
      " Starting outer fold 3 / 5  Starting inner fold 1 / 2 (acc: 0.5370)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5370)\n",
      " Starting outer fold 4 / 5  Starting inner fold 1 / 2 (acc: 0.5370)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5926)\n",
      " Starting outer fold 5 / 5  Starting inner fold 1 / 2 (acc: 0.5741)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5556)\n",
      " => mean acc: 0.5519\n",
      "\n",
      "= p-values =\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p-values</th>\n",
       "      <th>pac_acc_experts_dominant</th>\n",
       "      <th>pac_acc_crowd_dominant</th>\n",
       "      <th>pac_acc_crowd_expert_dominant</th>\n",
       "      <th>pac_acc_crowd_unique_dominant</th>\n",
       "      <th>pac_acc_combined_dominant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pac_acc_experts_dominant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pac_acc_crowd_dominant</td>\n",
       "      <td>0.483699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pac_acc_crowd_expert_dominant</td>\n",
       "      <td>0.576215</td>\n",
       "      <td>0.568319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pac_acc_crowd_unique_dominant</td>\n",
       "      <td>0.455372</td>\n",
       "      <td>0.675712</td>\n",
       "      <td>0.491883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pac_acc_combined_dominant</td>\n",
       "      <td>0.708626</td>\n",
       "      <td>0.545506</td>\n",
       "      <td>0.692186</td>\n",
       "      <td>0.361084</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        p-values  pac_acc_experts_dominant  \\\n",
       "0       pac_acc_experts_dominant                       NaN   \n",
       "1         pac_acc_crowd_dominant                  0.483699   \n",
       "2  pac_acc_crowd_expert_dominant                  0.576215   \n",
       "3  pac_acc_crowd_unique_dominant                  0.455372   \n",
       "4      pac_acc_combined_dominant                  0.708626   \n",
       "\n",
       "   pac_acc_crowd_dominant  pac_acc_crowd_expert_dominant  \\\n",
       "0                     NaN                            NaN   \n",
       "1                     NaN                            NaN   \n",
       "2                0.568319                            NaN   \n",
       "3                0.675712                       0.491883   \n",
       "4                0.545506                       0.692186   \n",
       "\n",
       "   pac_acc_crowd_unique_dominant  pac_acc_combined_dominant  \n",
       "0                            NaN                        NaN  \n",
       "1                            NaN                        NaN  \n",
       "2                            NaN                        NaN  \n",
       "3                            NaN                        NaN  \n",
       "4                       0.361084                        NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=== Results of supervised learning on expert dominant labels ===\")\n",
    "model = pac(X_2D,\n",
    "            y_experts, \n",
    "            experts_idx)\n",
    "pac_acc_experts_dominant = pac_test(X_2D,\n",
    "                                  y_experts, \n",
    "                                  experts_idx,\n",
    "                                  experts_splits,\n",
    "                                  model)\n",
    "\n",
    "print(\"=== Results of supervised learning on crowd dominant labels ===\")\n",
    "model = pac(X_2D,\n",
    "            y_crowd, \n",
    "            crowd_idx)\n",
    "pac_acc_crowd_dominant = pac_test(X_2D,\n",
    "                                y_crowd,\n",
    "                                crowd_idx,\n",
    "                                crowd_splits,\n",
    "                                model)\n",
    "\n",
    "print(\"=== Results of supervised learning of crowd labels on expert subset ===\")\n",
    "model = pac(X_2D,\n",
    "            y_crowd, \n",
    "            experts_idx)\n",
    "pac_acc_crowd_expert_dominant = pac_test(X_2D,\n",
    "                                y_crowd,\n",
    "                                experts_idx,\n",
    "                                crowd_on_experts_splits,\n",
    "                                model)\n",
    "\n",
    "print(\"=== Results of supervised learning on crowd unique dominant labels ===\")\n",
    "model = pac(X_2D,\n",
    "            y_crowd, \n",
    "            _crowd_unique_idx)\n",
    "pac_acc_crowd_unique_dominant = pac_test(X_2D,\n",
    "                                y_crowd,\n",
    "                                _crowd_unique_idx,\n",
    "                                crowd_unique_splits,\n",
    "                                model)\n",
    "\n",
    "print(\"=== Results of supervised learning on combined dominant labels ===\")\n",
    "model = pac(X_2D,\n",
    "            y_combined, \n",
    "            combined_idx)\n",
    "pac_acc_combined_dominant = pac_test(X_2D,\n",
    "                                   y_combined,\n",
    "                                   combined_idx,\n",
    "                                   combined_splits,\n",
    "                                   model)\n",
    "\n",
    "pac_acc = [pac_acc_experts_dominant, pac_acc_crowd_dominant, pac_acc_crowd_expert_dominant, pac_acc_crowd_unique_dominant, pac_acc_combined_dominant]\n",
    "labels = ['pac_acc_experts_dominant', 'pac_acc_crowd_dominant', 'pac_acc_crowd_expert_dominant', 'pac_acc_crowd_unique_dominant', 'pac_acc_combined_dominant']\n",
    "print(\"= p-values =\")\n",
    "table = {'p-values': labels}\n",
    "table.update({lab: list() for lab in labels})\n",
    "nlabels = len(labels)\n",
    "for i in range(nlabels):\n",
    "    for e in range(i+1):\n",
    "        table[labels[i]].append(np.nan)\n",
    "    for j in range(i+1, nlabels):\n",
    "        f, p, mean, variance = alpaydin_F_test(pac_acc[i],\n",
    "                                               pac_acc[j])\n",
    "        table[labels[i]].append(p)\n",
    "        #print(\"RF {} vs {} estimators\".format(pac_acc[i], pac_acc[j]))\n",
    "        #print(\" f: {:.4f}, p: {:.4f}, mean: {:.4f}, var: {:.4f}\".format(f, p, mean, variance))\n",
    "\n",
    "significance = pd.DataFrame(table)\n",
    "display(significance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pretrain expert labels on expert subset and further train crowd labels on crowd subset ===\n",
      " Starting fold 1 / 5 (acc: 0.6154)\n",
      " Starting fold 2 / 5 (acc: 0.7692)\n",
      " Starting fold 3 / 5 (acc: 0.5385)\n",
      " Starting fold 4 / 5 (acc: 0.6154)\n",
      " Starting fold 5 / 5 (acc: 0.6154)\n",
      "Mean accuracy on test set: 0.6308\n",
      "\n",
      " Starting outer fold 1 / 5  Starting inner fold 1 / 2 (acc: 0.5926)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5472)\n",
      " Starting outer fold 2 / 5  Starting inner fold 1 / 2 (acc: 0.5185)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4906)\n",
      " Starting outer fold 3 / 5  Starting inner fold 1 / 2 (acc: 0.5000)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4717)\n",
      " Starting outer fold 4 / 5  Starting inner fold 1 / 2 (acc: 0.5556)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5094)\n",
      " Starting outer fold 5 / 5  Starting inner fold 1 / 2 (acc: 0.5185)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5660)\n",
      " => mean acc: 0.5270\n",
      "\n",
      "=== Pretrain expert labels on expert subset and further train crowd labels on expert subset ===\n",
      " Starting fold 1 / 5 (acc: 0.6923)\n",
      " Starting fold 2 / 5 (acc: 0.7692)\n",
      " Starting fold 3 / 5 (acc: 0.6154)\n",
      " Starting fold 4 / 5 (acc: 0.5385)\n",
      " Starting fold 5 / 5 (acc: 0.5385)\n",
      "Mean accuracy on test set: 0.6308\n",
      "\n",
      " Starting outer fold 1 / 5  Starting inner fold 1 / 2 (acc: 0.5517)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4286)\n",
      " Starting outer fold 2 / 5  Starting inner fold 1 / 2 (acc: 0.5172)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4286)\n",
      " Starting outer fold 3 / 5  Starting inner fold 1 / 2 (acc: 0.6207)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4643)\n",
      " Starting outer fold 4 / 5  Starting inner fold 1 / 2 (acc: 0.6552)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5000)\n",
      " Starting outer fold 5 / 5  Starting inner fold 1 / 2 (acc: 0.5862)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5000)\n",
      " => mean acc: 0.5252\n",
      "\n",
      "=== Pretrain expert labels on expert subset and further train crowd labels on unique subset ===\n",
      " Starting fold 1 / 5 (acc: 0.6923)\n",
      " Starting fold 2 / 5 (acc: 0.5385)\n",
      " Starting fold 3 / 5 (acc: 0.3846)\n",
      " Starting fold 4 / 5 (acc: 0.5385)\n",
      " Starting fold 5 / 5 (acc: 0.6154)\n",
      "Mean accuracy on test set: 0.5538\n",
      "\n",
      " Starting outer fold 1 / 5  Starting inner fold 1 / 2 (acc: 0.4615)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5417)\n",
      " Starting outer fold 2 / 5  Starting inner fold 1 / 2 (acc: 0.4615)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4167)\n",
      " Starting outer fold 3 / 5  Starting inner fold 1 / 2 (acc: 0.5000)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4583)\n",
      " Starting outer fold 4 / 5  Starting inner fold 1 / 2 (acc: 0.4231)\n",
      "  Starting inner fold 2 / 2 (acc: 0.3333)\n",
      " Starting outer fold 5 / 5  Starting inner fold 1 / 2 (acc: 0.4615)\n",
      "  Starting inner fold 2 / 2 (acc: 0.4583)\n",
      " => mean acc: 0.4516\n",
      "\n",
      "=== Pretrain expert labels on expert subset and further train combined labels on all ===\n",
      " Starting fold 1 / 5 (acc: 0.6154)\n",
      " Starting fold 2 / 5 (acc: 0.6923)\n",
      " Starting fold 3 / 5 (acc: 0.6154)\n",
      " Starting fold 4 / 5 (acc: 0.4615)\n",
      " Starting fold 5 / 5 (acc: 0.4615)\n",
      "Mean accuracy on test set: 0.5692\n",
      "\n",
      " Starting outer fold 1 / 5  Starting inner fold 1 / 2 (acc: 0.5556)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5370)\n",
      " Starting outer fold 2 / 5  Starting inner fold 1 / 2 (acc: 0.5556)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5926)\n",
      " Starting outer fold 3 / 5  Starting inner fold 1 / 2 (acc: 0.5556)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5370)\n",
      " Starting outer fold 4 / 5  Starting inner fold 1 / 2 (acc: 0.6481)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5185)\n",
      " Starting outer fold 5 / 5  Starting inner fold 1 / 2 (acc: 0.5741)\n",
      "  Starting inner fold 2 / 2 (acc: 0.5556)\n",
      " => mean acc: 0.5630\n",
      "\n",
      "= p-values =\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p-values</th>\n",
       "      <th>pacv_acc_experts_dominant</th>\n",
       "      <th>pacv_acc_crowd_dominant</th>\n",
       "      <th>pacv_acc_crowd_unique_dominant</th>\n",
       "      <th>pacv_acc_combined_dominant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pacv_acc_experts_dominant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pacv_acc_crowd_dominant</td>\n",
       "      <td>0.559912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pacv_acc_crowd_unique_dominant</td>\n",
       "      <td>0.059888</td>\n",
       "      <td>0.223629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pacv_acc_combined_dominant</td>\n",
       "      <td>0.233069</td>\n",
       "      <td>0.512902</td>\n",
       "      <td>0.013538</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         p-values  pacv_acc_experts_dominant  \\\n",
       "0       pacv_acc_experts_dominant                        NaN   \n",
       "1         pacv_acc_crowd_dominant                   0.559912   \n",
       "2  pacv_acc_crowd_unique_dominant                   0.059888   \n",
       "3      pacv_acc_combined_dominant                   0.233069   \n",
       "\n",
       "   pacv_acc_crowd_dominant  pacv_acc_crowd_unique_dominant  \\\n",
       "0                      NaN                             NaN   \n",
       "1                      NaN                             NaN   \n",
       "2                 0.223629                             NaN   \n",
       "3                 0.512902                        0.013538   \n",
       "\n",
       "   pacv_acc_combined_dominant  \n",
       "0                         NaN  \n",
       "1                         NaN  \n",
       "2                         NaN  \n",
       "3                         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=== Pretrain expert labels on expert subset and further train crowd labels on crowd subset ===\")\n",
    "model = pac(X_2D,\n",
    "                 y_experts, \n",
    "                 experts_idx)\n",
    "pacv_acc_experts_dominant = pac_test(X_2D,\n",
    "                                  y_crowd, \n",
    "                                  crowd_idx,\n",
    "                                  crowd_splits,\n",
    "                                  model)\n",
    "\n",
    "print(\"=== Pretrain expert labels on expert subset and further train crowd labels on expert subset ===\")\n",
    "model = pac(X_2D,\n",
    "                 y_experts, \n",
    "                 experts_idx)\n",
    "pacv_acc_crowd_dominant = pac_test(X_2D,\n",
    "                                y_crowd,\n",
    "                                experts_idx,\n",
    "                                crowd_on_experts_splits,\n",
    "                                model)\n",
    "\n",
    "print(\"=== Pretrain expert labels on expert subset and further train crowd labels on unique subset ===\")\n",
    "model = pac(X_2D,\n",
    "                 y_experts, \n",
    "                 experts_idx)\n",
    "pacv_acc_crowd_unique_dominant = pac_test(X_2D,\n",
    "                                y_crowd,\n",
    "                                _crowd_unique_idx,\n",
    "                                crowd_unique_splits,\n",
    "                                model)\n",
    "\n",
    "print(\"=== Pretrain expert labels on expert subset and further train combined labels on all ===\")\n",
    "model = pac(X_2D,\n",
    "                 y_experts, \n",
    "                 experts_idx)\n",
    "pacv_acc_combined_dominant = pac_test(X_2D,\n",
    "                                   y_combined,\n",
    "                                   combined_idx,\n",
    "                                   combined_splits,\n",
    "                                   model)\n",
    "\n",
    "pac_acc = [pacv_acc_experts_dominant, pacv_acc_crowd_dominant, pacv_acc_crowd_unique_dominant, pacv_acc_combined_dominant]\n",
    "labels = ['pacv_acc_experts_dominant', 'pacv_acc_crowd_dominant', 'pacv_acc_crowd_unique_dominant', 'pacv_acc_combined_dominant']\n",
    "\n",
    "print(\"= p-values =\")\n",
    "table = {'p-values': labels}\n",
    "table.update({lab: list() for lab in labels})\n",
    "nlabels = len(labels)\n",
    "for i in range(nlabels):\n",
    "    for e in range(i+1):\n",
    "        table[labels[i]].append(np.nan)\n",
    "    for j in range(i+1, nlabels):\n",
    "        f, p, mean, variance = alpaydin_F_test(pac_acc[i],\n",
    "                                               pac_acc[j])\n",
    "        table[labels[i]].append(p)\n",
    "        #print(\"RF {} vs {} estimators\".format(pac_acc[i], pac_acc[j]))\n",
    "        #print(\" f: {:.4f}, p: {:.4f}, mean: {:.4f}, var: {:.4f}\".format(f, p, mean, variance))\n",
    "\n",
    "significance = pd.DataFrame(table)\n",
    "display(significance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert numpy arrays to PyTorch tensors\n",
    "X_2D = torch.from_numpy(X_2D)\n",
    "X_3D = torch.from_numpy(X_3D)\n",
    "\n",
    "y_crowd = torch.from_numpy(y_crowd)\n",
    "y_experts = torch.from_numpy(y_experts)\n",
    "y_combined = torch.from_numpy(y_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(y_hat, y):\n",
    "    # y := 1D array of class labels\n",
    "    # y_hat := 2D array of one-hot class labels\n",
    "    _, labels = y_hat.max(dim=1)\n",
    "    return torch.mean(torch.eq(labels, y).float())\n",
    "\n",
    "def fit(model, X, y, index, lr=0.01, l2norm=0.01, n_folds=10, n_epoch=250, patience=-1, state=None, finetune=False):\n",
    "    n_samples = X[index].shape[0]\n",
    "\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    best_state = None\n",
    "    best_state_opt = None\n",
    "    best_score = -1\n",
    "    for fold_i in range(n_folds):\n",
    "        print(\"Starting fold {} / {}\".format(fold_i+1, n_folds), end='')\n",
    "        if state is None:\n",
    "            model.init()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "        else:\n",
    "            model.load_state_dict(state[0])\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "            optimizer.load_state_dict(state[1])\n",
    "            if finetune:\n",
    "                for layer in model.layers[:-1]:\n",
    "                    layer.requires_grad = False\n",
    "            \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # early stopping\n",
    "        patience_left = patience\n",
    "        best_fold_score = -1\n",
    "        delta = 1e-4\n",
    "        best_fold_state = None\n",
    "        best_fold_state_opt = None\n",
    "        \n",
    "        train_fold_idx, test_fold_idx  = create_splits(y[index])\n",
    "        train_idx = index[train_fold_idx]\n",
    "        test_idx = index[test_fold_idx]\n",
    "        for epoch in range(n_epoch):\n",
    "            model.train()\n",
    "            \n",
    "            y_hat = model(X[train_idx].float())\n",
    "            train_acc = categorical_accuracy(y_hat, y[train_idx])\n",
    "            train_loss = criterion(y_hat, y[train_idx].long())\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            test_loss = None\n",
    "            with torch.no_grad():\n",
    "                y_hat = model(X[test_idx].float())\n",
    "                test_acc = categorical_accuracy(y_hat, y[test_idx])\n",
    "                test_loss = criterion(y_hat, y[test_idx].long())\n",
    "                \n",
    "            train_loss = float(train_loss.item())\n",
    "            test_loss = float(test_loss.item())\n",
    "\n",
    "            if best_fold_score < 0:\n",
    "                best_fold_score = test_loss\n",
    "                best_fold_state = model.state_dict()\n",
    "                best_fold_state_opt = optimizer.state_dict()\n",
    "                            \n",
    "            if patience <= 0:\n",
    "                continue\n",
    "            if test_loss >= best_fold_score - delta:\n",
    "                patience_left -= 1\n",
    "            else:\n",
    "                best_fold_score = test_loss\n",
    "                best_fold_state = model.state_dict()\n",
    "                best_fold_state_opt = optimizer.state_dict()\n",
    "                patience_left = patience\n",
    "            if patience_left <= 0:\n",
    "                model.load_state_dict(best_fold_state)\n",
    "                optimizer.load_state_dict(best_fold_state_opt)\n",
    "                break\n",
    "                \n",
    "        test_idx = index[create_splits(y[index])[1]]  # get new random test set to validate on\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(X[test_idx].float())\n",
    "            test_acc = categorical_accuracy(y_hat, y[test_idx])\n",
    "            test_loss = float(criterion(y_hat, y[test_idx].long()).item())\n",
    "        \n",
    "        loss += test_loss\n",
    "        acc += test_acc\n",
    "        if best_score < 0 or best_score > test_loss:\n",
    "            best_state = best_fold_state\n",
    "            best_state_opt = best_fold_state_opt\n",
    "            best_score = test_loss\n",
    "        print(\" - training accuracy: {:.4f} / loss: {:.4f} - test accuracy: {:.4f} / loss: {:.4f}\".format(train_acc,\n",
    "                                                                                          train_loss,\n",
    "                                                                                          test_acc,\n",
    "                                                                                          test_loss))\n",
    "        \n",
    "    loss /= n_folds\n",
    "    acc /= n_folds\n",
    "    print(\"average loss on test set: {:.4f}\".format(loss))\n",
    "    print(\"average accuracy on test set: {:.4f}\".format(acc))\n",
    "    \n",
    "    return (best_state, best_state_opt)\n",
    "\n",
    "def fit_test(model, X, y, index, splits, lr=0.01, l2norm=0.01, n_epoch=250, patience=-1, state=None, finetune=False):\n",
    "    n_samples = X[index].shape[0]\n",
    "    acc_lst = list()\n",
    "    for fold_i in range(5):\n",
    "        print(\"Starting outer fold {} / {}\".format(fold_i+1, 5))\n",
    "        acc_inner = list()\n",
    "        split_a_idx, split_b_idx  = splits[fold_i]\n",
    "\n",
    "        for fold_j in range(2):\n",
    "            print(\" Starting inner fold {} / {}\".format(fold_j+1, 2), end='')\n",
    "            if fold_j % 2 == 0:\n",
    "                train_fold_idx, test_fold_idx  = split_a_idx, split_b_idx\n",
    "            else:\n",
    "                train_fold_idx, test_fold_idx  = split_b_idx, split_a_idx\n",
    "                \n",
    "            train_idx = index[train_fold_idx]\n",
    "            test_idx = index[test_fold_idx]\n",
    "        \n",
    "            if state is None:\n",
    "                model.init()\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "            else:\n",
    "                model.load_state_dict(state[0])\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "                optimizer.load_state_dict(state[1])\n",
    "                if finetune:\n",
    "                    for layer in model.layers[:-1]:\n",
    "                        layer.requires_grad = False\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            # early stopping\n",
    "            patience_left = patience\n",
    "            best_fold_score = -1\n",
    "            delta = 1e-4\n",
    "            best_fold_state = None\n",
    "            best_fold_state_opt = None\n",
    "        \n",
    "            for epoch in range(n_epoch):\n",
    "                model.train()\n",
    "\n",
    "                y_hat = model(X[train_idx].float())\n",
    "                train_acc = categorical_accuracy(y_hat, y[train_idx])\n",
    "                train_loss = criterion(y_hat, y[train_idx].long())\n",
    "                optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                model.eval()\n",
    "                test_loss = None\n",
    "                with torch.no_grad():\n",
    "                    y_hat = model(X[test_idx].float())\n",
    "                    test_acc = categorical_accuracy(y_hat, y[test_idx])\n",
    "                    test_loss = criterion(y_hat, y[test_idx].long())\n",
    "\n",
    "                train_loss = float(train_loss.item())\n",
    "                test_loss = float(test_loss.item())\n",
    "                \n",
    "                if best_fold_score < 0:\n",
    "                    best_fold_score = test_loss\n",
    "                    best_fold_state = model.state_dict()\n",
    "                    best_fold_state_opt = optimizer.state_dict()\n",
    "\n",
    "                if patience <= 0:\n",
    "                    continue\n",
    "                if test_loss >= best_fold_score - delta:\n",
    "                    patience_left -= 1\n",
    "                else:\n",
    "                    best_fold_score = test_loss\n",
    "                    best_fold_state = model.state_dict()\n",
    "                    best_fold_state_opt = optimizer.state_dict()\n",
    "                    patience_left = patience\n",
    "                if patience_left <= 0:\n",
    "                    model.load_state_dict(best_fold_state)\n",
    "                    optimizer.load_state_dict(best_fold_state_opt)\n",
    "                    break\n",
    "            \n",
    "            # do a final run over the test set after loading a previous state\n",
    "            with torch.no_grad():\n",
    "                y_hat = model(X[test_idx].float())\n",
    "                test_acc = categorical_accuracy(y_hat, y[test_idx])\n",
    "                test_loss = criterion(y_hat, y[test_idx].long())\n",
    "            \n",
    "            test_loss = float(test_loss.item())\n",
    "            print(\" (acc: {:.4f})\".format(test_acc))\n",
    "            acc_inner.append(test_acc)\n",
    "    \n",
    "        acc_lst.append(np.array(acc_inner))\n",
    "        \n",
    "    print(\" => mean acc: {:.4f}\\n\".format(np.mean(np.array([np.mean(inner) for inner in acc_lst]))))\n",
    "\n",
    "    return acc_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierNN(nn.Module):\n",
    "    \"\"\"Simple Neural Network Classifier\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, p_dropout=0.05):\n",
    "        super().__init__()\n",
    "        hidden_dim = (input_dim-output_dim)//2\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Sequential(\n",
    "                            nn.Linear(input_dim, hidden_dim),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Dropout(p=p_dropout)))\n",
    "            \n",
    "        self.layers.append(nn.Sequential(\n",
    "                            nn.Linear(hidden_dim, output_dim),\n",
    "                            nn.ReLU(inplace=True)))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)                          \n",
    "                           \n",
    "        return self.softmax(X)\n",
    "        \n",
    "    def init(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.normal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results on expert dominant labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.7143 / loss: 0.5112 - test accuracy: 0.6333 / loss: 0.6660\n",
      "Starting fold 2 / 10 - training accuracy: 0.9286 / loss: 0.3849 - test accuracy: 0.4667 / loss: 0.7976\n",
      "Starting fold 3 / 10 - training accuracy: 0.9286 / loss: 0.3694 - test accuracy: 0.5333 / loss: 0.7229\n",
      "Starting fold 4 / 10 - training accuracy: 0.9643 / loss: 0.3463 - test accuracy: 0.5667 / loss: 0.7055\n",
      "Starting fold 5 / 10 - training accuracy: 1.0000 / loss: 0.3406 - test accuracy: 0.5333 / loss: 0.7357\n",
      "Starting fold 6 / 10 - training accuracy: 0.8571 / loss: 0.4617 - test accuracy: 0.5000 / loss: 0.7685\n",
      "Starting fold 7 / 10 - training accuracy: 0.9643 / loss: 0.3770 - test accuracy: 0.7000 / loss: 0.6450\n",
      "Starting fold 8 / 10 - training accuracy: 1.0000 / loss: 0.3256 - test accuracy: 0.5667 / loss: 0.7209\n",
      "Starting fold 9 / 10 - training accuracy: 0.9286 / loss: 0.3627 - test accuracy: 0.5667 / loss: 0.7257\n",
      "Starting fold 10 / 10 - training accuracy: 0.9643 / loss: 0.3554 - test accuracy: 0.5333 / loss: 0.7046\n",
      "average loss on test set: 0.7192\n",
      "average accuracy on test set: 0.5600\n",
      "Starting outer fold 1 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5667)\n",
      " Starting inner fold 2 / 2 (acc: 0.6786)\n",
      "Starting outer fold 2 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5667)\n",
      " Starting inner fold 2 / 2 (acc: 0.6786)\n",
      "Starting outer fold 3 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5667)\n",
      " Starting inner fold 2 / 2 (acc: 0.6071)\n",
      "Starting outer fold 4 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5667)\n",
      " Starting inner fold 2 / 2 (acc: 0.6429)\n",
      "Starting outer fold 5 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5667)\n",
      " Starting inner fold 2 / 2 (acc: 0.6429)\n",
      " => mean acc: 0.6083\n",
      "\n",
      "\n",
      "=== Results on crowd dominant labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.8679 / loss: 0.4174 - test accuracy: 0.5741 / loss: 0.7188\n",
      "Starting fold 2 / 10 - training accuracy: 0.9057 / loss: 0.3924 - test accuracy: 0.5741 / loss: 0.7389\n",
      "Starting fold 3 / 10 - training accuracy: 0.8868 / loss: 0.4415 - test accuracy: 0.5185 / loss: 0.7371\n",
      "Starting fold 4 / 10 - training accuracy: 0.9245 / loss: 0.4067 - test accuracy: 0.5556 / loss: 0.7122\n",
      "Starting fold 5 / 10 - training accuracy: 0.8491 / loss: 0.4524 - test accuracy: 0.5741 / loss: 0.7217\n",
      "Starting fold 6 / 10 - training accuracy: 0.9623 / loss: 0.3720 - test accuracy: 0.5741 / loss: 0.7126\n",
      "Starting fold 7 / 10 - training accuracy: 0.8113 / loss: 0.4997 - test accuracy: 0.5926 / loss: 0.7120\n",
      "Starting fold 8 / 10 - training accuracy: 0.8491 / loss: 0.4307 - test accuracy: 0.5741 / loss: 0.7436\n",
      "Starting fold 9 / 10 - training accuracy: 0.9623 / loss: 0.3502 - test accuracy: 0.5556 / loss: 0.7358\n",
      "Starting fold 10 / 10 - training accuracy: 0.7358 / loss: 0.4594 - test accuracy: 0.6667 / loss: 0.6688\n",
      "average loss on test set: 0.7201\n",
      "average accuracy on test set: 0.5759\n",
      "Starting outer fold 1 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5741)\n",
      " Starting inner fold 2 / 2 (acc: 0.5849)\n",
      "Starting outer fold 2 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5926)\n",
      " Starting inner fold 2 / 2 (acc: 0.5849)\n",
      "Starting outer fold 3 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5741)\n",
      " Starting inner fold 2 / 2 (acc: 0.5283)\n",
      "Starting outer fold 4 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5926)\n",
      " Starting inner fold 2 / 2 (acc: 0.5660)\n",
      "Starting outer fold 5 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5741)\n",
      " Starting inner fold 2 / 2 (acc: 0.5849)\n",
      " => mean acc: 0.5756\n",
      "\n",
      "\n",
      "=== Results on combined dominant labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.7778 / loss: 0.4503 - test accuracy: 0.5741 / loss: 0.6902\n",
      "Starting fold 2 / 10 - training accuracy: 0.9630 / loss: 0.3734 - test accuracy: 0.5926 / loss: 0.6949\n",
      "Starting fold 3 / 10 - training accuracy: 0.8889 / loss: 0.4194 - test accuracy: 0.5741 / loss: 0.6971\n",
      "Starting fold 4 / 10 - training accuracy: 0.8148 / loss: 0.4993 - test accuracy: 0.5556 / loss: 0.6449\n",
      "Starting fold 5 / 10 - training accuracy: 0.7963 / loss: 0.4349 - test accuracy: 0.5741 / loss: 0.7037\n",
      "Starting fold 6 / 10 - training accuracy: 0.8519 / loss: 0.4563 - test accuracy: 0.5556 / loss: 0.6991\n",
      "Starting fold 7 / 10 - training accuracy: 0.9815 / loss: 0.3721 - test accuracy: 0.5741 / loss: 0.7233\n",
      "Starting fold 8 / 10 - training accuracy: 0.8889 / loss: 0.4530 - test accuracy: 0.5370 / loss: 0.7407\n",
      "Starting fold 9 / 10 - training accuracy: 0.8704 / loss: 0.4428 - test accuracy: 0.5185 / loss: 0.7115\n",
      "Starting fold 10 / 10 - training accuracy: 0.8704 / loss: 0.4257 - test accuracy: 0.5556 / loss: 0.6696\n",
      "average loss on test set: 0.6975\n",
      "average accuracy on test set: 0.5611\n",
      "Starting outer fold 1 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.6111)\n",
      " Starting inner fold 2 / 2 (acc: 0.6481)\n",
      "Starting outer fold 2 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.6111)\n",
      " Starting inner fold 2 / 2 (acc: 0.6296)\n",
      "Starting outer fold 3 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.6111)\n",
      " Starting inner fold 2 / 2 (acc: 0.6296)\n",
      "Starting outer fold 4 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5926)\n",
      " Starting inner fold 2 / 2 (acc: 0.6296)\n",
      "Starting outer fold 5 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.6111)\n",
      " Starting inner fold 2 / 2 (acc: 0.6296)\n",
      " => mean acc: 0.6204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## hyperparameters\n",
    "lr = 0.01\n",
    "n_epoch = 250\n",
    "p_dropout = 0.1\n",
    "\n",
    "## define model\n",
    "indim = X_2D.shape[1]\n",
    "outdim = np.unique(y_experts[experts_idx]).shape[0]\n",
    "assert outdim == np.unique(y_crowd[crowd_idx]).shape[0]\n",
    "\n",
    "model = ClassifierNN(input_dim=indim,\n",
    "                     output_dim=outdim,\n",
    "                     p_dropout=p_dropout)\n",
    "\n",
    "print(\"=== Results on expert dominant labels ===\")\n",
    "state = fit(model, X_2D, y_experts, experts_idx, lr=lr, n_epoch=n_epoch)\n",
    "neural_net_acc_dominant_experts = fit_test(model, X_2D, y_experts, experts_idx, experts_splits, lr=lr, n_epoch=n_epoch, state=state)\n",
    "\n",
    "print(\"\\n=== Results on crowd dominant labels ===\")\n",
    "state = fit(model, X_2D, y_crowd, crowd_idx, lr=lr, n_epoch=n_epoch)\n",
    "neural_net_acc_dominant_crowd = fit_test(model, X_2D, y_crowd, crowd_idx, crowd_splits, lr=lr, n_epoch=n_epoch, state=state)\n",
    "\n",
    "print(\"\\n=== Results on combined dominant labels ===\")\n",
    "state = fit(model, X_2D, y_combined, combined_idx, lr=lr, n_epoch=n_epoch)\n",
    "neural_net_acc_dominant_combined = fit_test(model, X_2D, y_combined, combined_idx, combined_splits, lr=lr, n_epoch=n_epoch, state=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= p-values =\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p-values</th>\n",
       "      <th>neural_net_acc_dominant_experts</th>\n",
       "      <th>neural_net_acc_dominant_crowd</th>\n",
       "      <th>neural_net_acc_dominant_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neural_net_acc_dominant_experts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neural_net_acc_dominant_crowd</td>\n",
       "      <td>0.681507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neural_net_acc_dominant_combined</td>\n",
       "      <td>0.738612</td>\n",
       "      <td>0.134837</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           p-values  neural_net_acc_dominant_experts  \\\n",
       "0   neural_net_acc_dominant_experts                              NaN   \n",
       "1     neural_net_acc_dominant_crowd                         0.681507   \n",
       "2  neural_net_acc_dominant_combined                         0.738612   \n",
       "\n",
       "   neural_net_acc_dominant_crowd  neural_net_acc_dominant_combined  \n",
       "0                            NaN                               NaN  \n",
       "1                            NaN                               NaN  \n",
       "2                       0.134837                               NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_acc = [neural_net_acc_dominant_experts, neural_net_acc_dominant_crowd, neural_net_acc_dominant_combined]\n",
    "labels = ['neural_net_acc_dominant_experts', 'neural_net_acc_dominant_crowd', 'neural_net_acc_dominant_combined']\n",
    "\n",
    "print(\"= p-values =\")\n",
    "table = {'p-values': labels}\n",
    "table.update({lab: list() for lab in labels})\n",
    "nlabels = len(labels)\n",
    "for i in range(nlabels):\n",
    "    for e in range(i+1):\n",
    "        table[labels[i]].append(np.nan)\n",
    "    for j in range(i+1, nlabels):\n",
    "        f, p, mean, variance = alpaydin_F_test(nn_acc[i],\n",
    "                                               nn_acc[j])\n",
    "        table[labels[i]].append(p)\n",
    "        #print(\"RF {} vs {} estimators\".format(nn_acc[i], nn_acc[j]))\n",
    "        #print(\" f: {:.4f}, p: {:.4f}, mean: {:.4f}, var: {:.4f}\".format(f, p, mean, variance))\n",
    "\n",
    "significance = pd.DataFrame(table)\n",
    "display(significance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pretrain on expert dominant labels; tune on crowd labels on crowd subset ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.9286 / loss: 0.3653 - test accuracy: 0.6333 / loss: 0.6992\n",
      "Starting fold 2 / 10 - training accuracy: 0.8571 / loss: 0.4039 - test accuracy: 0.5333 / loss: 0.6851\n",
      "Starting fold 3 / 10 - training accuracy: 0.9286 / loss: 0.4025 - test accuracy: 0.4667 / loss: 0.7599\n",
      "Starting fold 4 / 10 - training accuracy: 0.9643 / loss: 0.3605 - test accuracy: 0.5667 / loss: 0.6913\n",
      "Starting fold 5 / 10 - training accuracy: 0.9643 / loss: 0.3588 - test accuracy: 0.6000 / loss: 0.6727\n",
      "Starting fold 6 / 10 - training accuracy: 1.0000 / loss: 0.3267 - test accuracy: 0.5333 / loss: 0.7595\n",
      "Starting fold 7 / 10 - training accuracy: 0.8214 / loss: 0.4798 - test accuracy: 0.6333 / loss: 0.6427\n",
      "Starting fold 8 / 10 - training accuracy: 0.9643 / loss: 0.3631 - test accuracy: 0.6333 / loss: 0.6786\n",
      "Starting fold 9 / 10 - training accuracy: 0.8929 / loss: 0.4510 - test accuracy: 0.6000 / loss: 0.7115\n",
      "Starting fold 10 / 10 - training accuracy: 0.8571 / loss: 0.3997 - test accuracy: 0.6000 / loss: 0.7105\n",
      "average loss on test set: 0.7011\n",
      "average accuracy on test set: 0.5800\n",
      "Starting fold 1 / 10 - training accuracy: 0.9811 / loss: 0.3486 - test accuracy: 0.5000 / loss: 0.7712\n",
      "Starting fold 2 / 10 - training accuracy: 0.9811 / loss: 0.3442 - test accuracy: 0.5000 / loss: 0.7678\n",
      "Starting fold 3 / 10 - training accuracy: 0.9811 / loss: 0.3570 - test accuracy: 0.5000 / loss: 0.7615\n",
      "Starting fold 4 / 10 - training accuracy: 0.9811 / loss: 0.3493 - test accuracy: 0.5000 / loss: 0.7469\n",
      "Starting fold 5 / 10 - training accuracy: 0.9811 / loss: 0.3472 - test accuracy: 0.5000 / loss: 0.7663\n",
      "Starting fold 6 / 10 - training accuracy: 0.9811 / loss: 0.3453 - test accuracy: 0.5185 / loss: 0.7587\n",
      "Starting fold 7 / 10 - training accuracy: 0.9811 / loss: 0.3564 - test accuracy: 0.5000 / loss: 0.7562\n",
      "Starting fold 8 / 10 - training accuracy: 0.9811 / loss: 0.3539 - test accuracy: 0.5185 / loss: 0.7563\n",
      "Starting fold 9 / 10 - training accuracy: 0.9811 / loss: 0.3418 - test accuracy: 0.5185 / loss: 0.7461\n",
      "Starting fold 10 / 10 - training accuracy: 0.9623 / loss: 0.3591 - test accuracy: 0.5000 / loss: 0.7512\n",
      "average loss on test set: 0.7582\n",
      "average accuracy on test set: 0.5056\n",
      "Starting outer fold 1 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5000)\n",
      " Starting inner fold 2 / 2 (acc: 0.5283)\n",
      "Starting outer fold 2 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5185)\n",
      " Starting inner fold 2 / 2 (acc: 0.5472)\n",
      "Starting outer fold 3 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5000)\n",
      " Starting inner fold 2 / 2 (acc: 0.5849)\n",
      "Starting outer fold 4 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5741)\n",
      " Starting inner fold 2 / 2 (acc: 0.5283)\n",
      "Starting outer fold 5 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5741)\n",
      " Starting inner fold 2 / 2 (acc: 0.5849)\n",
      " => mean acc: 0.5440\n",
      "\n",
      "\n",
      "=== Pretrain on expert dominant labels; tune on crowd labels on crowd subset; finetune ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.9643 / loss: 0.3387 - test accuracy: 0.6000 / loss: 0.6803\n",
      "Starting fold 2 / 10 - training accuracy: 0.7857 / loss: 0.5158 - test accuracy: 0.5667 / loss: 0.6534\n",
      "Starting fold 3 / 10 - training accuracy: 0.7143 / loss: 0.5251 - test accuracy: 0.4333 / loss: 0.8100\n",
      "Starting fold 4 / 10 - training accuracy: 0.7857 / loss: 0.5465 - test accuracy: 0.6333 / loss: 0.6565\n",
      "Starting fold 5 / 10 - training accuracy: 1.0000 / loss: 0.3220 - test accuracy: 0.6000 / loss: 0.7153\n",
      "Starting fold 6 / 10 - training accuracy: 0.8214 / loss: 0.4417 - test accuracy: 0.5333 / loss: 0.7379\n",
      "Starting fold 7 / 10 - training accuracy: 0.7500 / loss: 0.4387 - test accuracy: 0.6000 / loss: 0.7107\n",
      "Starting fold 8 / 10 - training accuracy: 0.9286 / loss: 0.3937 - test accuracy: 0.7333 / loss: 0.6257\n",
      "Starting fold 9 / 10 - training accuracy: 0.9286 / loss: 0.3557 - test accuracy: 0.7000 / loss: 0.6381\n",
      "Starting fold 10 / 10 - training accuracy: 0.9286 / loss: 0.4588 - test accuracy: 0.6333 / loss: 0.6846\n",
      "average loss on test set: 0.6912\n",
      "average accuracy on test set: 0.6033\n",
      "Starting fold 1 / 10 - training accuracy: 0.9434 / loss: 0.3758 - test accuracy: 0.5000 / loss: 0.7457\n",
      "Starting fold 2 / 10 - training accuracy: 1.0000 / loss: 0.3361 - test accuracy: 0.5370 / loss: 0.7373\n",
      "Starting fold 3 / 10 - training accuracy: 1.0000 / loss: 0.3354 - test accuracy: 0.5370 / loss: 0.7337\n",
      "Starting fold 4 / 10 - training accuracy: 1.0000 / loss: 0.3348 - test accuracy: 0.5185 / loss: 0.7395\n",
      "Starting fold 5 / 10 - training accuracy: 1.0000 / loss: 0.3304 - test accuracy: 0.5556 / loss: 0.7275\n",
      "Starting fold 6 / 10 - training accuracy: 1.0000 / loss: 0.3270 - test accuracy: 0.5185 / loss: 0.7356\n",
      "Starting fold 7 / 10 - training accuracy: 1.0000 / loss: 0.3415 - test accuracy: 0.5556 / loss: 0.7180\n",
      "Starting fold 8 / 10 - training accuracy: 1.0000 / loss: 0.3520 - test accuracy: 0.5556 / loss: 0.7187\n",
      "Starting fold 9 / 10 - training accuracy: 1.0000 / loss: 0.3315 - test accuracy: 0.5556 / loss: 0.7214\n",
      "Starting fold 10 / 10 - training accuracy: 1.0000 / loss: 0.3324 - test accuracy: 0.5556 / loss: 0.7038\n",
      "average loss on test set: 0.7281\n",
      "average accuracy on test set: 0.5389\n",
      "Starting outer fold 1 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5926)\n",
      " Starting inner fold 2 / 2 (acc: 0.5283)\n",
      "Starting outer fold 2 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.6111)\n",
      " Starting inner fold 2 / 2 (acc: 0.5283)\n",
      "Starting outer fold 3 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.6296)\n",
      " Starting inner fold 2 / 2 (acc: 0.6038)\n",
      "Starting outer fold 4 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.6481)\n",
      " Starting inner fold 2 / 2 (acc: 0.5849)\n",
      "Starting outer fold 5 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5926)\n",
      " Starting inner fold 2 / 2 (acc: 0.5472)\n",
      " => mean acc: 0.5867\n",
      "\n",
      "\n",
      "\n",
      "=== Pretrain on expert dominant labels; tune on crowd labels on crowd unique subset ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.8571 / loss: 0.3775 - test accuracy: 0.5667 / loss: 0.6934\n",
      "Starting fold 2 / 10 - training accuracy: 0.8214 / loss: 0.4270 - test accuracy: 0.6000 / loss: 0.6425\n",
      "Starting fold 3 / 10 - training accuracy: 0.9643 / loss: 0.3706 - test accuracy: 0.4667 / loss: 0.7772\n",
      "Starting fold 4 / 10 - training accuracy: 0.8214 / loss: 0.4720 - test accuracy: 0.6000 / loss: 0.6742\n",
      "Starting fold 5 / 10 - training accuracy: 0.9286 / loss: 0.3721 - test accuracy: 0.6000 / loss: 0.7002\n",
      "Starting fold 6 / 10 - training accuracy: 0.9286 / loss: 0.3702 - test accuracy: 0.5333 / loss: 0.7519\n",
      "Starting fold 7 / 10 - training accuracy: 0.7857 / loss: 0.4234 - test accuracy: 0.6000 / loss: 0.7031\n",
      "Starting fold 8 / 10 - training accuracy: 1.0000 / loss: 0.3225 - test accuracy: 0.5000 / loss: 0.7761\n",
      "Starting fold 9 / 10 - training accuracy: 0.9286 / loss: 0.3953 - test accuracy: 0.5667 / loss: 0.7125\n",
      "Starting fold 10 / 10 - training accuracy: 1.0000 / loss: 0.3342 - test accuracy: 0.5667 / loss: 0.7205\n",
      "average loss on test set: 0.7152\n",
      "average accuracy on test set: 0.5600\n",
      "Starting fold 1 / 10 - training accuracy: 1.0000 / loss: 0.3428 - test accuracy: 0.5370 / loss: 0.7529\n",
      "Starting fold 2 / 10 - training accuracy: 0.9811 / loss: 0.3467 - test accuracy: 0.5556 / loss: 0.7450\n",
      "Starting fold 3 / 10 - training accuracy: 1.0000 / loss: 0.3402 - test accuracy: 0.5556 / loss: 0.7397\n",
      "Starting fold 4 / 10 - training accuracy: 0.9811 / loss: 0.3345 - test accuracy: 0.5370 / loss: 0.7539\n",
      "Starting fold 5 / 10 - training accuracy: 0.9623 / loss: 0.3523 - test accuracy: 0.5370 / loss: 0.7400\n",
      "Starting fold 6 / 10 - training accuracy: 0.9811 / loss: 0.3451 - test accuracy: 0.5000 / loss: 0.7500\n",
      "Starting fold 7 / 10 - training accuracy: 0.9623 / loss: 0.3454 - test accuracy: 0.5370 / loss: 0.7404\n",
      "Starting fold 8 / 10 - training accuracy: 1.0000 / loss: 0.3348 - test accuracy: 0.5000 / loss: 0.7579\n",
      "Starting fold 9 / 10 - training accuracy: 0.9623 / loss: 0.3511 - test accuracy: 0.5370 / loss: 0.7486\n",
      "Starting fold 10 / 10 - training accuracy: 1.0000 / loss: 0.3304 - test accuracy: 0.5000 / loss: 0.7482\n",
      "average loss on test set: 0.7477\n",
      "average accuracy on test set: 0.5296\n",
      "Starting outer fold 1 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.6154)\n",
      " Starting inner fold 2 / 2 (acc: 0.5833)\n",
      "Starting outer fold 2 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5769)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting inner fold 2 / 2 (acc: 0.5417)\n",
      "Starting outer fold 3 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5769)\n",
      " Starting inner fold 2 / 2 (acc: 0.5417)\n",
      "Starting outer fold 4 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5769)\n",
      " Starting inner fold 2 / 2 (acc: 0.5833)\n",
      "Starting outer fold 5 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5769)\n",
      " Starting inner fold 2 / 2 (acc: 0.5417)\n",
      " => mean acc: 0.5715\n",
      "\n",
      "\n",
      "=== Pretrain on expert dominant labels; tune on crowd labels on crowd unique subset; finetune ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.9643 / loss: 0.3638 - test accuracy: 0.5667 / loss: 0.6958\n",
      "Starting fold 2 / 10 - training accuracy: 0.8571 / loss: 0.4712 - test accuracy: 0.4667 / loss: 0.8064\n",
      "Starting fold 3 / 10 - training accuracy: 0.9643 / loss: 0.3578 - test accuracy: 0.4667 / loss: 0.7041\n",
      "Starting fold 4 / 10 - training accuracy: 0.9286 / loss: 0.3723 - test accuracy: 0.5000 / loss: 0.7506\n",
      "Starting fold 5 / 10 - training accuracy: 0.9286 / loss: 0.3844 - test accuracy: 0.6333 / loss: 0.6860\n",
      "Starting fold 6 / 10 - training accuracy: 0.9286 / loss: 0.4210 - test accuracy: 0.5333 / loss: 0.7540\n",
      "Starting fold 7 / 10 - training accuracy: 0.9643 / loss: 0.4078 - test accuracy: 0.5667 / loss: 0.7054\n",
      "Starting fold 8 / 10 - training accuracy: 0.7143 / loss: 0.5997 - test accuracy: 0.5667 / loss: 0.7524\n",
      "Starting fold 9 / 10 - training accuracy: 0.8929 / loss: 0.4088 - test accuracy: 0.5333 / loss: 0.7012\n",
      "Starting fold 10 / 10 - training accuracy: 0.8929 / loss: 0.3952 - test accuracy: 0.6333 / loss: 0.6544\n",
      "average loss on test set: 0.7210\n",
      "average accuracy on test set: 0.5467\n",
      "Starting fold 1 / 10 - training accuracy: 0.9434 / loss: 0.3631 - test accuracy: 0.4815 / loss: 0.7824\n",
      "Starting fold 2 / 10 - training accuracy: 0.9811 / loss: 0.3463 - test accuracy: 0.5185 / loss: 0.7628\n",
      "Starting fold 3 / 10 - training accuracy: 0.9811 / loss: 0.3488 - test accuracy: 0.5000 / loss: 0.7727\n",
      "Starting fold 4 / 10 - training accuracy: 0.9811 / loss: 0.3473 - test accuracy: 0.5000 / loss: 0.7810\n",
      "Starting fold 5 / 10 - training accuracy: 0.9623 / loss: 0.3560 - test accuracy: 0.5185 / loss: 0.7625\n",
      "Starting fold 6 / 10 - training accuracy: 0.9623 / loss: 0.3531 - test accuracy: 0.5370 / loss: 0.7554\n",
      "Starting fold 7 / 10 - training accuracy: 0.9811 / loss: 0.3472 - test accuracy: 0.5185 / loss: 0.7562\n",
      "Starting fold 8 / 10 - training accuracy: 0.9811 / loss: 0.3464 - test accuracy: 0.4815 / loss: 0.7776\n",
      "Starting fold 9 / 10 - training accuracy: 0.9623 / loss: 0.3687 - test accuracy: 0.5185 / loss: 0.7579\n",
      "Starting fold 10 / 10 - training accuracy: 0.9623 / loss: 0.3588 - test accuracy: 0.5185 / loss: 0.7617\n",
      "average loss on test set: 0.7670\n",
      "average accuracy on test set: 0.5093\n",
      "Starting outer fold 1 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5769)\n",
      " Starting inner fold 2 / 2 (acc: 0.5833)\n",
      "Starting outer fold 2 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.6154)\n",
      " Starting inner fold 2 / 2 (acc: 0.5417)\n",
      "Starting outer fold 3 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5769)\n",
      " Starting inner fold 2 / 2 (acc: 0.5833)\n",
      "Starting outer fold 4 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.6154)\n",
      " Starting inner fold 2 / 2 (acc: 0.5833)\n",
      "Starting outer fold 5 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.6154)\n",
      " Starting inner fold 2 / 2 (acc: 0.5833)\n",
      " => mean acc: 0.5875\n",
      "\n",
      "\n",
      "\n",
      "=== Pretrain on expert dominant labels; tune on crowd labels on expert subset ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.9643 / loss: 0.3688 - test accuracy: 0.5333 / loss: 0.7444\n",
      "Starting fold 2 / 10 - training accuracy: 1.0000 / loss: 0.3670 - test accuracy: 0.6000 / loss: 0.6695\n",
      "Starting fold 3 / 10 - training accuracy: 0.8214 / loss: 0.4156 - test accuracy: 0.4667 / loss: 0.8122\n",
      "Starting fold 4 / 10 - training accuracy: 0.8214 / loss: 0.4722 - test accuracy: 0.5333 / loss: 0.7526\n",
      "Starting fold 5 / 10 - training accuracy: 0.9286 / loss: 0.3644 - test accuracy: 0.5000 / loss: 0.7435\n",
      "Starting fold 6 / 10 - training accuracy: 0.8929 / loss: 0.4058 - test accuracy: 0.5333 / loss: 0.7091\n",
      "Starting fold 7 / 10 - training accuracy: 0.9643 / loss: 0.3661 - test accuracy: 0.6333 / loss: 0.6823\n",
      "Starting fold 8 / 10 - training accuracy: 0.8929 / loss: 0.4019 - test accuracy: 0.5333 / loss: 0.7243\n",
      "Starting fold 9 / 10 - training accuracy: 0.9286 / loss: 0.4102 - test accuracy: 0.5333 / loss: 0.7073\n",
      "Starting fold 10 / 10 - training accuracy: 0.9643 / loss: 0.3551 - test accuracy: 0.6000 / loss: 0.6793\n",
      "average loss on test set: 0.7224\n",
      "average accuracy on test set: 0.5467\n",
      "Starting fold 1 / 10 - training accuracy: 1.0000 / loss: 0.3230 - test accuracy: 0.5517 / loss: 0.7296\n",
      "Starting fold 2 / 10 - training accuracy: 1.0000 / loss: 0.3267 - test accuracy: 0.5517 / loss: 0.7352\n",
      "Starting fold 3 / 10 - training accuracy: 1.0000 / loss: 0.3199 - test accuracy: 0.5862 / loss: 0.7103\n",
      "Starting fold 4 / 10 - training accuracy: 1.0000 / loss: 0.3233 - test accuracy: 0.5862 / loss: 0.7292\n",
      "Starting fold 5 / 10 - training accuracy: 1.0000 / loss: 0.3322 - test accuracy: 0.5862 / loss: 0.7185\n",
      "Starting fold 6 / 10 - training accuracy: 1.0000 / loss: 0.3291 - test accuracy: 0.5862 / loss: 0.7174\n",
      "Starting fold 7 / 10 - training accuracy: 1.0000 / loss: 0.3302 - test accuracy: 0.5862 / loss: 0.7193\n",
      "Starting fold 8 / 10 - training accuracy: 1.0000 / loss: 0.3284 - test accuracy: 0.5862 / loss: 0.7291\n",
      "Starting fold 9 / 10 - training accuracy: 1.0000 / loss: 0.3230 - test accuracy: 0.5517 / loss: 0.7329\n",
      "Starting fold 10 / 10 - training accuracy: 1.0000 / loss: 0.3256 - test accuracy: 0.5862 / loss: 0.7253\n",
      "average loss on test set: 0.7247\n",
      "average accuracy on test set: 0.5759\n",
      "Starting outer fold 1 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5862)\n",
      " Starting inner fold 2 / 2 (acc: 0.3929)\n",
      "Starting outer fold 2 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5862)\n",
      " Starting inner fold 2 / 2 (acc: 0.4286)\n",
      "Starting outer fold 3 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.5862)\n",
      " Starting inner fold 2 / 2 (acc: 0.5000)\n",
      "Starting outer fold 4 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.6207)\n",
      " Starting inner fold 2 / 2 (acc: 0.5000)\n",
      "Starting outer fold 5 / 5\n",
      " Starting inner fold 1 / 2 (acc: 0.6207)\n",
      " Starting inner fold 2 / 2 (acc: 0.4643)\n",
      " => mean acc: 0.5286\n",
      "\n",
      "\n",
      "=== Pretrain on expert dominant labels; tune on crowd labels on expert subset; finetune ===\n",
      "Starting fold 1 / 10 - training accuracy: 1.0000 / loss: 0.3484 - test accuracy: 0.5667 / loss: 0.7298\n",
      "Starting fold 2 / 10"
     ]
    }
   ],
   "source": [
    "## hyperparameters\n",
    "lr = 0.01\n",
    "n_epoch = 250\n",
    "p_dropout = 0.1\n",
    "\n",
    "## define model\n",
    "indim = X_2D.shape[1]\n",
    "outdim = np.unique(y_experts[experts_idx]).shape[0]\n",
    "assert outdim == np.unique(y_crowd[crowd_idx]).shape[0]\n",
    "\n",
    "model = ClassifierNN(input_dim=indim,\n",
    "                     output_dim=outdim,\n",
    "                     p_dropout=p_dropout)\n",
    "\n",
    "print(\"=== Pretrain on expert dominant labels; tune on crowd labels on crowd subset ===\")\n",
    "state = fit(model, X_2D, y_experts, experts_idx, lr=lr, n_epoch=n_epoch)\n",
    "state = fit(model, X_2D, y_crowd, crowd_idx, lr=lr, n_epoch=n_epoch, state=state)\n",
    "vneural_net_acc_dominant_experts_crowd = fit_test(model, X_2D, y_crowd, crowd_idx, crowd_splits, lr=lr, n_epoch=n_epoch, state=state)\n",
    "\n",
    "print(\"\\n=== Pretrain on expert dominant labels; tune on crowd labels on crowd subset; finetune ===\")\n",
    "state = fit(model, X_2D, y_experts, experts_idx, lr=lr, n_epoch=n_epoch)\n",
    "state = fit(model, X_2D, y_crowd, crowd_idx, lr=lr, n_epoch=n_epoch, state=state, finetune=True)\n",
    "vneural_net_acc_dominant_experts_crowd_ft = fit_test(model, X_2D, y_crowd, crowd_idx, crowd_splits, lr=lr, n_epoch=n_epoch, state=state, finetune=True)\n",
    "\n",
    "\n",
    "print(\"\\n\\n=== Pretrain on expert dominant labels; tune on crowd labels on crowd unique subset ===\")\n",
    "state = fit(model, X_2D, y_experts, experts_idx, lr=lr, n_epoch=n_epoch)\n",
    "state = fit(model, X_2D, y_crowd, crowd_idx, lr=lr, n_epoch=n_epoch, state=state)\n",
    "vneural_net_acc_dominant_experts_crowdu = fit_test(model, X_2D, y_crowd, crowd_idx, crowd_unique_splits, lr=lr, n_epoch=n_epoch, state=state)\n",
    "\n",
    "print(\"\\n=== Pretrain on expert dominant labels; tune on crowd labels on crowd unique subset; finetune ===\")\n",
    "state = fit(model, X_2D, y_experts, experts_idx, lr=lr, n_epoch=n_epoch)\n",
    "state = fit(model, X_2D, y_crowd, crowd_idx, lr=lr, n_epoch=n_epoch, state=state, finetune=True)\n",
    "vneural_net_acc_dominant_experts_crowdu_ft = fit_test(model, X_2D, y_crowd, crowd_idx, crowd_unique_splits, lr=lr, n_epoch=n_epoch, state=state, finetune=True)\n",
    "\n",
    "\n",
    "print(\"\\n\\n=== Pretrain on expert dominant labels; tune on crowd labels on expert subset ===\")\n",
    "state = fit(model, X_2D, y_experts, experts_idx, lr=lr, n_epoch=n_epoch)\n",
    "state = fit(model, X_2D, y_crowd, experts_idx, lr=lr, n_epoch=n_epoch, state=state)\n",
    "vneural_net_acc_dominant_experts_crowdexp = fit_test(model, X_2D, y_crowd, experts_idx, crowd_on_experts_splits, lr=lr, n_epoch=n_epoch, state=state)\n",
    "\n",
    "print(\"\\n=== Pretrain on expert dominant labels; tune on crowd labels on expert subset; finetune ===\")\n",
    "state = fit(model, X_2D, y_experts, experts_idx, lr=lr, n_epoch=n_epoch)\n",
    "state = fit(model, X_2D, y_crowd, experts_idx, lr=lr, n_epoch=n_epoch, state=state, finetune=True)\n",
    "vneural_net_acc_dominant_experts_crowdexp_ft = fit_test(model, X_2D, y_crowd, experts_idx, crowd_on_experts_splits, lr=lr, n_epoch=n_epoch, state=state, finetune=True)\n",
    "\n",
    "\n",
    "print(\"\\n\\n=== Pretrain on expert dominant labels; tune on combined labels on all ===\")\n",
    "state = fit(model, X_2D, y_experts, experts_idx, lr=lr, n_epoch=n_epoch)\n",
    "state = fit(model, X_2D, y_combined, combined_idx, lr=lr, n_epoch=n_epoch, state=state)\n",
    "vneural_net_acc_dominant_experts_comb = fit_test(model, X_2D, y_combined, combined_idx, combined_splits, lr=lr, n_epoch=n_epoch, state=state)\n",
    "\n",
    "print(\"\\n=== Pretrain on expert dominant labels; tune on combined labels on all; finetune ===\")\n",
    "state = fit(model, X_2D, y_experts, experts_idx, lr=lr, n_epoch=n_epoch)\n",
    "state = fit(model, X_2D, y_combined, combined_idx, lr=lr, n_epoch=n_epoch, state=state, finetune=True)\n",
    "vneural_net_acc_dominant_experts_comb = fit_test(model, X_2D, y_combined, combined_idx, combined_splits, lr=lr, n_epoch=n_epoch, state=state, finetune=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_acc = [random_forest_acc_experts_dominant, random_forest_acc_crowd_dominant, random_forest_acc_combined_dominant]\n",
    "\n",
    "nhypotheses = len(N_ESTIMATORS)\n",
    "for k in range(nhypotheses):\n",
    "    table = {'p-values - {} estimators'.format(N_ESTIMATORS[k]): labels}\n",
    "    table.update({lab: list() for lab in labels})\n",
    "\n",
    "    nlabels = len(labels)\n",
    "    for i in range(nlabels):\n",
    "        for e in range(nlabels):\n",
    "            if e != i:\n",
    "                table[labels[i]].append(np.nan)\n",
    "            else:\n",
    "                f, p, mean, variance = alpaydin_F_test(nn_acc[i],\n",
    "                                                       rf_acc[j][k])\n",
    "                table[labels[i]].append(p)\n",
    "                #print(\"RF {} vs {} estimators\".format(nn_acc[i], nn_acc[j]))\n",
    "                #print(\" f: {:.4f}, p: {:.4f}, mean: {:.4f}, var: {:.4f}\".format(f, p, mean, variance))\n",
    "\n",
    "    significance = pd.DataFrame(table)\n",
    "    display(significance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
