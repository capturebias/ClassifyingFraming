{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install sklearn\n",
    "#%pip install torch\n",
    "#%pip install ipywidgets\n",
    "#%pip install IProgress\n",
    "#%pip install transformers\n",
    "\n",
    "from math import sqrt\n",
    "import IProgress\n",
    "import os\n",
    "from time import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "## project structure\n",
    "DATA_DIR = \"/data/projects/capturingBias/research/framing/data/\"  # change to \"./\" for current directory\n",
    "DATA_NPZ = DATA_DIR + \"sequences_raw.npz\"\n",
    "TARGETS_NPZ = DATA_DIR + \"targets.npz\"\n",
    "\n",
    "## load files\n",
    "data = np.load(DATA_NPZ, allow_pickle=True)\n",
    "targets = np.load(TARGETS_NPZ)\n",
    "\n",
    "sequences = data['transcriptions']\n",
    "target_idx = targets['target_idx']\n",
    "y_likert_crowd = targets['y_likert_crowd']\n",
    "y_likert_experts = targets['y_likert_experts']\n",
    "y_dominant_crowd = targets['y_dominant_crowd']\n",
    "y_dominant_experts = targets['y_dominant_experts']\n",
    "y_likert_combined = targets['y_likert_combined']\n",
    "y_dominant_combined = targets['y_dominant_combined']\n",
    "\n",
    "\n",
    "# likert\n",
    "likert_expert_idx = np.where(y_likert_experts > -1)[0]\n",
    "likert_crowd_idx = np.setdiff1d(np.where(y_likert_crowd > -1)[0],\n",
    "                                likert_expert_idx,\n",
    "                                assume_unique=True)\n",
    "likert_combined_idx = np.concatenate([likert_crowd_idx,\n",
    "                                      likert_expert_idx])\n",
    "\n",
    "# dominant\n",
    "dominant_expert_idx = np.where(y_dominant_experts > -1)[0]\n",
    "dominant_crowd_idx = np.setdiff1d(np.where(y_dominant_crowd > -1)[0],\n",
    "                                  dominant_expert_idx,\n",
    "                                  assume_unique=True)\n",
    "dominant_combined_idx = np.concatenate([dominant_crowd_idx,\n",
    "                                        dominant_expert_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=-1):\n",
    "    if seed < 0:\n",
    "        seed = np.random.randint(0, 2**32-1)\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "#set_seed(47)  # make reproducable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(y, test_ratio=.2):\n",
    "    train_idx = list()\n",
    "    test_idx = list()\n",
    "    \n",
    "    strats = [np.where(y == lab)[0] for lab in np.unique(y) if lab > -1]\n",
    "    for strat in strats:\n",
    "        n = strat.shape[0]\n",
    "        train_idx.append(strat[:int(n*(1-test_ratio))])\n",
    "        test_idx.append(strat[int(n*(1-test_ratio)):])\n",
    "        \n",
    "    train_idx = np.concatenate(train_idx)\n",
    "    test_idx = np.concatenate(test_idx)\n",
    "    \n",
    "    np.random.shuffle(train_idx)\n",
    "    np.random.shuffle(test_idx)\n",
    "    \n",
    "    return (train_idx, test_idx)\n",
    "\n",
    "def create_splits_one_hot(y):\n",
    "    vec = -np.ones(y.shape[0])\n",
    "    nonzero = y.nonzero()\n",
    "    vec[nonzero[:,0]] = nonzero[:,1].float()\n",
    "    \n",
    "    return create_splits(vec)\n",
    "\n",
    "def mkbatches_varlength(sequences, nbins):\n",
    "    n = sequences.shape[0]\n",
    "    # sort on length\n",
    "    seq_lengths = [len(sequences[i]) for i in range(n)]\n",
    "    idc = np.arange(n, dtype=np.int32)\n",
    "    _, sequences_sorted_idc = zip(*sorted(zip(seq_lengths, idc)))\n",
    "\n",
    "    return np.array_split(sequences_sorted_idc, nbins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def majority_class(y):\n",
    "    ct = Counter(y)\n",
    "    return ct.most_common(1)[0][1] / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority class accuracy on Likert labels (baseline)\n",
      " crowd labels:  0.2459\n",
      " expert labels: 0.2414\n",
      " combined labels: 0.2269\n",
      "\n",
      "Majority class accuracy on Dominant labels (baseline)\n",
      " crowd labels:  0.5536\n",
      " expert labels: 0.5345\n",
      " combined labels: 0.5439\n"
     ]
    }
   ],
   "source": [
    "majority_class_acc_crowd_likert = majority_class(y_likert_crowd[likert_crowd_idx])\n",
    "majority_class_acc_experts_likert = majority_class(y_likert_experts[likert_expert_idx])\n",
    "majority_class_acc_combined_likert = majority_class(y_likert_combined[likert_combined_idx])\n",
    "\n",
    "print(\"Majority class accuracy on Likert labels (baseline)\")\n",
    "print(\" crowd labels:  {:.4f}\".format(majority_class_acc_crowd_likert))\n",
    "print(\" expert labels: {:.4f}\".format(majority_class_acc_experts_likert))\n",
    "print(\" combined labels: {:.4f}\".format(majority_class_acc_combined_likert))\n",
    "\n",
    "majority_class_acc_crowd_dominant = majority_class(y_dominant_crowd[dominant_crowd_idx])\n",
    "majority_class_acc_experts_dominant = majority_class(y_dominant_experts[dominant_expert_idx])\n",
    "majority_class_acc_combined_dominant = majority_class(y_dominant_combined[dominant_combined_idx])\n",
    "\n",
    "print(\"\\nMajority class accuracy on Dominant labels (baseline)\")\n",
    "print(\" crowd labels:  {:.4f}\".format(majority_class_acc_crowd_dominant))\n",
    "print(\" expert labels: {:.4f}\".format(majority_class_acc_experts_dominant))\n",
    "print(\" combined labels: {:.4f}\".format(majority_class_acc_combined_dominant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert numpy arrays to PyTorch tensors\n",
    "y_likert_crowd = torch.from_numpy(y_likert_crowd)\n",
    "y_likert_experts = torch.from_numpy(y_likert_experts)\n",
    "y_likert_combined = torch.from_numpy(y_likert_combined)\n",
    "y_dominant_crowd = torch.from_numpy(y_dominant_crowd)\n",
    "y_dominant_experts = torch.from_numpy(y_dominant_experts)\n",
    "y_dominant_combined = torch.from_numpy(y_dominant_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(y_hat, y):\n",
    "    # y := 1D array of class labels\n",
    "    # y_hat := 2D array of one-hot class labels\n",
    "    _, labels = y_hat.max(dim=1)\n",
    "    return torch.mean(torch.eq(labels, y).float())\n",
    "\n",
    "        \n",
    "def zero_pad(array: np.ndarray) -> torch.Tensor:\n",
    "    n = array.shape[0]\n",
    "    m = max([len(array[i]) for i in range(n)])\n",
    "    \n",
    "    mask = np.ones((n, m), dtype=np.int32)\n",
    "    padded_array = np.zeros((n, m), dtype=np.int32)\n",
    "    for i in range(n):\n",
    "        l = len(array[i])\n",
    "        padded_array[i,:l] = array[i]\n",
    "        mask[i,l:] = np.zeros((m-l), dtype=np.int32)\n",
    "        \n",
    "    return (torch.from_numpy(padded_array), torch.from_numpy(mask))\n",
    "\n",
    "def fit(model, X, y, index, lr=0.01, l2norm=0.01, n_folds=10, n_epoch=250, patience=7, state=None, finetune=False):\n",
    "    n_samples = X[index].shape[0]\n",
    "\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    best_state = None\n",
    "    best_state_opt = None\n",
    "    best_score = -1\n",
    "    for fold_i in range(n_folds):\n",
    "        print(\"Starting fold {} / {}\".format(fold_i+1, n_folds), end='')\n",
    "        if state is None:\n",
    "            model.init()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "        else:\n",
    "            model.load_state_dict(state[0])\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "            optimizer.load_state_dict(state[1])\n",
    "            if finetune:\n",
    "                for layer in model.layers[:-1]:\n",
    "                    layer.requires_grad = False\n",
    "            \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # early stopping\n",
    "        patience_left = patience\n",
    "        best_fold_score = -1\n",
    "        delta = 1e-4\n",
    "        best_fold_state = None\n",
    "        best_fold_state_opt = None\n",
    "        \n",
    "        train_fold_idx, test_fold_idx  = create_splits(y[index])\n",
    "        train_idx = index[train_fold_idx]\n",
    "        test_idx = index[test_fold_idx]\n",
    "        train_idx_batches = mkbatches_varlength(X[train_idx], nbins=4)\n",
    "        for epoch in range(n_epoch):\n",
    "            model.train()\n",
    "            \n",
    "            batch = train_idx_batches[epoch%4]\n",
    "            X_batch = X[train_idx[batch]]\n",
    "            X_batch, X_mask = zero_pad(X_batch)\n",
    "            \n",
    "            y_hat = model(X_batch.long(), X_mask.float())\n",
    "            train_acc = categorical_accuracy(y_hat, y[train_idx[batch]])\n",
    "            train_loss = criterion(y_hat, y[train_idx[batch]].long())\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            X_test, _ = zero_pad(X[test_idx])\n",
    "            test_loss = None\n",
    "            with torch.no_grad():\n",
    "                y_hat = model(X_test.long(), None)\n",
    "                test_acc = categorical_accuracy(y_hat, y[test_idx])\n",
    "                test_loss = criterion(y_hat, y[test_idx].long())\n",
    "                \n",
    "            train_loss = float(train_loss.item())\n",
    "            test_loss = float(test_loss.item())\n",
    "\n",
    "            if best_fold_score < 0:\n",
    "                best_fold_score = test_loss\n",
    "                best_fold_state = model.state_dict()\n",
    "                best_fold_state_opt = optimizer.state_dict()\n",
    "                            \n",
    "            if patience <= 0:\n",
    "                continue\n",
    "            if test_loss >= best_fold_score - delta:\n",
    "                patience_left -= 1\n",
    "            else:\n",
    "                best_fold_score = test_loss\n",
    "                best_fold_state = model.state_dict()\n",
    "                best_fold_state_opt = optimizer.state_dict()\n",
    "                patience_left = patience\n",
    "            if patience_left <= 0:\n",
    "                model.load_state_dict(best_fold_state)\n",
    "                optimizer.load_state_dict(best_fold_state_opt)\n",
    "                break\n",
    "                \n",
    "        test_idx = index[create_splits(y[index])[1]]  # get new random test set to validate on\n",
    "        X_test, _ = zero_pad(X[test_idx])\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(X_test.long(), None)\n",
    "            test_acc = categorical_accuracy(y_hat, y[test_idx])\n",
    "            test_loss = float(criterion(y_hat, y[test_idx].long()).item())\n",
    "        \n",
    "        loss += test_loss\n",
    "        acc += test_acc\n",
    "        if best_score < 0 or best_score > test_loss:\n",
    "            best_state = best_fold_state\n",
    "            best_state_opt = best_fold_state_opt\n",
    "            best_score = test_loss\n",
    "        print(\" - training accuracy: {:.4f} / loss: {:.4f} - test accuracy: {:.4f} / loss: {:.4f}\".format(train_acc,\n",
    "                                                                                          train_loss,\n",
    "                                                                                          test_acc,\n",
    "                                                                                          test_loss))\n",
    "        \n",
    "    loss /= n_folds\n",
    "    acc /= n_folds\n",
    "    print(\"average loss on test set: {:.4f}\".format(loss))\n",
    "    print(\"average accuracy on test set: {:.4f}\".format(acc))\n",
    "    \n",
    "    return (acc, (best_state, best_state_opt))\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, out_dim, p_dropout=0.05):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transformer = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        for param in self.transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        hid_dim = self.transformer.config.dim\n",
    "        self.layers = nn.Sequential(nn.Linear(hid_dim, hid_dim),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Dropout(p=p_dropout),\n",
    "                                    \n",
    "                                    nn.Linear(hid_dim, out_dim))\n",
    "                                    \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.init()\n",
    "        \n",
    "    def forward(self, input_ids, mask):\n",
    "        H = self.transformer(input_ids, attention_mask=mask)[0]  # (bs, seq_len, dim)\n",
    "        H_pooled = H[:, 0]  # (bs, dim)\n",
    "        for layer in self.layers:\n",
    "            H_pooled = layer(H_pooled)\n",
    "                           \n",
    "        return self.softmax(H_pooled)\n",
    "    \n",
    "    def init(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad:\n",
    "                nn.init.normal_(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length in sample set: 80\n",
      "Max length in sample set: 122\n"
     ]
    }
   ],
   "source": [
    "tokenized_sequences = [tokenizer.encode(seq[:512], add_special_tokens=True) for seq in sequences]\n",
    "tokenized_sequences_length = [len(seq) for seq in tokenized_sequences]\n",
    "\n",
    "samples_tokenized_length = np.array(tokenized_sequences_length)[target_idx]\n",
    "samples_min_length = min(samples_tokenized_length)\n",
    "samples_max_length = max(samples_tokenized_length)\n",
    "print(\"Min length in sample set: %i\" % samples_min_length)\n",
    "print(\"Max length in sample set: %i\" % samples_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(tokenized_sequences_length, log=True, bins=50)\n",
    "plt.ylabel('log no. sequences')\n",
    "plt.xlabel('Length');\n",
    "\n",
    "plt.hist(samples_tokenized_length, log=True, bins=10)\n",
    "plt.ylabel('log no. sequences')\n",
    "plt.xlabel('Length');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only consider sample data for now\n",
    "sample_input = np.array(tokenized_sequences)[target_idx]\n",
    "y_likert_crowd = y_likert_crowd[target_idx]\n",
    "y_likert_experts = y_likert_experts[target_idx]\n",
    "y_likert_combined = y_likert_combined[target_idx]\n",
    "y_dominant_crowd = y_dominant_crowd[target_idx]\n",
    "y_dominant_experts = y_dominant_experts[target_idx]\n",
    "y_dominant_combined = y_dominant_combined[target_idx]\n",
    "\n",
    "# likert\n",
    "likert_expert_idx = np.where(y_likert_experts > -1)[0]\n",
    "likert_crowd_idx = np.setdiff1d(np.where(y_likert_crowd > -1)[0],\n",
    "                                likert_expert_idx,\n",
    "                                assume_unique=True)\n",
    "likert_combined_idx = np.concatenate([likert_crowd_idx,\n",
    "                                      likert_expert_idx])\n",
    "\n",
    "# dominant\n",
    "dominant_expert_idx = np.where(y_dominant_experts > -1)[0]\n",
    "dominant_crowd_idx = np.setdiff1d(np.where(y_dominant_crowd > -1)[0],\n",
    "                                  dominant_expert_idx,\n",
    "                                  assume_unique=True)\n",
    "dominant_combined_idx = np.concatenate([dominant_crowd_idx,\n",
    "                                        dominant_expert_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results on expert dominant labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.5455 / loss: 0.7678 - test accuracy: 0.4615 / loss: 0.8517\n",
      "Starting fold 2 / 10 - training accuracy: 0.6364 / loss: 0.6769 - test accuracy: 0.4615 / loss: 0.8517\n",
      "Starting fold 3 / 10 - training accuracy: 0.4545 / loss: 0.8587 - test accuracy: 0.5385 / loss: 0.7748\n",
      "Starting fold 4 / 10 - training accuracy: 0.4545 / loss: 0.8587 - test accuracy: 0.5385 / loss: 0.7748\n",
      "Starting fold 5 / 10 - training accuracy: 0.5455 / loss: 0.7678 - test accuracy: 0.4615 / loss: 0.8517\n",
      "Starting fold 6 / 10 - training accuracy: 0.4545 / loss: 0.8587 - test accuracy: 0.5385 / loss: 0.7748\n",
      "Starting fold 7 / 10 - training accuracy: 0.4545 / loss: 0.8587 - test accuracy: 0.5385 / loss: 0.7748\n",
      "Starting fold 8 / 10 - training accuracy: 0.3636 / loss: 0.9496 - test accuracy: 0.5385 / loss: 0.7748\n",
      "Starting fold 9 / 10 - training accuracy: 0.4545 / loss: 0.8587 - test accuracy: 0.5385 / loss: 0.7748\n",
      "Starting fold 10 / 10 - training accuracy: 0.4545 / loss: 0.8587 - test accuracy: 0.5385 / loss: 0.7748\n",
      "average loss on test set: 0.7979\n",
      "average accuracy on test set: 0.5154\n",
      "\n",
      "=== Results on crowd dominant labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.3636 / loss: 0.9496 - test accuracy: 0.4167 / loss: 0.8966\n",
      "Starting fold 2 / 10 - training accuracy: 0.3636 / loss: 0.9496 - test accuracy: 0.4167 / loss: 0.8966\n",
      "Starting fold 3 / 10 - training accuracy: 0.7273 / loss: 0.5861 - test accuracy: 0.5000 / loss: 0.8102\n",
      "Starting fold 4 / 10 - training accuracy: 0.6364 / loss: 0.6769 - test accuracy: 0.5833 / loss: 0.7299\n",
      "Starting fold 5 / 10 - training accuracy: 0.6364 / loss: 0.6769 - test accuracy: 0.5833 / loss: 0.7299\n",
      "Starting fold 6 / 10 - training accuracy: 0.5455 / loss: 0.7678 - test accuracy: 0.4167 / loss: 0.8966\n",
      "Starting fold 7 / 10 - training accuracy: 0.5455 / loss: 0.7678 - test accuracy: 0.4167 / loss: 0.8966\n",
      "Starting fold 8 / 10 - training accuracy: 0.7273 / loss: 0.5860 - test accuracy: 0.5833 / loss: 0.7299\n",
      "Starting fold 9 / 10 - training accuracy: 0.5455 / loss: 0.7678 - test accuracy: 0.5833 / loss: 0.7299\n",
      "Starting fold 10 / 10 - training accuracy: 0.6364 / loss: 0.6769 - test accuracy: 0.5833 / loss: 0.7299\n",
      "average loss on test set: 0.8046\n",
      "average accuracy on test set: 0.5083\n",
      "\n",
      "=== Results on combined dominant labels ===\n",
      "Starting fold 1 / 10 - training accuracy: 0.3636 / loss: 0.9496 - test accuracy: 0.4583 / loss: 0.8549\n",
      "Starting fold 2 / 10 - training accuracy: 0.2727 / loss: 1.0405 - test accuracy: 0.4583 / loss: 0.8549\n",
      "Starting fold 3 / 10 - training accuracy: 0.6818 / loss: 0.6314 - test accuracy: 0.5417 / loss: 0.7716\n",
      "Starting fold 4 / 10 - training accuracy: 0.3182 / loss: 0.9951 - test accuracy: 0.4583 / loss: 0.8549\n",
      "Starting fold 5 / 10 - training accuracy: 0.6364 / loss: 0.6769 - test accuracy: 0.5417 / loss: 0.7716\n",
      "Starting fold 6 / 10 - training accuracy: 0.5909 / loss: 0.7224 - test accuracy: 0.5417 / loss: 0.7716\n",
      "Starting fold 7 / 10 - training accuracy: 0.5652 / loss: 0.7480 - test accuracy: 0.5417 / loss: 0.7716\n",
      "Starting fold 8 / 10 - training accuracy: 0.6364 / loss: 0.6769 - test accuracy: 0.5417 / loss: 0.7716\n",
      "Starting fold 9 / 10 - training accuracy: 0.2727 / loss: 1.0405 - test accuracy: 0.4583 / loss: 0.8549\n",
      "Starting fold 10 / 10 - training accuracy: 0.4091 / loss: 0.9042 - test accuracy: 0.4583 / loss: 0.8549\n",
      "average loss on test set: 0.8133\n",
      "average accuracy on test set: 0.5000\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier(out_dim=2)  # num labels\n",
    "## hyperparameters\n",
    "lr = 0.01\n",
    "n_epoch = 52  # multiple of 4 (batches)\n",
    "p_dropout = 0.1\n",
    "\n",
    "print(\"=== Results on expert dominant labels ===\")\n",
    "acc_dominant_experts, _ = fit(model, sample_input, y_dominant_experts, dominant_expert_idx, lr=lr, n_epoch=n_epoch)\n",
    "\n",
    "print(\"\\n=== Results on crowd dominant labels ===\")\n",
    "acc_dominant_crowd, _ = fit(model, sample_input, y_dominant_crowd, dominant_crowd_idx, lr=lr, n_epoch=n_epoch)\n",
    "\n",
    "print(\"\\n=== Results on combined dominant labels ===\")\n",
    "acc_dominant_combined, _ = fit(model, sample_input, y_dominant_combined, dominant_combined_idx, lr=lr, n_epoch=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
